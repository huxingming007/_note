1、spring aop and ioc
一、AOP
作用：面向切面编程，是面向对象的升华，切面是将与业务无关的代码（比方说日志、事务、安全等等）进行封装，供业务代码进行调用，减少系统中的重复代码，降低系统的耦合度，增加系统的服务性。
应用场景：接口鉴权、打印日志、方法耗时统计、spring的声明式事务！！(有时间看一下网址：https://segmentfault.com/a/1190000007469982#articleHeader4)-----安全、日志、事务
advice通知(增强): 拦截到连接点之后要执行的代码,通知分为前置 后置 异常 最终 环绕。告诉AOP 切面之后做什么，也就是说，我们知道了在哪里进行切面，那么我们也该让spring知道在切点处做什么。 
joinpoint连接点: advice 是在 join point 上执行的, 而 point cut 规定了哪些 join point 可以执行哪些 advice。joinpoint可以使一个执行方法。
poincut切入点: 通知定义了切面的“什么”和“何时”的话，那么切点就定义了“何处”。来描述对哪些 Join point 使用 advise。首先我们需要告诉AOP在哪里进行切面。比如在某个类的方法前后进行切面。 
              public interface Pointcut {
                     ClassFilter getClassFilter();
                     MethodMatcher getMethodMatcher(); // AOP 的作用是代理方法，那么，Spring 怎么知道代理哪些方法呢？必须通过某种方式来匹配方法的名称来决定是否对该方法进行增强，这就是 MethodMatcher 的作用。
                     Pointcut TRUE = TruePointcut.INSTANCE;
              }
Aspect切面：通知和切点的结合。
Advisor通知器：将 Advice 和 PointCut 结合起来。
织入(Weaving)：将aspect应用到目标对象并且创建代理对象的过程。动态代理织入, 在运行期为目标类添加增强(Advice)生成子类的方式。Spring 采用动态代理织入，而AspectJ采用编译器织入和类装载期织入。
AOP的实现策略：
AOP实现的关键在于AOP框架自动创建的AOP代理，AOP代理主要分为静态代理和动态代理，静态代理的代表为AspectJ；而动态代理则以Spring AOP为代表。
AspectJ是静态代理的增强，所谓的静态代理就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强。
JDK动态代理 在运行期间动态生成实现对象，在这个实现对象的接口方法中可以添加增强代码。// Proxy.newProxyInstance(targetObject.getClass().getClassLoader(),targetObject.getClass().getInterfaces(), InvocationHandler实现类);
字节码生成（CGLIB动态代理） 运行期间指定一个类的子类对象，并且覆盖其中特定方法，覆盖方法时可以添加增强代码。
        //通过反射机制给他实例化
        Enhancer enhancer = new Enhancer();
        //注入父类，告诉CGLib，生成的子类需要继承那个类
        enhancer.setSuperclass(clazz);
        //添加监听 this对象必须实现MethodInterceptor类并且实现方法intercept 调用代理类的方法时，会进入intercept方法!!!!
        enhancer.setCallback(this);
        //1.生成源代码
        //2.编译成.class文件
        //3.加载到JVM中，返回对象
        return enhancer.create();
自定义类加载器：类被加载时对其进行增强。
与AspectJ的静态代理不同，Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。
Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理。JDK动态代理通过反射来接收被代理的类，并且要求被代理的类必须实现一个接口。JDK动态代理的核心是InvocationHandler接口和Proxy类。

如果目标类没有实现接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成某个类的子类，注意，CGLIB是通过继承的方式做的动态代理，
因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。

AOP源码分析：
           1、XML的方式
           <bean id="testAdvisor" class="com.xavior.aopdemo.TestAdvisor"></bean>
           <bean id="testTarget" class="com.xavior.aopdemo.TestTarget"></bean>
           <bean id="testAOP" class="org.springframework.aop.framework.ProxyFactoryBean">  // 重点关注ProxyFactoryBean
                 <property name="targetName">
                   <value>testTarget</value>
                </property>
                <property name="interceptorNames">
                  <list>
                     <value>testAdvisor</value>
                  </list>
                </property>
           </bean>
           TestTarget target = (TestTarget) applicationContext.getBean("testAOP");
           AbstractApplicationContext.getBean，从容器或获取 Bean，再调用 doGetBean 方法。进入getObjectForBeanInstance：
            // 显然ProxyFactoryBean instanceof FactoryBean 会往下走
            if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) {
            return beanInstance;
            }
           然后会进入方法 doGetObjectFromFactoryBean(factory, beanName)，该方法会直接进入 object = factory.getObject() 行，也就是 ProxyFactoryBean 的 getObject 方法，
           还记得我们说过，Spring 允许我们从写 getObject 方法来实现特定逻辑吗？ 我们看看该方法实现：
           public Object getObject() throws BeansException {
               initializeAdvisorChain();// 为代理对象配置Advisor通知器链：里面有获取切点和通知的方法！！
               if (isSingleton()) {// 单例
                  return getSingletonInstance();
               }
               else {
                 if (this.targetName == null) {
                        logger.warn("Using non-singleton proxies with singleton targets is often undesirable. " +
                        "Enable prototype proxies by setting the 'targetName' property.");
                 }// 非单例
                return newPrototypeInstance();
               }
            }
           getSingletonInstance()方法里面最重要的一行：getProxy(createAopProxy());先创建AOP，再获取代理。createAopProxy()这个方法，如果目标类含有接口，则创建JdkDynamicAopProxy，否则
           创建ObjenesisCglibAopProxy，最后么，就是获取代理对象。
           	protected Object getProxy(AopProxy aopProxy) {
		               return aopProxy.getProxy(this.proxyClassLoader);
	          }
	          总结：起最重要作用的还是 ProxyFactoryBean ，在定制化Bean的过程中起到了很大的作用，也提醒了我们，如果想在Spring的bean容器实现一些特别的功能，可以实现 FactoryBean 接口，自定义
	               自己的需要Bean。

           2、注解方式
           基于XML：扩展FactoryBean接口（ProxyFactoryBean），对特定的bean进行增强。
           接口Pointcut里面有个MethodMatcher用于决定对该方法是否进行增强。
           接口Advice是一个标识，什么都没有定义，但是我们常用的几个接口，比如 BeforeAdvice，AfterAdvice，都是继承自它。
           接口Advisor(通知器)结合Pointcut接口和Advice，里面最重要的两个方法：getAdvice，getPointcut。
           接下来，我们可以停下来思考一下，现在有了这个东西，我们怎么实现面向切面编程； 
              1. 首先我们需要告诉AOP在哪里进行切面。比如在某个类的方法前后进行切面。 
              2. 告诉AOP 切面之后做什么，也就是说，我们知道了在哪里进行切面，那么我们也该让spring知道在切点处做什么。 
              3. 我们知道，Spring AOP 的底层实现是动态代理（不管是JDK还是Cglib），那么就需要一个代理对象，那么如何生成呢？
              通过ProxyFactoryBean生成代理对象createAopProxy，里面使用默认的DefaultAopProxyFactory生成AopProxy（继承结构：一个是 JDK 动态代理，一个是 Cglib 代理），
        
           基于注解：扩展BeanPostProcessor(后置处理器接口)接口，对特定的bean进行增强。
           public interface BeanPostProcessor {
           Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException;
           Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException; // 会在bean初始化后，调用此实现方法，createProxy创建代码对象
           }
           AnnotationAwareAspectJAutoProxyCreator（这个类就是根据注解创建代理的默认类）的抽象父类实现BeanPostProcessor中的postProcessAfterInitialization方法，
           ProxyFactory proxyFactory = new ProxyFactory(); // ProxyFactory和ProxyFactoryBean一样继承了 ProxyCreatorSupport!! 底层
           return proxyFactory.getProxy(getProxyClassLoader());
           
           补充：基于xml和基于注解在AbstractBeanFactory 的 doGetBean 方法开始分道扬镳，走向了不同的逻辑。
           // 基于XML扩展的是FactoryBean 基于注解扩展的BeanPostProcessor接口
           if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) {
			         return beanInstance;
		       }
          总结：
             首先，通过分析源码我们知道注解方式和 XML 配置方式的底层实现都是一样的，都是通过继承 ProxyCreatorSupport 来实现的，不同的通过扩展不同的 Spring 提供的接口，XML 扩展的是FactoryBean 接口， 
             而注解方式扩展的是 BenaPostProcessor 接口，通过Spring 的扩展接口，能够对特定的Bean进行增强。而 AOP 正式通过这种方式实现的。这也提醒了我们，我们也可以通过扩展 Spring 的某些接口来增强我们
             需要的 Bean 的某些功能。当然，篇幅有限，我们这篇文章只是了解了XML 配置方式和注解方式创建代理的区别，关于如何 @Aspect 和 @Around 的底层实现，还有通知器的底层实现，我们还没有分析，但我们隐隐
             的感觉到，其实万变不离其宗，底层的也是通过扩展 advice 和 pointcut 接口来实现的。 我们将会在后面的文章继续分析 AOP 是如何编织通知的。

           3、SpringBoot Aop 切面编织过程和代理执行过程
              3.1、AnnotationAwareAspectJAutoProxyCreator后置处理器注册过程，上文已经介绍过这个类了， 他继承了 AbstractAutoProxyCreator 抽象类，该类实现了后置处理器接口的方法。
              实现BeanPostProcessor。我们要看下在哪里有注册后置处理器的逻辑-》ioc容器初始化会调用refresh() 方法，
              // 注册Bean的后处理器，在Bean创建过程中调用
              registerBeanPostProcessors(beanFactory);
              // 将后置处理器添加进工厂
              beanFactory.addBeanPostProcessor(postProcessor);
              3.2、标注 @Aspect 注解的 bean 的后置处理器的处理过程：@Aspec 同时也会使用类似 @Component 的注解，表示这是一个Bean，只要是bean，就会调用getBean方法，只要调用 getBean 方法，
                   都会调用 AbstractAutowireCapableBeanFactory 的 createBean 方法
                   // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance.返回一个代理
			             Object bean = resolveBeforeInstantiation(beanName, mbdToUse);
			             // 实例化前方法  进入该方法拿出在之前注册在BeanFactory 的后置处理器，并循环调用他们的 postProcessBeforeInstantiation 方法  我们关注的是AbstractAutoProxyCreator
			             // 后置处理器，
			             bean = applyBeanPostProcessorsBeforeInstantiation(targetType, beanName);
					         if (bean != null) {
					         // 初始化后方法
						       bean = applyBeanPostProcessorsAfterInitialization(bean, beanName);
					         }
					     3.3、创建代理的过程，在执行完 resolveBeforeInstantiation 返回 null 之后，就执行 doCreateBean 方法
					          Object beanInstance = doCreateBean(beanName, mbdToUse, args);
					          // 依赖注入工作
					          populateBean(beanName, mbd, instanceWrapper);
									  if (exposedObject != null) {
									  	    // 初始化bean
													exposedObject = initializeBean(beanName, exposedObject, mbd);
									  }
									  上面的initializeBean逻辑是一个非常重要的，在里面调用了3个重要逻辑：applyBeanPostProcessorsBeforeInitialization、invokeInitMethods、applyBeanPostProcessorsAfterInitialization
									  首先调用了前置处理器的 postProcessBeforeInitialization 方法，简单的返回了bean。 
									  然后判断是否实现 InitializingBean 接口从而决定是否调用 afterPropertiesSet 方法。
									  最后到了最重要的 applyBeanPostProcessorsAfterInitialization 方法， 该方法会循环调用每个bean的后置处理器的 postProcessAfterInitialization 方法。
	// 重写了BeanPostProcessor接口的方法 注意区分，BeanPostProcessor 是初始化，InstantiationAwareBeanPostProcessor 是实例化，实例化先执行，初始化后执行。
	@Override
	public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException {
		if (bean != null) {
			Object cacheKey = getCacheKey(bean.getClass(), beanName);
			if (!this.earlyProxyReferences.contains(cacheKey)) {
				return wrapIfNecessary(bean, beanName, cacheKey);
			}
		}
		return bean;
	}
	                 然后直接进入wrapIfNecessary方法，再经过一系列的判断，进入
	                 // 生成代理
	                 Object proxy = createProxy(
					                bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean));
					         小总结来一波：我们从整个流程可以看到，AOP的编织是通过定义@Aspect 和 @Around 等注解创建通知器，和我们在XML中编程几乎一样，只是Spring 向我们屏蔽了底层。最后，Spring 
					                     拿着目标类和通过器去创建代理。
					    3.4、目标对象方法调用的过程。
					         applicationContext.getBean("....")此时返回的就是一个代理对象，在执行该代理对象的方法时，代理类内部回调了 CglibAopProxy 的内部类 DynamicAdvisedInterceptor 的 
					         intercept 方法。。。。。。。
					  
					  4、深入理解Spring 之 Spring 进阶开发必知必会 之 Spring 扩展接口
					     4.1、FactroyBean，上文已经讲过，spring使用此接口构建AOP，ProxyFactoryBean就是实现了该接口（XML的方式）。
					     4.2、BeanPostProcess 在每个bean初始化成前后做操作。spring用注解的方式构建AOP就是用了扩展BeanPostProcess的方式。
					          public interface BeanPostProcessor {
					          	Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException;
					          	// 注解方式的AOP的实现就是在 postProcessAfterInitialization 方法中实现的
					          	Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException;
					          }
					          我们可以自定义一个实现类，实现上述两个方法，项目中所有的Bean在初始化的时候都调用该方法。因此，我们在以后的开发中就可以做一些自定义的事情。
					     4.3、InstantiationAwareBeanPostProcessor 在Bean实例化前后做一些操作。继承BeanPostProcess，新增三个方法，分别是 postProcessBeforeInstantiation （实例化之前），
					          postProcessAfterInstantiation （实例化之后），postProcessPropertyValues （在处理Bean属性之前），开发者可以在这三个方法中添加自定义逻辑，比如AOP。
					     4.4、BeanNameAware、ApplicationContextAware 和 BeanFactoryAware 针对bean工厂，可以获取上下文（BeanFactory、ApplicationContext），可以获取当前bean的id。
					     4.5、BeanFactoryPostProcessor Spring允许在Bean创建之前，读取Bean的元属性，并根据自己的需求对元属性进行改变，比如将Bean的scope从singleton改变为prototype。
               4.6、InitialingBean 在属性设置完毕后做一些自定义操作。 DisposableBean 在关闭容器前做一些操作。
               总结：我们了解了 Spring 留给我们的扩展接口，以提高我们使用 Spring 的水平，在以后的业务中，也就可以基于 Spring 做一些除了简单的注入这种基本功能之外的功能。同时，我们也发现，
                    Spring 的扩展性非常的高，符合设计模式中的开闭原则，对修改关闭，对扩展开放，实现这些的基础就是 Spring 的 IOC，IOC 可以说是 Spring 的核心， 在 IOC 的过程中，对预定义
                    的接口做了很多的预留工作。这让其他框架与 Spring 的组合变得更加的简单，我们在以后的开发工作中也可以借鉴 Spring 的思想，让程序更加的优美。
              5、深入理解 Spring 之 SpringBoot 事务原理
                 5.1、BookShopService#purchase方法上面已经@Transactional，BookShopService实例对象其实已经被Cglib 代理了，那么他肯定会走 DynamicAdvisedInterceptor 的 intercept 
                     方法，该方法最重要的事情就是执行通知器或者拦截器的方法，那么该代理有拦截器吗？显然是有的，debug发现该拦截器是TransactionInterceptor，这个类实现了Advice MethodInterceptor
                     的invoke方法，我们来看看TransactionInterceptor的invoke方法做了啥》》》》》
                 5.2、第一步获取事务属性，通过注解解析器解析Method 对象是否含有注解@Transactional》》》》
                 5.3、@Transactional 注解解析器 SpringTransactionAnnotationParser如何解析注解的呢，返回了一个 RuleBasedTransactionAttribute 对象。有了事务属性，再获取事务管理器。
                      也就是 determineTransactionManager 方法。》》》》
                 5.4、第二步获取事务管理器，final PlatformTransactionManager tm = determineTransactionManager(txAttr); DataSourceTransactionManager里面方法：getDataSouce
                      doCommit doRollback.........
    protected void doCommit(DefaultTransactionStatus status) {
		DataSourceTransactionObject txObject = (DataSourceTransactionObject) status.getTransaction();
		Connection con = txObject.getConnectionHolder().getConnection();
		if (status.isDebug()) {
			logger.debug("Committing JDBC transaction on Connection [" + con + "]");
		}
		try {
			con.commit();
		}
		catch (SQLException ex) {
			throw new TransactionSystemException("Could not commit JDBC transaction", ex);
		}
	  }
	                     获得事务管理器后，继续往下走。。。底层也是调用 JDBC 的 Connection 的 rollback 方法。
	  try {
				// 该方法会执行下一个通知器的拦截方法（如果有的话），最后执行目标方法，
				retVal = invocation.proceedWithInvocation();
			}
			catch (Throwable ex) {
				// 目标方法发生异常，被try住
				completeTransactionAfterThrowing(txInfo, ex);
				throw ex;
			}
			finally {
				cleanupTransactionInfo(txInfo);
			}
			// 目标方法成功执行后，事务提交
			commitTransactionAfterReturning(txInfo);
			            5.5、TransactionInterceptor 的 completeTransactionAfterThrowing 方法（事务如何回滚）
		// 该事务对象是否和该异常匹配，如果匹配，则回滚，否则提交。
		if (txInfo.transactionAttribute.rollbackOn(ex)) {
				try {
					txInfo.getTransactionManager().rollback(txInfo.getTransactionStatus());
				}
				catch (TransactionSystemException ex2) {
					。。
				}
				。。。
			}
			else {
				// We don't roll back on this exception.
				// Will still roll back if TransactionStatus.isRollbackOnly() is true.
				try {
					txInfo.getTransactionManager().commit(txInfo.getTransactionStatus());
				}
				catch (TransactionSystemException ex2) {
					。。
				}
				。。
			}
                 5.5、TransactionInterceptor 的 commitTransactionAfterReturning 方法（事务如何提交）
                      底层也是调用 JDBC 的 Connection 的 commit 方法。
                 5.6、事务运行之前做了哪些工作？事务管理器从何而来？ TransactionAttributeSource 属性何时生成？AnnotationTransactionAttributeSource 构造什么时候调用？
                      在Spring 中，有一个现成的类，ProxyTransactionManagementConfiguration
                 5.7、mybatis 和 Spring 的事务管理权力之争
                      判断Spirng 持有的 SqlSession 和 Mybatis 持有的是否是同一个，如果是，则交给Spring，否则，Mybatis 自己处理。可以说很合理。
                         
                 总结：整个事务其实是建立在AOP的基础之上，其核心类就是 TransactionInterceptor，该类就是 invokeWithinTransaction 方法是就事务处理的核心方法，其中封装了我们创建的 
                      DataSourceTransactionManager 对象，该对象就是执行回滚或者提交的执行单位 其实，TransactionInterceptor 和我们平时标注 @Aspect 注解的类的作用相同，就是拦截指定
                      的方法，而在TransactionInterceptor 中是通过是否标有事务注解来决定的。如果一个类中任意方法含有事务注解，那么这个方法就会被代理。而Mybatis 的事务和Spring 的事务协作
                      则根据他们的SqlSession 是否是同一个SqlSession 来决定的，如果是同一个，则交给Spring，如果不是，Mybatis 则自己处理。


                      



spring事务如何实现????   http://blog.csdn.net/bazingaea/article/details/53133620 
（1）编程式事务管理：需要手动编写代码，在实际开发中很少使用
（2）spring声明式事务管理：建立在AOP之上的，其本质就是在方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。
（2.1）基于TransactionProxyFactoryBean的方式，需要为每个进行事务管理的类做相应配置
（2.2）基于AspectJ的XML方式，不需要改动类，在XML文件中配置好即可。
<!-- 配置事务管理器 -->
<bean id="transactionManager" 
	class="org.springframework.jdbc.datasource.DataSourceTransactionManager">
	<property name="dataSource" ref="dataSource"/>
</bean>

<!-- 配置的事务的通知 -->
<tx:advice id="txAdvice" transaction-manager="transactionManager">
	<tx:attributes>
		<!-- 
		propagation:事务传播行为
		isolation:事务隔离级别
		read-only:只读
		rollback-for:发生哪些异常回滚
		no-rollback-for:发生哪些异常不回滚
		timeout:有效期
		 -->
		<tx:method name="transfer" propagation="REQUIRED"/>
	</tx:attributes>
</tx:advice>

<!-- 配置切面 -->
<aop:config>
	<aop:pointcut expression="execution(* demo3.AccountService.*(..))" id="pointcut1"/>
	<aop:advisor advice-ref="txAdvice" pointcut-ref="pointcut1"/>
</aop:config>
（2.3）基于注解的方式，配置简单（<!-- 配置事务管理器 -->  <!-- 开启注解事务 -->  ），需要在业务层类中添加注解
（2.2）和（2.3）在开发中使用比较多，前者配置一目了然，可以在XML文件中得到所有信息，后者配置简单方便
(2.3)
<!-- 配置事务管理器 -->  
<bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">  
    <property name="dataSource" ref="dataSource"/>  
</bean>        
<!-- 开启注解事务 -->  
<tx:annotation-driven transaction-manager="transactionManager"/>  

    propagation:事务传播行为。
                PROPAGATION_REQUIRED：如果当前存在事务，那么加入事务，不存在事务，那么新建一个事务。这个是默认值。
                PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前事务存在，则把当前事务挂起。
                PROPAGATION_SUPPORTS：如果当前存在事务，则加入事务，如果当前没有事务，则以非事务的方式运行。
                PROPAGATION_NOT_SUPPORTED：以非事务的方式运行，如果当前存在事务，则把当前事务挂起。
                PROPAGATION_NEVER：以非事务的方式运行，如果当前存在事务，则抛出异常。
                PROPAGATION_MANDATORY（强制的）：如果当前存在事务，那么加入事务，如果当前没有事务，则抛出异常。
                PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。
                              
		isolation:事务隔离级别，默认为数据库的隔离级别。读未提交、读已提交（一般的数据默认值）、可重复读、序列化。
		read-only:只读，默认值为false，代码只读，不修改数据的情况下，设置为true可以帮助数据库引擎优化事务。
		rollback-for：发生哪些异常回滚，默认是针对unchecked exception回滚。也就是默认对RuntimeException()异常或是其子类进行事务回滚；
		             如果事务被try catch了 则不进行回滚 try catch(){throw ...}异常可以选择往外抛就会回滚
		             检查型异常（Checked Exception）与非检查型异常（Unchecked Exception）：前者编译器通不过，必须try catch，不然编译失败   后者编译器不会去检查，可以不用try catch
		no-rollback-for:发生哪些异常不回滚
		timeout：事务超时，允许一个事务所执行的时间，超过这个时间，事务没有执行完，则自动回滚事务。默认设置为底层事务系统的超时值，如果底层数据库事务系统没有设置超时值，那么就是none，没有超时限制。





二、IOC:
为了解决对象之间的耦合度过高的问题!!!!Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。
我们可以把IOC容器的工作模式看做是工厂模式的升华，可以把IOC容器看作是一个工厂，这个工厂里要生产的对象都在配置文件中给出定义，然后利用编程语言的的反射编程，根据配置文件中给出的类名生成相应的对象。
从实现来看，IOC是把以前在工厂方法里写死的对象生成代码，改变为由配置文件来定义，也就是把工厂和对象生成这两者独立分隔开来，目的就是提高灵活性和可维护性。
原理：底层就是java的反射。给定一个字符串能创建一个实例，利用set方法对实例的依赖进行注入。

IOC概念和DI概念：调用者不负责被调用者实例的创建，而是由spring容器来创建，控制权交给了外部容器
控制权发生了反转，因此叫做控制反转。spring容器负责被调用者实例的创建，实例创建后又负责将实例注入调用者，因此成为依赖注入（构造器注入、Setter方法注入、接口注入）。
BeanFactory与ApplicationContext：BeanFactory（IOC的核心接口）是spring容器的顶层接口，提供了容器最基本的功能（getbean、bean的生命周期的管理包括初始化、销毁）。
而ApplicationContext添加了更多高级的功能（支持国际化、统一的资源文件读取方式、针对Web应用的WebApplicationContext等等），另外
ApplicationContext容器初始化完成后，容器中所有singleton bean 也被实例化了，getbean的时候，速度会更快。
BeanDefinition：IOC的基本数据结构。我们知道，每个bean都有自己的信息，各个属性，类名，类型，是否单例，这些都是bena的信息，spring中如何管理bean的信息呢？对，就是 BeanDefinition， Spring通过
定义 BeanDefinition 来管理基于Spring的应用中的各种对象以及他们直接的相互依赖关系。BeanDefinition 抽象了我们对 Bean的定义，是让容器起作用的主要数据类型。对 IOC 容器来说，BeanDefinition 
就是对依赖反转模式中管理的对象依赖关系的数据抽象。也是容器实现依赖反转功能的核心数据结构。
beanfactory和factorybean的区别：前者是一个工厂（容器），是管理bean的一个工厂。
                              一般情况下，spring通过反射机制利用<bean>的class属性指定实例化bean，在某些情况下，实例化bean过程比较复杂，如果按照传统的方式，则需要在 <bean> 中提供大量的配置信息。
                              这个时候就可以采用编码的方式，spring提交了一个接口FactoryBean<T>，用户可以通过实现该接口定制实例化 Bean 的逻辑。
                              它就是一个bean，也是被beanfactory容器所管理，与普通bean不一样的地方在于，根据bean的id获取到的bean并不是factorybean本身，而是FactoryBean的getObject()返回的对象。
依赖注入的集中方式：set注入（必须要有set方法）、构造器注入（利用构造方法）、工厂方法注入、注解的方式注入（byName、byType）                         
spring注解：
          Autowired是自动注入，自动从spring的上下文找到合适的bean来注入，byType
          Resource用来指定名称注入
          Qualifier和Autowired配合使用，指定bean的名称
          Service，Controller，Repository分别标记类是Service层类，Controller层类，数据存储层的类，spring扫描注解配置时，会标记这些类要生成bean。
          Component是一种泛指，标记类是组件，spring扫描注解配置时，会标记这些类要生成bean。
          @Autowired//默认按type注入
          @Qualifier("cusInfoService")//一般作为@Autowired()的修饰用 场景是可能存在多个实例
          @Resource(name="cusInfoService")//默认按name注入，可以通过name和type属性进行选择性注入
          @Autowired(required=true)// 一定要找到匹配的Bean，否则抛异常。 默认值就是true 
          @Scope("prototype")// 该注解可以使每次使用bean时都实例化一个新的对象 另外还有singleton,prototype,session,request,session,globalSession




IOC源码分析：刷新容器refresh是初始化容器的最重要方法。在refresh方法中完成的事情：
           1、首先构建BeanFactory，在obtainFreshBeanFactory()里面构建，默认实现是DefaultListableBeanFactory。
           1.1、创建一个XmlBeanDefinitionReader进行loadBeanDefinitions
           1.2、构建Document对象，BeanDefinitionDocumentReader对象 进行registerBeanDefinitions
           1.3、解析XML 文档，创建BeanDefinition 创建BeanDefinitionHolder
           。。
           	
           最后一步：
           	      将beanName和beanDefinition放进一个 ConcurrentHashMap（256）中 private final Map<String, BeanDefinition> beanDefinitionMap = new ConcurrentHashMap<String, BeanDefinition>(256);
           2、创建Bean实例并构建Bean的依赖关系网：我们刚刚创建了Bean工厂，并创建 BeanDefinitions 放进Map里，以beanName为key。那么我们现在有了Bean定义，但还没有实例，也没有构建Bean与Bean之间的依赖关系。
           2.1、

总结一下 IOC 的初始化过程吧: 
1. 资源(Resource)定位; 
2. BeanDefinition 的载入（从xml中读取配置文件，解析成BeanDefinition）和 BeanFactory 的构造. 
3. 向 IOC 容器(BeanFactory)注册 BeanDefinition. map.put(beanname,beandefinition)
4. 根据 lazy-init 属性初始化 Bean 实例和依赖注入.依赖注入的前提是容器中的BeanDefinition数据已经建立好。
补充：lazy-init 默认为false立即加载，设置为true延迟加载，第一次向容器通过getBean索取bean时实例化的。ApplicationContext实现的默认行为就是在启动服务器时将所有singleton bean提前进行实例化
(也就是依赖注入)，当真正的请求bean的时候，其实只是从缓存中读取而已。依赖注入发生在容器执行refresh的过程中，即IoC容器初始化的过程中。

ioc容器初始化后，依赖注入发生的时间：
(1).用户第一次通过getBean方法向IoC容索要Bean时，IoC容器触发依赖注入。
(2).当用户在Bean定义资源中为<Bean>元素配置了lazy-init属性，即让容器在解析注册Bean定义时进行预实例化，触发依赖注入。

依赖注入其实包括两个主要过程：
生产Bean所包含的Java对象
Bean对象生成之后,把这些Bean对象的依赖关系设置好

自己实现一个IOC容器？？？https://blog.csdn.net/qq_38182963/article/details/78724305
1、设计组件：BeanFactory 容器，BeanDefinition Bean的基本数据结构，当然还需要加载Bean的资源加载器。大概最后最重要的就是这几个组件。容器用来存放初始化好的Bean，BeanDefinition 
           就是Bean的基本数据结构，比如Bean的名称，Bean的属性 PropertyValue，Bean的方法，是否延迟加载，依赖关系等。资源加载器就简单了，就是一个读取XML配置文件的类，读取每个标签并解析。
2、设计接口：BeanFactory两个方法getBean和registerBeanDefinition
           BeanDefinition实体类，字段bean beanClass bean的属性 bean 的类全限定名称
           BeanDefinitionReader接口，从XML中读取配置文件， 解析成 BeanDefinition，AbstractBeanDefinitionReader实现BeanDefinitionReader，XmlBeanDefinitionReader继承AbstractBeanDefinitionReader
                                    创建一个资源管理器ResourceLoader，readxml，把读取到的内容封装到beanDefinition，最后注册到容器中（hashmap,key是beanname,value是beanDefinition）。
           ------    华丽的分界线  ----  以上只是从XML读取解析beanDefinition  ----   以下是初始化我们需要的Bean（不是Bean定义）并且实现依赖注入--------------
           AbstractBeanFactory实现BeanFactory，getBean和registerBeanDefinition里面都有doCreate方法（真正生成bean），利用反射创建bean，并且还需要对该对象进行属性注入，如果属性是 ref
           类型，那么既是依赖关系，则需要调用 getBean 方法递归的去寻找那个Bean。
           
三、常见note题目
你怎样定义类的作用域?
当定义一个<bean> 在Spring里，我们还能给这个bean声明一个作用域。它可以通过bean 定义中的scope属性来定义。如，当Spring要在需要的时候每次生产一个新的bean实例，bean的scope属性被指定为prototype。
另一方面，一个bean每次使用的时候必须返回同一个实例，这个bean的scope 属性 必须设为 singleton。
解释Spring支持的几种bean的作用域?
Spring框架支持以下五种bean的作用域：
singleton : bean在每个Spring ioc 容器中只有一个实例。
prototype：一个bean的定义可以有多个实例。
request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。
session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。
global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。
缺省的Spring bean 的作用域是Singleton.

Spring框架中的单例bean是线程安全的吗?
不，Spring框架中的单例bean不是线程安全的。大部分的Spring bean并没有可变的状态(比如Service类和DAO类)，所以在某种程度上说Spring的单例bean是线程安全的。开发者自行保证线程安全。最浅显的解决办法
就是将多态bean的作用域由“singleton”变更为“prototype”。只有无状态的Bean才可以在多线程环境下共享，Spring使用ThreadLocal解决线程安全问题。

解释Spring框架中bean的生命周期?
Spring容器 从XML 文件中读取bean的定义，并实例化bean。
Spring根据bean的定义填充所有的属性。
如果bean实现了BeanNameAware 接口，Spring 传递bean 的ID 到 setBeanName方法。
如果Bean 实现了 BeanFactoryAware 接口， Spring传递beanfactory 给setBeanFactory 方法。
如果有任何与bean相关联的BeanPostProcessors，Spring会在postProcesserBeforeInitialization()方法内调用它们。
如果bean实现IntializingBean了，调用它的afterPropertySet方法，如果bean声明了初始化方法，调用此初始化方法。
如果有BeanPostProcessors 和bean 关联，这些bean的postProcessAfterInitialization() 方法将被调用。
如果bean实现了 DisposableBean ，它将调用destroy()方法。

before 实例化 testController InstantiationAwareBeanPostProcessor
after 实例化 testController  InstantiationAwareBeanPostProcessor
before 初始化 testController BeanPostProcessor
afterpropertiesset...       InitializingBean
after 初始化 testController  BeanPostProcessor
destory...                  DisposableBean

总结：
Spring Bean的生命周期简单易懂。在一个bean实例被初始化时，需要执行一系列的初始化操作以达到可用的状态。同样的，当一个bean不在被调用时需要进行相关的析构操作，并从bean容器中移除。
Spring bean factory 负责管理在spring容器中被创建的bean的生命周期。Bean的生命周期由两组回调（call back）方法组成。
 初始化之后调用的回调方法。
 销毁之前调用的回调方法。
InitializingBean和DisposableBean回调接口
针对特殊行为的其他Aware接口
Bean配置文件中的Custom init()方法和destroy()方法
@PostConstruct和@PreDestroy注解方式

哪些是重要的bean生命周期方法？ 你能重载它们吗？
有两个重要的bean 生命周期方法，第一个是setup ， 它是在容器加载bean的时候被调用。第二个方法是 teardown 它是在容器卸载类的时候被调用。
The bean 标签有两个重要的属性（init-method和destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）。
补充：spring中afterPropertiesSet方法与init-method：使用afterPropertiesSet，必须实现InitializingBean，完成该bean的所有赋值后，会进行调用afterPropertiesSet方法。init-method,该bean在
初始化完成后，调用配置的方法。

Spring 框架中都用到了哪些设计模式？
Spring框架中使用到了大量的设计模式，下面列举了比较有代表性的：
代理模式―在AOP和remoting中被用的比较多。
单例模式―在spring配置文件中定义的bean默认为单例模式。
模板方法―用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。
前端控制器―Spring提供了DispatcherServlet来对请求进行分发。
视图帮助(View Helper )―Spring提供了一系列的JSP标签，高效宏来辅助将分散的代码整合在视图里。
依赖注入―贯穿于BeanFactory / ApplicationContext接口的核心理念。
工厂模式―BeanFactory用来创建对象的实例。

@Required注解？
用在set方法上，一旦用了这个注解，那么容器在初始化bean的时候必须要进行set，也就是说必须对这个值进行依赖注入。

IoC 和 DI的区别？
IoC 控制反转，指将对象的创建权，反转到Spring容器 ， DI 依赖注入，指Spring创建对象的过程中，将对象依赖属性通过配置进行注入，让相互协作的几个类保持松散耦合。


1、2 springboot
添加start依赖：依赖更加简洁，依赖版本互相兼容；
自动配置；减少spring配置的数量。利用了spring4对条件化配置的支持。
groovy编写程序，并且使用springboot cli运行程序；
actuator；maven中添加spring-boot-actuator;查看管理端点：比方说列出运行所配置的bean，查看自动配置情况，列出应用的线程，展现当前应用的健康状况。        


-----------------------------------------------------------------------------

2、mysql
索引数据结构（适合索引字段，不适合索引字段 大数据查询优化） 
二叉树：第一个节点的选择很重要，如果插入的数据很有连续性的话，并没有起到二分查找的作用，比方连续插入1、2、3、4、5、6，就变成了一条直线；
平衡二叉树：解决了二叉树插入连续性的问题，但是还有其他缺点，太深了（高度太高，磁盘IO耗时大），太小了（磁盘块即节点存的数据很少，没有利用磁盘的预读能力即局部性原理）；
B树和B+树：解决了高度问题，磁盘预读问题。另外他们之间的区别，B+树关键字搜索采用闭合区间，B+树的非叶子节点只保存关键字（没有存真实数据的地址信息）和子节点的引用（非叶子节点可以存很多关键字，使高度更低，IO操作更少），
B+树关键字对应的数据存在叶子节点中，B+树叶子节点是顺序排列，并且相邻节点具有顺序引用的关系；
为什么使用B+树：
B+树是B-树的变种(PLUS版)多路绝对平衡查找树，他拥有B-树的优势 
B+树扫库、表能力更强
B+树的磁盘读写能力更强
B+树的排序能力更强 B+树的查询效率更加稳定(仁者见仁、智者见智)

B树补充：
B树结构可以显著减少定位记录所经历的中间过程，从而加快存取速度，（B树比二叉树的高度低，从而减少磁盘IO操作次数）。
索引本身也很大，不可能全部存储在内存中，以索引文件的形式存储在磁盘上，评价一个数据结构作为索引的优劣最重要的指标就是查找过程中磁盘IO操作的次数。
B树的设计考虑磁盘预读取这点，一个B树的结点通常和一个完整磁盘页（page）一样大，这样每个节点只需要一次IO就可以完全载入。
磁盘预读：为了尽量减少IO操作，磁盘读取每次都会预读，大小通常为页的整数倍。即使需要读取一个字节，磁盘也会读取一页的数据放入内存，内存与磁盘以页为
单位交换数据。因为局部性原理，通常一个数据被用到，其附近的数据约会立马被用到。

B+树在myisam的体现形式和innodb不一样的地方：
innodb是以主键索引来组织数据的存储（数据库表行中的顺序是和索引顺序相同），索引和数据都是存在一个文件中（IBD），用主键索引可以直接定位，用辅助索引先定位查到这条记录的主键，然后再用主键索引定位记录；
myisam索引和数据存的是两个文件，分别是myi和myd，主键索引和辅助索引定位找到这个关键字，然后根据存储的记录地址，去myd查询记录；

索引知识补充：
列的离散性：越大越适合做索引，比方说性别：男、女，离散性很差，即使加了索引，存储引擎会执行计划选择不通过索引查询；
最左匹配原则；
联合索引：1、经常用的列优先 【最左匹配原则】 2，选择性(离散度)高的列优先【离散度高原则】 3，宽度小的列优先【最少空间原则】一个磁盘块（节点）可以放更多的关键字；
覆盖索引：如果查询列可以直接通过节点中的关键字返回，即为覆盖索引。优点：覆盖索引可减少数据库IO，将随机IO变为顺序IO，可提高查询性能；
索引列的数据长度能少则少。 
索引一定不是越多越好，越全越好，一定是建合适的。
匹配列前缀可用到索引 like 9999%，like %9999%、like %9999用不到索引; 
Where 条件中 not in 和 <>操作无法使用索引;
匹配范围值，order by 也可用到索引; 
多用指定列查询，只返回自己想到的数据列，少用select *; 
联合索引中如果不是按照索引最左列开始查找，无法使用索引; 
联合索引中精确匹配最左前列并范围匹配另外一列可以用到索引; 
联合索引中如果查询中有某个列的范围查询，则其右边的所有列都无法使用索引;

几大存储引擎：存储引擎是MySql中具体的与文件打交道的子系统
1、innodb比myisam多了支持事务、行级锁、支持外键、聚集索引（主键）的方式进行存储（myisam索引和数据是分开存储的），另外myisam的select count(*) 操作无需进行数据扫描；
2、插拔式的存储引擎，每个表都可以指定自己的存储引擎，cvs存储引擎适合快速导入导出，archive存储引擎（数据压缩）适合存日志数据，大量数据采集或者数据备份，memory存储引擎适合做临时表计算；

MySQL查询执行过程：客户端连接（半双工） - 查询缓存 - 解析器 - 查询优化处理 - 查询存储引擎 - 调用存储引擎的api - 返回客户端
半双工通信: 在任何一个时刻，要么是有服务器向客户端发送数据，要么是客户端向服务端发 送数据，这两个动作不能同时发生。所以我们无法也无需将一个消息切成小块进 行传输；
特点和限制: 客户端一旦开始发送消息，另一端要接收完整个消息才能响应。 客户端一旦开始接收数据没法停下来发送指令。
查询缓存：mysql是默认关闭的，坑点：如果对一个表进行了更改，那么这个表涉及到的缓存都会失效。试用场景：以读为主的场景。
查询优化处理：找到最优的执行计划，调整条件位置、覆盖索引扫描、子查询优化、提前终止查询（limit）、in的优化。执行计划中（explain）的type字段非常重要从好到坏依次是：
system const（索引一次就能找到，主键索引） eq_ref（唯一索引） ref（索引） range（检索给定范围的行） index（索引全表扫描，把索引从头到尾扫描了一遍） all（遍历全表）

每个MySQL连接，或者说是一个线程，时刻都有一个状态标识这个连接正在做什么。
show processlist
Sleep
线程正在等待客户端发送数据
Query
连接线程正在执行查询
Locked
线程正在等待表锁的释放
Sorting result
线程正在对结果进行排序
Sending data
向请求端返回数据
可通过kill {id}的方式进行连接的杀掉

查询优化器如何找到最优的执行计划：1、调整条件位置，利用联合索引；2、min函数和max函数，找索引最右和最左边；3、覆盖索引扫描；4、子查询优化（子查询虽然可以使查询语句很灵活,但执行效率不高，需要建立一张临时表，尽量使用join查询）；5、limit提前终止查询；

定位慢sql查询：SHOW VARIABLES like 'slow_query_log';slow_query_log_file慢查询sql的日志文件 long_query_time超过这个时间算是慢查询
慢查询日志分析工具：MySQLsla 或者   mysqldumpslow -t 10 -s at /var/lib/mysql/gupaoedu-slow.log

事务：数据库操作的最小工作单元，不可分割的操作集合；
原子性(Atomicity) 最小的工作单元，整个工作单元要么一起提交成功，要么全部失败回滚
一致性(Consistency) 事务中操作的数据及状态改变是一致的，即写入资料的结果必须完全符合预设的规则，不会因为出现系统意外等原因导致状态的不一致
隔离性(Isolation) 一个事务所操作的数据在提交之前，对其他事务的可见性设定(一般设定为不可见)
持久性(Durability) 事务所做的修改就会永久保存，不会因为系统意外导致数据的丢失
事务并发带来的问题：
1、脏读：一个事务读取了另外一个事务未提交的数据，一旦回滚，读取的数据是错误的；
2、不可重复读：在同一个事务中，同样的条件，你读取过的数据，再次读取出来发现值不一样了；
3、幻读：在同一个事务中，同样的条件，第1次和第2次读出来的记录数不一样；
四种隔离级别：mysql的默认隔离级别是：可重复读 select @@tx_isolation  innodb引擎的可重复读可以避免幻读
Read Uncommitted(未提交读) --未解决并发问题 事务未提交对其他事务也是可见的，脏读(dirty read)
Read Committed(提交读) --解决脏读问题 一个事务开始之后，只能看到自己提交的事务所做的修改，不可重复读(nonrepeatable read)
Repeatable Read (可重复读) --解决不可重复读问题 在同一个事务中多次读取同样的数据结果是一样的，这种隔离级别未定义解决幻读的问题
Serializable(串行化) --解决所有问题 最高的隔离级别，通过强制事务的串行执行

MySQL锁：共享锁、排他锁、意向锁共享锁（表锁）、意向锁排他锁（表锁）、自增锁（自增字段自增的时候需要用到）
共享锁：读锁，多个事务对同一个数据共享一把锁，只能读不能修改。select ... lock in share mode
排他锁：写锁，排它锁不能和其他锁共享，只有获取排它锁的事务才能对数据行进行读取和修改（其他事务要读取数据可来自快照），delete/insert/update自动加排它锁，select * from xxx where id = xx for update
意向共享锁：一个数据行加共享锁之前必须先获得表的意向共享锁。另类的表锁。数据操作之前自动加的。
意向排他锁：一个数据行加排它锁之前必须先获得表的意向排它锁。另类的表锁。数据操作之前自动加的。
表锁与行锁的区别: innodb的行锁其实是对索引进行加锁，只有通过索引条件进行加锁，innodb才会使用行级锁，不然会使用表级锁。
锁定粒度:表锁 > 行锁
加锁效率:表锁 > 行锁 
冲突概率:表锁 > 行锁 
并发性能:表锁 < 行锁
行锁的算法：
1、记录锁
select * from t where id = 4 for update；id=4这条记录存在，锁住4这条记录。
2、临键锁 next-key（默认）可以解决幻读：这就是innodb可重复读的事务隔离级别解决了幻读问题。
比方说有记录id 1  4  7  10 四条记录  select * from t where id>5 and id<9 for update; 锁住 （4，7】（7，10】   即没释放锁之前，不能插入5，6，8，9，不能更改7，10
3、间隙锁 gap ：记录不存在，临键锁退化成间隙锁
比方说有记录id 1  4  7  10 四条记录  select * from t where id>4 and id<6 for update; 锁住 （4，7）   即没释放锁之前，不能插入5，6 
select * from t where id >10 for update;锁住了大于10的数据行，即没释放锁之前，不能插入10之后的数据。
死锁避免：
1)类似的业务逻辑以固定的顺序访问表和行。 
2)大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。
3)在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概率。
4)降低隔离级别，如果业务允许，将隔离级别调低也是较好的选择
5)为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添 加上锁(或者说是表锁)

MVCC (Multiversion Concurrency Control):
1、即多版本并发控制技术,它使得innodb，不再单纯的使用行锁来进行数据库的并发控制，取而代之的是把数据库的行锁与行的多个版本结合起来，只需要很小的开销,就可以实现非锁定读，从而大大提高数据库系统的并发性能；
2、解决的问题：用来避免写操作的堵塞，从而引发读操作的并发问题。
实现方式，通过给数据行增加两个隐藏列：创建版本号和删除版本号，版本号是怎么来的呢，每次开启一个事务的时候，都会有一个唯一的递增版本号。
undo log:开始事务前，即对数据进行操作前，把要用到的数据进行备份，如果rollback，就可以用到这些备份数据进行回滚到事务前，实现了事务的原子性，还可以防止脏读（事务未提交之前，其他并发事务读取的是备份中的数据）；
         在MySQL innodb中用来实现多版本控制。
redo log:重做，最新的数据备份到一个地方，为了事务的持久化而产生的产物，防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redolog进行重做，从而达到事务的未入磁盘数据进行持久
         化这一特性。

MySQL内存配置：每一个connection的内存配置，sort_buffer_size排序缓存区默认256K，join_buffer_size关联查询缓冲区默认256k,上述配置4000连接占用内存:4000*(0.256M+0.256M) = 2G
Innodb_buffer_pool_size
innodb buffer/cache的大小(默认128M)
Innodb_buffer_pool
     数据缓存
     索引缓存
     缓冲数据
     内部结构
大的缓冲池可以减小多次磁盘I/O访问相同的表数据以提高性能 参考计算公式:
Innodb_buffer_pool_size = (总物理内存 - 系统运行所用 - connection 所用)* 90%
MySQL innodb 数据预热：
因为mysql热点数据都在innodb buffer pool中，如果mysql突然关闭，buffer pool被清空！所有数据都需要从硬盘加载至内存中的buffer pool，当需要某个数据时，才从硬盘拿取,导致速度异常慢！这个时候，我们可以将一些数据
手动加载至buffer pool，称为人为预热！当然，还有一种方式，在mysql正常关闭的时候，将内存中的buffer pool备份值硬盘！在下次开启时，将该备份导入至buffer pool！这是5.6的快速预热特性！前提是正常关闭，如果是突然关
闭或者kill -9关闭，就需要手动预热了！


mysql选择合适的数据类型存储数据对性能有很大的影响：
(1)越小的数据类型通常更好：越小的数据类型通常在磁盘、内存和CPU缓存中都需要更少的空间，处理起来更快。
(2)简单的数据类型更好：整型数据比起字符，处理开销更小，因为字符串的比较更复杂。在MySQL中，应该用内置的日期和时间数据类型，而不是用字符串来存储时间；
   以及用整型数据类型存储IP地址。
(3)尽量避免NULL：应该指定列为NOT NULL，除非你想存储NULL。在MySQL中，含有空值的列很难进行查询优化，因为它们使得索引、索引的统计信息以及比较运算更加复杂。
   你应该用0、一个特殊的值或者一个空串代替空值。
大数据查询优化：
（1、优化sql语句和索引
http://database.51cto.com/art/201407/445934.htm
对查询进行优化，要尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。
应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描，数据库字段尽量不用null，可以使用特殊字符标识。
应尽量避免在 where 子句中使用 != 或 <> 操作符，否则将引擎放弃使用索引而进行全表扫描。
应尽量避免在 where 子句中使用 or 来连接条件，如果一个字段有索引，一个字段没有索引，将导致引擎放弃使用索引而进行全表扫描。
索引并不是越多越好，索引固然可以提高相应的 select 的效率，但同时也降低了 insert 及 update 的效率，因为 insert 或 update 时有可能会重建索引，所以怎样建索引需要慎重考虑，视具体情况而定。
应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。
in 和 not in 也要慎用，否则会导致全表扫描
（2、加缓存，memcached、redis
（3、分库分表（和分区的侧重点不一样，分区是突破磁盘的读写能力，分表是提高mysql并发能力。分库分表后需要注意：事务问题、跨库跨表的join问题、额外的数据管理负担和
     数据运算压力）
	 业务拆分，不同的业务访问不同的数据库，分布式事务问题 、 跨库join 表关联的问题。
（4、使用mongodb进行存储

（5、如果一个表没有事务，插入和查询操作占了很多一部分 ，可以把存储引擎改成myisam

MySQL分区：是指根据一定的规则，数据库把一个表分成多个更小的、更容易管理的部分。分区对于应用来说是完全透明的，不影响应用的业务逻辑。每个分区都是一个独立的对象，可以独立处理。
突破磁盘极限；
查询优化，热点数据归类到一个分区或者多个分区内，查询的时候就不用去查剩下的那些分区，增大查询效率；
涉及到例如SUM()和COUNT()这样聚合函数的查询，可以很容易地进行并行处理；
分区表的更易管理，比如删除过去某一时间的历史数据，直接执行truncate，或者狠点drop整个分区，这比detele删除效率更高；
range分区、list分区、hash分区（你所要做的只是基于将要被哈希的列值指定一个列值或表达式，以及指定被分区的表将要被分割成的分区数量。）、key分区
hash分区：分散热点读，确保数据在各个分区尽可能平均分布。常规HASH分区（取模）和线性HASH分区，因为常规HASH分区的算法是取模，那么新增一个分区的代价非常高，所有的数据都需要重新计算过，所以出现了线性
         HASH分区。


MySQL优化查询案例：https://mp.weixin.qq.com/s/tcEoMlz2rdWLsO5q5lO0gw
sql调优总结：
列类型尽量定义成数值类型，且长度尽可能短，如主键和外键，类型字段等等
建立单列索引
根据需要建立多列联合索引
当单个列过滤之后还有很多数据，那么索引的效率将会比较低，即列的区分度较低，
那么如果在多个列上建立索引，那么多个列的区分度就大多了，将会有显著的效率提高。
根据业务场景建立覆盖索引
只查询业务需要的字段，如果这些字段被索引覆盖，将极大的提高查询效率
多表连接的字段上需要建立索引
这样可以极大的提高表连接的效率
where条件字段上需要建立索引
排序字段上需要建立索引
分组字段上需要建立索引
Where条件上不要使用运算函数，以免索引失效
使用短索引：如果对字符串进行索引，应该指定一个前缀长度，节省索引空间，一个磁盘块放更多的关键字，减少磁盘IO操作次数。
不要过度使用索引：insert update会更改索引。
主键索引选择：最常访问条件的列作为主键。innodb普通索引都包含主键的键值，所以主键要尽可能选择较短的数据类型，有效的减少索引占有磁盘空间。

show profile：再做SQL优化的时候，帮助我们了解时间消耗在哪里。
order by优化：减少using filesort，通过索引直接返回数据，WHERE条件和ORDER BY使用相同的索引。适当加大sort_buffer_siza排序区，尽量让排序在内存中完成，而不是通过创建临时表放在文件中进行。尽量使用select 字段
             而不是select * ，可以减少排序区的使用。
MySQL分页优化：1、使用覆盖索引，主键关联回表查询；2、limit m n 改成limit n，只适合排序字段不会出现重复字段。



2.1、mycat
数据库产生性能瓶颈的原因：数据库连接数、表数据量、硬件资源（磁盘IO）。。。。
解决方案： 读写分离（基础MySQL的主从复制）
         分库分表，对数据的库表进行拆分，用分片的方式对数据进行管理。
            垂直拆分：比如单库拆分成用户库、订单库、账户库。这个是根据功能来进行拆分。
            水平拆分：一般来说，简单的水平切分主要是将某个表再依照某个字段的某种规则来分散到多个表之中。每一个表中包括一部分数据。将单张表的数据切分到多个服务器上去，每个服务器具有相应的库与表，
                    只是表中数据集合不同。 水平分库分表能够有效的缓解单机和单库的性能瓶颈和压力，突破IO、连接数、硬件资源等的瓶颈。
分库分表后引入的问题：
           分布式事务：1.使用分布式事务中间件 2.使用MySQL自带的针对跨库的事务一致性方案(XA),不过性能要比单库的慢10倍左右。3.能否避免掉跨库操作(比如将用户和商品放在同一个库中)
           跨库join问题：粗略的解决方法： 全局表：基础数据，所有库都拷贝一份。 字段冗余：这样有些字段就不用join去查询了。 系统层组装：分别查询出所有，然后组装起来，较复杂。
           横向扩容问题：当我们使用HASH取模做分表的时候,针对数据量的递增,可能需要动态的增加表,此时就需要考虑因为reHash导致数据迁移的问题。
           结果集合并、排序的问题：因为我们是将数据分散存储到不同的库、表里的,当我们查询指定数据列表时,数据来源于不同的子库或者子表,就必然会引发结果集合并、排序的问题。如果每次查询都需要排序、
                               合并等操作,性能肯定会受非常大的影响。走缓存可能一条路!
         
MySQL基于binlog的主从复制原理：
                           1. master将操作记录到二进制日志(binary log)中 (这些记录叫做二进制日志事件，binary log events)
                           2. Slave通过I/O Thread异步将master的binary log events拷贝到它的中继日志(relay log);
                           3. Slave执行relay日志中的事件，匹配自己的配置 将需要执行的数据，在slave服务上执行一遍从 而达到复制数据的目的。

mycat的分片规则：连续分片（按照日期）、离散分片（取模、一致性hash）、综合分片（结合上面两种）。另外一致性hash的优点是扩容的时候迁移数据量比较少，实际节点少，会造成数据不均匀分布，这个时候需要增加
               虚拟节点，默认是160倍，也就是虚拟节点数是物理节点数的160倍。


-------------------------------------------------------------------------------------------

3、hashmap（数组加链表/红黑树） currenthashmap
http://www.importnew.com/10620.html
概念来一波：
size->存放KV的数量（包括数组和链表）、capacity->容量桶的个数，默认为16，扩容一般是2倍，总之是2的幂次方、loadFactor->负载因子，默认0.75、threshold->size大于threshold会resize操作，threshold=capacity*loadFactor
PUT方法：
有一个叫做table大小是16的Node数组。每当往hashmap里面存放key-value对的时候，都会为它们实例化一个Node对象，这个node对象就会存储在前面提到的node数组table中。
现在你一定很想知道，上面创建的node对象将会存放在具体哪个位置（在table中的精确位置,计算方式：length-1 & hash）。答案就是，根据key的hashcode()方法计算出来的hash值（来决定）。
hash值用来计算key在node数组的索引。
如果两个元素有相同的hashcode，他们也会被放到同一个索引上的，问题出现了，该怎么放呢？原来他是以链表（LinkedList）的形式来存储的,如果链表的长度大于8链表就会转变成红黑树（jdk1.8增加的）
如果我们再次放入同样的key会怎样呢？逻辑上，它应该替换老的value。事实上，它确实是这么做的。在迭代的过程中，会调用equals()方法来检查key的相等性(key.equals(k))，
如果这个方法返回true，它就会用当前node的value来替换之前的value。
对key做null检查。如果key是null，会被存储到table[0]，因为null的hash值总是0。
GET方法：
(1 校验table是否为null，长度是否大于0，table是否存在这个key，反之返回null；
(2 table中first的key 进行equal，相等的话，直接进行返回；
(3 first是否有next，没有的话，直接null返回，有next，如果是树节点，根据树的方式进行返回，如果不是树节点，迭代这个链表；
note：
http://www.importnew.com/7099.html
“你知道HashMap的工作原理吗？” “你知道HashMap的get()方法的工作原理吗？”
“当两个对象的hashcode相同会发生什么？” 
“如果两个键的hashcode相同，你如何获取值对象？
“如果HashMap的大小超过了负载因子(load factor)定义的容量，怎么办？”
除非你真正知道HashMap的工作原理，否则你将回答不出这道题。默认的负载因子大小为0.75，
也就是说，当一个map填满了75%的bucket时候，和其它集合类(如ArrayList等)一样，将会创建原来HashMap大小的两倍的bucket数组，来重新调整map的大小，
并将原来的对象放入新的bucket数组中。这个过程叫作rehashing(重新进行计算，因为length变化了)，因为它调用hash方法找到新的bucket位置。
“为什么String, Interger这样的wrapper类适合作为键？” String, Interger这样的wrapper类作为HashMap的键是再适合不过了，而且String最为常用。因为String是不可变的，
也是final的，而且已经重写了equals()和hashCode()方法了。其他的wrapper类也有这个特点。不可变性是必要的，因为为了要计算hashCode()，就要防止键值改变，
如果键值在放入时和获取时返回不同的hashcode的话，那么就不能从HashMap中找到你想要的对象。不可变性还有其他的优点如线程安全。
如果你可以仅仅通过将某个field声明成final就能保证hashCode是不变的，那么请这么做吧。因为获取对象的时候要用到equals()和hashCode()方法，
那么键对象正确的重写这两个方法是非常重要的。如果两个不相等的对象返回不同的hashcode的话，那么碰撞的几率就会小些，这样就能提高HashMap的性能。
“我们可以使用CocurrentHashMap来代替Hashtable吗？”这是另外一个很热门的面试题，因为ConcurrentHashMap越来越多人用了。我们知道Hashtable是synchronized的，
但是ConcurrentHashMap同步性能更好，因为它仅仅根据同步级别对map的一部分进行上锁。ConcurrentHashMap当然可以代替HashTable，但是HashTable提供更强的线程安全性。
"HashMap底层数据结构，如何处理hash冲突，为何HashMap的大小要设置为2的n次幂，为什么IndexFor方法里，需要hash&length-1，为什么HashMap允许null值，resize()过程，多线程下resize为什么会出现死循环，详细解释，结合源码"
"为何HashMap的大小要设置为2的n次幂???"  最主要减少碰撞!!!!
h:为插入元素的hashcode
length:为map的容量大小
&：与操作 比如 1101 & 1011=1001
如果length为2的次幂  则length-1 （二进制全为1）转化为二进制必定是11111……的形式，在于h的二进制与操作效率会非常的快，
而且空间不浪费；如果length不是2的次幂，比如length为15，则length-1为14，对应的二进制为1110，在于h与操作，
最后一位都为0，而0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，
数组可以使用的位置比数组长度小了很多，这意味着进一步增加了碰撞的几率，减慢了查询的效率！这样就会造成空间的浪费
对比
"仔细观察可以发现其实老方法h%length与h&（length-1）得到的结果其实是一个值，但是为什么hashmap中要用后者呢"
1.length（2的整数次幂）的特殊性导致了length-1的特殊性（二进制全为1）
2.位运算快于十进制运算，hashmap扩容也是按位扩容，所以相比较就选择了后者
为什么Hashtable ConcurrentHashmap不支持key或者value为null  而hashmap支持呢???
理解如下：ConcurrentHashmap和Hashtable都是支持并发的，这样会有一个问题，当你通过get(k)获取对应的value时，如果获取到的是null时，你无法判断，它是put（k,v）的时候value为null，还是这个key从来没有做过映射。HashMap是非并发的，可以通过contains(key)来做这个判断。而支持并发的Map在调用m.contains（key）和m.get(key),m可能已经不同了。
resize()过程，多线程下resize为什么会出现死循环，详细解释，结合源码???
  transfer函数其实是在并发情况下导致死循环的因素..因为这里涉及到了指针的移动的过程..!!!!
HashMap jdk1.8之后，为何转链式结构为红黑树，为何长度为8的时候，才转成红黑树???
这里存在一个问题，即使负载因子和Hash算法设计的再合理，也免不了会出现拉链过长的情况，一旦出现拉链过长，则会严重影响HashMap的性能。于是，在JDK1.8版本中，对数据结构做了进一步的优化，引入了红黑树。
而当链表长度太长（默认超过8）时，链表就转换为红黑树，利用红黑树快速增删改查的特点提高HashMap的性能，其中会用到红黑树的插入、删除、查找等算法。
"hashmap为什么是线程不安全的？？？"
个人觉得HashMap在并发时可能出现的问题主要是两方面,首先如果多个线程同时使用put方法添加元素，而且假设正好存在两个put的key发生了碰撞(hash值一样)，那么根据HashMap的实现，这两个key会添加到数组的
同一个位置，这样最终就会发生其中一个线程的put的数据被覆盖。第二就是如果多个线程同时检测到元素个数超过数组大小*loadFactor，这样就会发生多个线程同时对Node数组进行扩容，都在重新计算元素位置以及复
制数据，但是最终只有一个线程扩容后的数组会赋给table，也就是说其他线程的都会丢失，并且各自线程put的数据也丢失。（一个线程正在扩容，另外的线程正在往hashmap中put数据，put的数据很有可能会丢失，扩容会拿到oldtable进行迭代元素，重新计算，放入一个newtable。）
关于HashMap线程不安全这一点，《Java并发编程的艺术》一书中是这样说的：
HashMap在并发执行put操作时会引起死循环，导致CPU利用率接近100%。因为多线程会导致HashMap的Node链表形成环形数据结构，一旦形成环形数据结构，Node的next节点永远不为空，就会在获取Node时产生死循环。

ConcurrentHashMap
ConcurrentHashMap在JDK6和7采用了分段锁的设计，只有在同一个分段内才存在竞争锁的关系，不同的分段锁之间没有锁竞争。
ConcurrentHashMap在JDK8有了很大的改动，数组+链表+红黑树（JDK1.8增加了红黑树部分）
PUT方法：
ConcurrentHashMap最常用的就是put和get两个方法。现在来介绍put方法，这个put方法依然沿用HashMap的put方法的思想，
根据hash值计算这个新插入的点在table中的位置i，如果i位置是空的，直接放进去，否则进行判断，如果i位置是树节点，按照树的方式插入新的节点，否则把i插入到链表的末尾。
ConcurrentHashMap中依然沿用这个思想，有一个最重要的不同点就是ConcurrentHashMap不允许key或value为null值。另外由于涉及到多线程，put方法就要复杂一点。
在多线程中可能有以下两个情况
如果一个或多个线程正在对ConcurrentHashMap进行扩容操作，当前线程也要进入扩容的操作中。这个扩容的操作之所以能被检测到，是因为transfer方法中在空结点上插入
forward节点，(扩容的时候,如果这个位置为空，就在原table中的i位置放入forwardNode节点)如果检测到需要插入的位置被forward节点占有，就帮助进行扩容；
如果检测到要插入的节点是非空且不是forward节点，就对这个节点加锁，这样就保证了线程安全。尽管这个有一些影响效率，但是还是会比hashTable的synchronized要好得多。
整体流程就是首先定义不允许key或value为null的情况放入  对于每一个放入的值，首先利用spread方法对key的hashcode进行一次hash计算，由此来确定这个值在table中的位置。

1、先检查table是否为null，为null的情况下，需要初始化，sizeCtl=-1；
2、位置（计算length-1&hash）上为空，那么直接放入，不需要加锁操作，利用casTabAt操作；
3、位置存在节点，如果这个节点的hash值为-1（forward节点），帮助扩容；
4、位置存在节点，不是forward节点，进行加锁，判断节点类型：
   4.1、链表结构：依次向后遍历，如果key值一样，只需要更新value即可；
   4.2、黑红树：直接调用树节点的插入方法进行插入新的值。

sizeCtl:
负数代表正在进行初始化或扩容操作
-1代表正在初始化
-N 表示有N-1个线程正在进行扩容操作
其余情况： 
1、如果table未初始化，表示table需要初始化的大小。 
2、如果table初始化完成，表示table的容量，默认是table大小的0.75倍，居然用这个公式算0.75（n - (n >>> 2)）。
ForwardingNode：一个特殊的Node节点，hash值为-1，其中存储nextTable的引用。 
只有table发生扩容的时候，ForwardingNode才会发挥作用，作为一个占位符放在table中表示当前节点为null或则已经被移动。
GET方法：
get方法比较简单，给定一个key来确定value的时候，必须满足两个条件  key相同  hash值相同，对于节点可能在链表或树上的情况，需要分别去查找。
总结：
JDK6,7中的ConcurrentHashmap主要使用Segment来实现减小锁粒度，把HashMap分割成若干个Segment，在put的时候需要锁住Segment，get时候不加锁，
使用volatile来保证可见性，当要统计全局时（比如size），首先会尝试多次计算modcount来确定，这几次尝试中，是否有其他线程进行了修改操作，
如果没有，则直接返回size。如果有，则需要依次锁住所有的Segment来计算。

jdk7中ConcurrentHashmap中，当长度过长碰撞会很频繁，链表的增改删查操作都会消耗很长的时间，影响性能,所以jdk8 中完全重写了concurrentHashmap,
代码量从原来的1000多行变成了 6000多 行，实现上也和原来的分段式存储有很大的区别。
我们可以发现JDK8中的实现也是锁分离的思想，只是锁住的是一个Node，而不是JDK7中的Segment，而锁住Node之前的操作是无锁的并且也是线程安全的，
建立在之前提到的3个原子操作上。

主要设计上的变化有以下几点:
1、不采用segment而采用node，锁住node来实现减小锁粒度；
2、设计了MOVED状态，当resize的中过程中，线程2还在put数据，线程2会帮助resize；
3、使用3个CAS操作来确保node的一些操作的原子性，这种方式代替了锁；
4、sizeCtl的不同值来代表不同含义，起到了控制的作用。
5、至于为什么JDK8中使用synchronized而不是ReentrantLock，我猜是因为JDK8中对synchronized有了足够的优化吧。

cas这个算法的基本思想就是不断地去比较（while循坏）当前内存中的变量值与你指定的一个变量值是否相等，如果相等，则接受你指定的修改的值，否则拒绝你的操作。因为当前线程中的值已经不是最新的值，你的修改
很可能会覆盖掉其他线程修改的结果。这一点与乐观锁，SVN的思想是比较类似的。
PS：hashmap的扩容机制？？？
loadfactor(默认为0.75)扩容因子，当数据容量超过当前量最大容量（默认为16）*loadfactor，容量自动扩大2倍，并将当前的数据重新放入新的hashmap中。
所以初始的定义大小为2^n的大小最佳。


PS：arraylist和linkedlist的区别？？？
linkedlist jdk1.8 源码分析  http://blog.csdn.net/qq_19431333/article/details/54572876
arraylist jdk1.8 源码分析    http://blog.csdn.net/u012883858/article/details/51393055
数组结构和双向链表结构；
arraylist:随机查询速度快、
linkedlist:插入和删除速度快
ArrayList和LinkedList在性能上各有优缺点，都有各自所适用的地方，总的说来可以描述如下： 
1．对ArrayList和LinkedList而言，在列表末尾增加一个元素所花的开销都是固定的。对ArrayList而言，主要是在内部数组中增加一项，指向所添加的元素，尔可能会导致对数组重新进行分配；而对LinkedList而言，这个开销是统一的，分配一个内部Entry对象。
2．在ArrayList的中间插入或删除一个元素意味着这个列表中剩余的元素都会被移动；而在LinkedList的中间插入或删除一个元素的开销是固定的。
3．LinkedList不支持高效的随机元素访问。
4．ArrayList的空间浪费主要体现在在list列表的结尾预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗相当的空间
可以这样说：当操作是在一列数据的后面添加数据而不是在前面或中间,并且需要随机地访问其中的元素时,使用ArrayList会提供比较好的性能；
当你的操作是在一列数据的前面或中间添加或删除数据,并且按照顺序访问其中的元素时,就应该使用LinkedList了。

ArrayList不是线程安全的，只能用在单线程环境下，多线程环境下可以考虑用Collections.synchronizedList(List l)函数返回一个线程安全的ArrayList类，也可以使用concurrent并发包下的CopyOnWriteArrayList类。

我们先说说 容器的“快速失败”机制，它是Java集合中的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生fail-fast机制。它是一种迭代器检测bug的机制，比如：我们有线程A和线程B，
线程A通过Iterator访问集合中的元素，某个时刻，线程B修改了集合中的结构，那么程序就会抛出 ConcurrentModificationException 。
final void checkForComodification() {  
            if (modCount != expectedModCount)  
                throw new ConcurrentModificationException();  
        }  
		
PS：hashmap/concurrenthashmap的扩容机制和concurrenthashmap的锁机制源代码需要进行研究下？？？
PS:http://blog.csdn.net/wenyiqingnianiii/article/details/52204136    bucket解释
掌握了上面知识之后，我们可以在创建 HashMap 时根据实际需要适当地调整 load factor 的值；如果程序比较关心空间开销、内存比较紧张，可以适当地增加负载因子；如果程序比较关心时间开销，内存比较宽裕则
可以适当的减少负载因子。通常情况下，程序员无需改变负载因子的值。 

如果开始就知道 HashMap 会保存多个 key-value 对，可以在创建时就使用较大的初始化容量，如果 HashMap 中 Entry 的数量一直不会超过极限容量（capacity * load factor），HashMap 就无需调用 
resize() 方法重新分配 table 数组，从而保证较好的性能。当然，开始就将初始容量设置太高可能会浪费空间（系统需要创建一个长度为 capacity 的 Entry 数组），因此创建 HashMap 时初始化容量设置也
需要小心对待。 

LinkedHashMap？？？
继承了HashMap
大多数情况下，只要不涉及线程安全问题，Map基本都可以使用HashMap，不过HashMap有一个问题，就是迭代HashMap的顺序并不是HashMap放置的顺序，也就是无序。HashMap的这一缺点往往会带来困扰，
因为有些场景，我们期待一个有序的Map--->LinkedHashMap
LinkedHashMap采用的hash算法和HashMap相同，但是它重新定义了Entry。LinkedHashMap中的Entry增加了两个指针 before 和 after，它们分别用于维护双向链接列表。特别需要注意的是，
next用于维护HashMap各个桶中Entry的连接顺序，before、after用于维护Entry插入的先后顺序的，源代码如下：
private static class Entry<K,V> extends HashMap.Entry<K,V> {

    // These fields comprise the doubly linked list used for iteration.
    Entry<K,V> before, after;

    Entry(int hash, K key, V value, HashMap.Entry<K,V> next) {
        super(hash, key, value, next);
    }
    ...
}
ArrayList线程不安全的原因：arraylist默认容量为10，当只有一个可用空间了，两个线程同时进行add操作的时候，满足add的条件（有一个可用空间），进而不会进行扩容操作，那必然会有一个线程会把数据写到边界外，
                        报的错误是，ArrayIndexoutofboundsexception.


------------------------------------------------------------------------------------

4、线程池（使用场景，和多线程的区别）
http://blog.csdn.net/gol_phing/article/details/49032055
作用：如果并发的线程数量很多，并且每个线程都是执行一个时间很短的任务就结束了，这样频繁创建线程就会大大降低系统的效率，因为频繁创建线程和销毁需要时间。
线程池可以让线程复用，就是执行完一个任务，并不被销毁，而是可以继续执行其他的任务。
如何合理配置线程池的大小：如果是CPU密集型任务，就需要尽量压榨CPU，参考值可以设为 NCPU+1；
如果是IO密集型任务，参考值可以设置为2*NCPU；当然，这只是一个参考值，具体的设置还需要根据实际情况进行调整，比如可以先将线程池大小设置为参考值，再观察任务运行情况和系统负载、资源利用率来进行适当调整。
corePoolSize：核心池的大小，线程池中的线程数量大于这个参数，提交的任务会被放进任务缓存队列中；
maximumPoolSize：线程池最大线程数，这个参数也是一个非常重要的参数，它表示在线程池中最多能创建多少个线程；（补救措施，任务量过大的时候的一种补救措施）
keepAliveTime：表示线程没有任务执行时最多保持多久时间会终止，即当线程池中的线程数大于corePoolSize时，如果一个线程空闲的时间达到keepAliveTime，则会终止，直到线程池中的线程数不超过corePoolSize。
总结：
（1）如果当前线程池中的线程数量小于核心池的大小，则每来一个任务，都会创建一个线程执行这个任务；
（2）如果当前线程池中的线程数量大于等于核心池的大小，则每来一个任务，都会将任务添加到一个任务缓存队列中，若添加成功，则该任务会等待空闲线程将其取出去执行；诺添加失败（一般来说是任务队列已经满了），
    则会尝试创建新的线程去执行这个任务；
（3）如果当前线程池中的线程数量达到线程池最大的线程数，则会采取任务拒绝策略进行处理；
（4）如果线程池中的线程数量大于 corePoolSize时，如果某线程空闲时间超过keepAliveTime，线程将被终止，直至线程池中的线程数目不大于corePoolSize；
     如果允许为核心池中的线程设置存活时间，那么核心池中的线程空闲时间超过keepAliveTime，线程也会被终止。
（5）Worker是用来存放工作集，实际上他实现了Runnable 接口，首先执行通过构造器传进去的任务firsttask，执行完后，在通过while循环通过gettask去取新任务来执行；
    Worker w = new Worker(firstTask);final Thread t = w.thread;t.start();其实执行了worker的run方法。
    Worker(Runnable firstTask) {
      setState(-1); // inhibit interrupts until runWorker
      this.firstTask = firstTask;
      this.thread = getThreadFactory().newThread(this);
     }
（6）常见线程池
①newSingleThreadExecutor
单个线程的线程池，即线程池中每次只有一个线程工作，单线程串行执行任务
②newFixedThreadExecutor(n)
固定数量的线程池，没提交一个任务就是一个线程，直到达到线程池的最大数量，然后后面进入等待队列直到前面的任务完成才继续执行
③newCacheThreadExecutor
可缓存线程池，当线程池大小超过了处理任务所需的线程，那么就会回收部分空闲（一般是60秒无执行）的线程，当有任务来时，又智能的添加新线程来执行。corePoolSize=0 maximumPoolSize=最大
④newScheduleThreadExecutor
大小无限制的线程池，支持定时和周期性的执行线程
线程池的使用场景：适用于单次任务执行时间很短，但是并发访问量很高的情况；
类关系：？？？
ExecutorService：真正的线程池接口
ThreadPoolExecutor：线程池的默认实现（线程池最核心的类）execute()、submit()、shutdown()、shutdownNow()
Executors：负责生产线程池实例
Executor：顶层接口  executors继承executor Executor接口只有execute方法
网易notePS:  
1.线程池的构造方法几个参数的具体含义,int corePoolSize,  核心池大小
                                   int maximumPoolSize,线程池最大线程数
                                   long keepAliveTime,  上面有介绍
                                   BlockingQueue<Runnable> workQueue,等待队列，当达到corePoolSize的时候，就向该等待队列放入线程信息（默认为一个LinkedBlockingQueue），运行中的队列属性为：workers，为一个HashSet；内部被包装了一层，后面会看到这部分代码。
								   RejectedExecutionHandler handler   也就是参数maximumPoolSize达到后丢弃处理的方法，java提供了5种丢弃处理的方法，当然你也可以自己弄，主要是要实现接口：RejectedExecutionHandler中的方法：
2.当线程池中poolSize达到corePoolSize且阻塞队列已满，再来一个任务，如何处理???
拒绝策略:默认为对外泼出一个异常
1、CallerRunsPolicy：如果发现线程池还在运行，就直接运行这个线程
2、DiscardOldestPolicy：在线程池的等待队列中，将头取出一个抛弃，然后将当前线程放进去。
3、DiscardPolicy：什么也不做
4、AbortPolicy：java默认，抛出一个异常：RejectedExecutionException。

线程池补充：
workQueue：是一个BlockingQueue接口的队列，直接提交队列SynchronousQueue(newCachedThreadPool)、有界队列ArrayBlockingQueue()、无界队列LinkedBlockingQueue（newFixedThreadPool）（如果任务非常频繁，队列无限膨胀，容易耗尽系统资源）、优先队列（确保高优先级的任务先执行）。
拒绝策略：当任务多到超过系统负载的一个解决方式，抛出异常，阻止程序正常工作、poa调用者线程执行当前被丢弃的任务、丢弃最老的请求、不予任何处理。当然也是可以进行自定义的，我可以把丢弃的任务，进行日志打印。
ThreadFactory：可以进行自定义，比方说打印线程的创建信息，比方说设置为守护线程，当主线程退出后，强制销毁线程池。
线程池发生异常寻找堆栈信息：注意使用submit()会把异常信息给吃掉，改成execute()。另外使用execute()的时候，异常堆栈只能打印出任务发生错误的那一行，无法打印任务在哪里提交的代码行数，这个时候需要程序员自己进行扩展。
优雅关闭线程池：https://www.cnblogs.com/qingquanzi/p/9018627.html 一般来说没有关闭线程池，线程还在，代码如下：
while (task != null || (task = getTask()) != null) {...}
 private Runnable getTask() {
 	...
 	 Runnable r = timed ?
                    workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) :
                    workQueue.take();   // 没有任务会一直阻塞中
 ....
 }

shutdownNow方法的解释是：线程池拒接收新提交的任务，同时立马关闭线程池，线程池里的任务不再执行。可能会引发报错，一定要对任务进行异常捕获。task.run();run方法中有发生了阻塞，这个线程一旦中断，就会抛出异常。task.run还会执行。
shutdown方法的解释是：线程池拒接收新提交的任务，同时等待线程池里的任务执行完毕后关闭线程池。一定要确保任务里不会有永久阻塞的逻辑，否则线程池就关闭不了。
最后，一定要记得，shutdownNow和shuwdown调用完，线程池并不是立马就关闭了，要想等待线程池关闭，还需调用awaitTermination方法来阻塞等待。

线程池大小怎么确定？
过大和过小都不能发挥系统最大的性能，只要避免极大和极小的情况，线程池的大小对性能的影响不会太大。线程池大小的确定一般会考虑CPU和内存等因素，大概是这样子的，如果CPU密集型运算，个数设置成CPU核心数，避免了线程上下文
的切换，如果是IO密集型，等待时间会比较多，那么线程池个数相对来说可以设置更多点，比方说是CPU核心数乘以2。
IO密集型：涉及网络、硬盘IO的任务，这类任务消耗CPU操作很少，任务大部分时间在等待IO操作。大部分应用就是IO密集型，比如web应用。
CPU密集型：消耗cpu资源，比如一些计算，图片处理。


------------------------------------------------------------------------------------

5、redis 
Redis乐观锁：通过WATCH MULTI EXEC,加锁的客户端监视他需要操作的数据，一旦被其他客户端进行修改，撤销已操作的部分，然后进行不断重试；
            原理是数据版本（Version）记录机制实现，一般情况下认为不会发生冲突，在提交更新的时候，才会对数据是否冲突进行检测，如果冲突了，可以失败返回，可以继续重试。
            1. 乐观锁的实现，必须基于WATCH，然后利用redis的事务。
            2. WATCH生命周期，只是和事务关联的，一个事务执行完毕，相应的watch的生命周期即结束。
分布式锁（实现方式？你们用的哪种？redis分布式锁如何释放，如何避免死锁？如何设置锁的时间？）
redis队列 发布 订阅 redis分片集群节点故障数据如何处理？
如何优雅的增加分片集群节点？ 
Redis悲观锁：redis是单线程的，利用redis中的setnx方法，将 key 的值设为 value ，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。设置成功，返回 1，即获得锁；设置失败，返回 0，即没有获得锁；
------------------Redis分布式锁最佳实践------------------------------------
由于Redis是单线程模型，命令操作原子性，所以利用这个特性可以很容易的实现分布式锁。 获得一个锁
SET key uuid NX PX timeout
SET resource_name uniqueVal NX PX 30000
命令中的NX表示如果key不存在就添加，存在则直接返回。PX表示以毫秒为单位设置key的过期时间，这里是30000ms。 设置过期时间是防止获得锁的客户端突然崩溃掉或其他异常情况，导致redis中的对象锁一直无法释放，造成死锁。
Key的值需要在所有请求锁服务的客户端中，确保是个唯一值。 这是为了保证拿到锁的客户端能安全释放锁，防止这个锁对象被其他客户端删除。
举个例子：

A客户端拿到对象锁，但在因为一些原因被阻塞导致无法及时释放锁。
因为过期时间已到，Redis中的锁对象被删除。
B客户端请求获取锁成功。
A客户端此时阻塞操作完成，删除key释放锁。
C客户端请求获取锁成功。
这时B、C都拿到了锁，因此分布式锁失效。
要避免例子中的情况发生，就要保证key的值是唯一的，只有拿到锁的客户端才能进行删除。 基于这个原因，普通的del命令是不能满足要求的，我们需要一个能判断客户端传过来的value和锁对象的value是否一样的命令。遗憾的是Redis并没有这样的命令，但可以通过Lua脚本来完成：

if redis.call("get",KEYS[1]) == ARGV[1] then 
   return redis.call("del",KEYS[1])
 else 
   return 0
 end
逻辑很简单，获取key中的值和参数中的值相比较，相等删除，不相等返回0。
------------------Redis分布式锁最佳实践------------------------------------

redis分布式锁如何释放？？主动调用unlock方法，主动失效。或者设置锁的过期时间。
如何避免死锁？？
（1）设置一个失效时间，进行判断是否需要强制解锁。（SET key uuid NX PX timeout）
（2）死锁检测，某个线程持有锁和请求锁，都会被记录下来，如果线程A持有锁1，请求锁2，线程B持有锁2，请求锁1，这个时候会发生死锁，类似于这样的检测。
     检测出来后，一个简单的方式就是释放所有锁，一个更好的方案是给这些线程设置优先级。
（3）加锁顺序，如果能确保所有的线程都是按照相同的顺序获得锁，那么死锁就不会发生。
redis队列？？？可以使用redis的list功能！！利用PUSH 操作将任务存在lists中，然后工作线程再用POP操作将任务取出进行执行。？？？（生产者消费者模式）
redis发布 订阅模式？？几个client可以订阅同一个频道。publish发布   subscribe订阅
redis分片集群节点故障数据如何处理？？
为什么要redis集群？？通常，为了提高网站响应速度，总是把热点数据保存在内存中而不是直接从后端数据库中读取。Redis是一个很好的Cache工具。
大型网站应用，热点数据量往往巨大，几十G上百G是很正常的事儿，在这种情况下，如何正确架构Redis呢？
首先，无论我们是使用自己的物理主机，还是使用云服务主机，内存资源往往是有限制的，scale up不是一个好办法，我们需要scale out横向可伸缩扩展，
这需要由多台主机协同提供服务，即分布式多个Redis实例协同运行。
其次，目前硬件资源成本降低，多核CPU，几十G内存的主机很普遍，对于主进程是单线程工作的Redis，只运行一个实例就显得有些浪费。
同时，管理一个巨大内存不如管理相对较小的内存高效。因此，实际使用中，通常一台机器上同时跑多个Redis实例。
redis集群方案？？？？
https://www.jianshu.com/p/14835303b07e   集群  redis cluster 使用数据分片(sharding)而非一致性哈希(consistency hashing)来实现 一致性hash 总之，一致性哈希算法是希望在增删节点的时候，让尽可能多的缓存数据不失效。
客户端缓存 slot 与 redis节点的映射关系，当接收到 MOVED 响应时，会修改缓存中的映射关系。请求时会直接发送到正确的节点上，减少一次交互。
http://blog.csdn.net/xianymo/article/details/46412271   哨兵
Redis客户端通过连接哨兵，去获得master的信息，进行数据请求，并且在每个哨兵都注册一个MasterListener，用于监听master节点地址改变的消息。
https://www.cnblogs.com/helbing/p/5261274.html   复制

redis持久化！！？？ https://mp.weixin.qq.com/s/e-WKqzzeElBJAY3nrDkyBg 非常有参考性，里面还提到了问题排查和性能优化。
rdb：适用于数据备份，无法做到实时性。客户端显式触发（save,bgsave）和自动触发(配置文件)、主从同步的时候。
aof：常用，可以做到实时性，重写机制重要，缓冲区同步策略：always性能最差，everysec性能和数据安全的平衡点，no基于系统的落盘策略。
服务器重启，数据恢复：优先加载AOF，当没有AOF才会加载RDB；
fork操作优化：RDB和重写AOF文件，都需要FORK操作，fork操作会阻塞Redis的所有命令执行，直到fork操作结束，优化点是必须减少fork操作的时间。
            1、优先使用物理机或者高效支持fork操作的虚拟机技术；
            2、控制Redis最大内存，一般设置为10个G；fork操作耗时和Redis最大内存成反比；
            3、降低fork的频率；
子进程开销：fork完后会创建子进程，子进程会有CPU、内存、磁盘开销。
          1、不要和其他高硬盘负载的服务放在同一台机器上面，比如MQ、数据库；
          2、在重写期间，建议更改缓冲区同步策略为no；
          3、建议使用SSD；
单机配置多Redis实例：Redis是单线程，为了充分利用多核CPU，单机配置多实例，优化：为了减少CPU、IO的竞争，写个脚本，让每个实例串行执行AOF。
Redis ops ： 每秒的操作数，官方是10万，Get/Set操作平均耗时200~600us（含往返网络通信）；                  
Redis进阶：https://mp.weixin.qq.com/s/UcwoFxCRJJzyHz59ViXYFA     
          1、采用二进制序列化，而非常见的Json序列化。   
          2、Redis的主要性能瓶颈是序列化、网络带宽和内存大小，滥用时处理器也会达到瓶颈。
          3、在Linux上多实例部署，实例个数等于处理器个数，各实例最大内存直接为本机物理内存，避免单个实例内存撑爆（比方说8核心处理器，那么就部署8个实例）。
          4、把海量数据（10亿+）根据key哈希（Crc16/Crc32）存放在多个实例上，读写性能成倍增长。
          5、合理设计每一对Key的Value大小，包括但不限于使用批量获取，原则是让每次网络包控制在1.4k字节附近，减少通信次数（实际经验几十k，几百k也是没问题的）。          
Redis序列化：https://blog.csdn.net/f641385712/article/details/84679456。
          1、StringRedisTemplate默认采用的是String的序列化策略，保存的key和value都是采用此策略序列化保存的。StringRedisSerializer
          2、RedisTemplate默认采用的是JDK的序列化策略，保存的key和value都是采用此策略序列化保存的。JdkSerializationRedisSerializer，优点是反序列化时不需要提供（传入）类型信息(class)，但缺点是需要实现Serializable接口，还有序列化后的结果非常庞大，是JSON格式的5倍左右，这样就会消耗redis服务器的大量内存，还有序列化后的结果是二进制数据，对开发非常不友好。
          3、推荐使用：StringRedisSerializer和GenericJackson2JsonRedisSerializer（类似于Jackson2JsonRedisSerializer，优点是速度快，序列化后的字符串短小精悍，不需要实现Serializable接口，不用自己手动指定对象的Class）
序列化补充：protobuf效率非常高，用protobuf序列化后的大小是json的10分之一，xml格式的20分之一，是二进制序列化的10分之一。缺点是下载一个编译软件，对传输对象进行编译生成一个proto文件，每当传输对象修改了，proto文件也必须修改。
           protostuff的出现，弥补了protobuf的缺点。

redis队列:非阻塞lpop rpop或者阻塞brpop 都有
Redis list的应用场景非常多，也是Redis最重要的数据结构之一，比如twitter的关注列表，粉丝列表等都可以用Redis的list结构来实现。
Lists 就是链表，相信略有数据结构知识的人都应该能理解其结构。使用Lists结构，我们可以轻松地实现最新消息排行等功能。
Lists的另一个应用就是消息队列，
可以利用Lists的PUSH操作，将任务存在Lists中，然后工作线程再用POP操作将任务取出进行执行。Redis还提供了操作Lists中某一段的api，你可以直接查询，删除Lists中某一段的元素。
redis的数据结构：应用场景：https://mp.weixin.qq.com/s/iWGj-WuTg81TvJodMsA_3g
1、string 可以对存储的数值进行自增或者自减 -》每次发布文章的时候 生成一个文章id  缓存页面的内容
2、list 有序的、可以放入重复的元素、RPUSH、LRANGE、LINDEX、LPOP   
3、set集合 无序的、利用散列表存蓄不一样的元素、增删改查、判断一个元素是否存在此集合中、交集、并集、合集   记录已经对这篇文章投票过的用户  sadd voted:10488 userId:111 报名111这个用户对文章10488已经投过票
4、hash 适合存储对象，存储多个键值对的映射 hset key sub_key1 value1  文章信息存入，hset article:11 title 'java编程思想' hset article:11 link 'www.baidu.com'    存登录信息  hset login: adfafdd(token值) 12424（userID）
5、zset有序集合(skiplist) 比集合多了一个score zscore获取时间  zadd key 222 member1 可以根据分值获得集合中的一部分元素   zadd time: article:11 now   超过总数限制会移除最老的token  sadd recent: token now  zrange rencent: 0 100 
   zincrby viewed: item 1  这个商品item每浏览一次分数加1   排行榜应用  取top N 操作
补充：所以在实际的使用过程中应该使用 set 存储单个大文本非结构化数据 hset 则存储结构化数据，一个 hash 存储一条数据，一个 filed 则存储 一条数据中的一个属性，value 则是属性对应的值。
     使用set存蓄结构化数据，有序列化和反序列化的开销，并且修改某个字段（filed）的开销会比较大。

缓存穿透：
1、https://www.jianshu.com/p/2104d11ee0a2 布隆过滤器  特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。常见的适用常见有，利用布隆过滤器减少磁盘 IO 或者网络请求，因为一旦一个值必定不存在的话，我们可以不用进行后续昂贵的查询请求。
2、简单粗暴：缓存不存在的key
3、根据缓存key的设计规则，将不符合规则的key进行过滤
缓存雪崩：缓存在同一时间失效或者缓存服务器宕机，请求都到了后端数据源，负载过大，压垮数据库。
1、发现缓存中取不到，进行加锁排队或者队列的方式保证缓存的单进程操作，避免并发请求全部落到后端存储系统，但是这种方式会对性能有损耗
2、将缓存失效的时间分散，降低每一个缓存过期时间的重复率
3、如果是因为缓存服务器故障导致的原因，一方面保证高可用（Redis的哨兵节点），另一方面采用多级缓存
4、缓存降级
Redis缓存和数据一致性问题：https://blog.csdn.net/qq_37779352/article/details/82977921
1、首先，可以肯定的是，redis中的数据和数据库中的数据不可能保证事务性达到统一的，这个是毫无疑问的，所以在实际应用中，我们都是基于当前的场景进行权衡降低出现不一致问题的出现概率
2、更新缓存还是让缓存失效：更新缓存的代价是否大做决定的，代价指的是是否需要很复杂的计算或者复杂的逻辑获取最新的结果
3、先操作数据库还是先操作缓存：更新数据库和更新缓存这两个操作，是无法保证原子性的，所以我们需要根据当前业务的场景的容忍性来选择。也就是如果出现不一致的情况下，哪一种更新方式对业务的影响最小，就先执行影响最小的方案
   先delete cache 再写db：假如delete之后写db之前，有一个读线程，把db中的旧数据读到cache中，导致数据不一致。最简单的解决方案是分布式锁、列队的方式、延时双删策略。
   先写db  再delete cache：如果db成功，cache失败，借助mq中间件达到数据一致性，对业务代码有侵入。还有一个方案，启动一个订阅程序去订阅MySQL的binlog，获得需要操作的数据，发送到MQ。
Redis内存回收策略：
1、默认是当内存不够时候，返回错误
2、lru(Least recently used,最近最少使用)：最近最少使用的数据删除，适合场景，热点数据相对比较明显
3、随机：对key的访问概率差不多
4、已经设置过期时间：随机、最近最少使用、将要过期的数据
Redis是单线程的，性能为什么这么快：
1、cpu并不是Redis的瓶颈，而在于网络和内存
2、多路复用：reactor设计模型，Java中的selector
3、避免线程竞争和上下文切换
lua脚本：
1、解决效率问题：减少网络传输，把命令写到脚本中，发给Redis服务器
2、解决原子性问题：脚本作为一个整体，脚本中的几条命令，不会被其他客户端打断
3、复用性
hashTags:
我们知道执行MSET操作是原子性的。但是在redis-cluster的情况下，我们仍然可以执行MSET操作，但是并不是原子性的，会存在某些key成功，某些失败，原因是多个key被映射到了不同的机器节点上面。
   解决方案：
   引入了hashTags的概念，当一个key包含 {} 的时候，就不对整个key做hash，而仅对 {} 包括的字符串做hash，比如：user:{user1}:id、user:{user1}.name
   
redisson:
Redisson它除了常规的操作命令以外，还基于redis本身的特性去实现了很多功能的封装，比如分布式锁、原子操作、布隆过滤器、队列等等。



------------------------------------------------------------------------------------


6、分布式任务调度框架 zk 
zookeeper用来注册服务和进行负载均衡，哪一个服务由哪一个机器来提供必需让调用者知道，简单来说就是ip地址和服务名称的对应关系。
当然也可以通过硬编码的方式把这种对应关系在调用方业务代码中实现，但是如果提供服务的机器挂掉调用者无法知晓，
如果不更改代码会继续请求挂掉的机器提供服务。zookeeper通过心跳机制可以检测挂掉的机器并将挂掉机器的ip和服务对应关系从列表中删除。
至于支持高并发，简单来说就是横向扩展，在不更改代码的情况通过添加机器来提高运算能力。
通过添加新的机器向zookeeper注册服务，服务的提供者多了能服务的客户就多了。
数据发布/订阅(dubbo的注册中心)：配置信息管理，项目启动的时候去zookeeper获得配置信息，并且在这数据节点上面注册一个数据变更的watcher监听，一旦配置信息有变更，会及时通知到客户端；
负载均衡：服务提供者启动，会去zk注册域名与ip1:port1,ip2:port2的映射关系，服务消费者使用域名的时候，会去zk查询对应域名对应的ip:port列表，并且根据某种策略选择一个ip:port返回，如果ip地址有变动，
        更新ZK即可，然后ZK通知到消费者（注册了一个watcher监听）进行及时更新；
命名服务：全局唯一id的生成，调用create()创建顺序节点，返回一个完整的节点名，然后再拼接type类型即可；补充全局唯一id生成的方案https://blog.csdn.net/linzhiqiang0316/article/details/80425437
心跳检测：a机器需要知道b机器是否存活，只需要让b机器在zk上注册一个临时节点即可，a机器去zk上面查询这个临时节点，如果存在即存活，利用zk临时节点的特点，减少了系统的耦合；
总之：用zk来实现分布式系统机器间的通信，不仅能够省去大量底层网络通信和协议设计上重复的工作，更为重要的一点是大大降低了系统之间的耦合，能够非常方便的实现异构系统之间的灵活通信。
集群管理：机器上下线管理，当新增机器的时候，指定的agent部署到这台机器，agent启动之后，向zk注册一个临时子节点，对主节点关注的监控系统，就会收到一个子节点变更的通知，于是就可以做相应的逻辑处理；
         运行过程agent会定时向zk写入本机的运行状态信息，监控中心可以订阅这些节点数据变更，来获取主机的运行情况，或者监控中心可以定时主动轮训收集节点的数据，有一定的延时，但是可以节省很多网卡流量；
master选举：客户端集群向zk创建一个临时节点（节点名字一样），只有一个客户端能够创建成功，成功的那个即是master节点，其他客户端，可以注册一个字节点变更的watcher，用于监控此master，如果master挂了，
           临时节点也就没了，其他客户端就会重新发起master选举；此案例，利用了zk在分布式高并发的情况下，可以保证节点的全局唯一性；
分布式锁：排他锁，客户端集群向zk创建一个临时节点，只有一个客户端能够创建成功，成功的那个就可以获得锁，其他客户端，注册一个watcher，锁释放即临时节点删除，其他客户端接到zk通知，重新发起一轮；
         共享锁：客户端集群向zk创建临时顺序节点，然后获得所有子节点，然后进行比较，谁的序列号最小，谁就能获得锁，不是最小的话，注册一个离你最近的那个子节点变更watcher，释放锁后通知其他客户端，其他客户端再进行比较序列号，
         重复上一轮。。。。
分布式队列：先入先出，注册一个顺序临时节点，比较节点的大小，最小的先执行，另外比自己序号小的最后一个节点注册一个watcher监听，接到watcher通知，重复上面的逻辑；
分布式屏障：在主节点注册顺序临时节点，获得子节点总数，比较barrier值，大于的话，执行逻辑，反之等待，另外等待的客户端必须在主节点注册一个子节点列表变化的watcher，接到通知后，重复操作；    
java客户端：curator和zkclient

补充：
无中心化的结构解决了负载均衡设备导致的单点故障问题，并且解决了服务配置中心的压力。
zk集群：https://www.jianshu.com/p/cc5c49157404

------------------------------------------------------------------------------------


7、dubbo调用原理 dubbo通信协议 dubbo序列化 
测试和生产公用一套zookeeper，怎么保证消费不冲突？？？
http://blog.csdn.net/zh521zh/article/details/77947953    dubbo提供了Filter扩展，可以通过自定义Filter来实现这个功能。
本文通过一个事例来演示如何实现dubbo接口的IP白名单。
（1 面试题：Dubbo中zookeeper做注册中心，如果注册中心集群都挂掉，发布者和订阅者之间还能通信么？ 
可以的，启动dubbo时，消费者会从zk拉取注册的生产者的地址接口等数据，缓存在本地。每次调用时，按照本地存储的地址进行调用
注册中心对等集群，任意一台宕掉后，会自动切换到另一台 
注册中心全部宕掉，服务提供者和消费者仍可以通过本地缓存通讯 
服务提供者无状态，任一台 宕机后，不影响使用 
服务提供者全部宕机，服务消费者会无法使用，并无限次重连等待服务者恢复 
（2 dubbo连接注册中心和直连的区别 
在开发及测试环境下，经常需要绕过注册中心，只测试指定服务提供者，这时候可能需要点对点直连， 
点对点直联方式，将以服务接口为单位，忽略注册中心的提供者列表，

服务注册中心，动态的注册和发现服务，使服务的位置透明，并通过在消费方获取服务提供方地址列表，实现软负载均衡和Failover， 注册中心返回服务提供者地址列表给消费者，
如果有变更，注册中心将基于长连接推送变更数据给消费者。 
服务消费者，从提供者地址列表中，基于软负载均衡算法，选一台提供者进行调用，如果调用失败，再选另一台调用。注册中心负责服务地址的注册与查找，相当于目录服务，
服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，服务消费者向注册中心获取服务提供者地址列表，并根据负载算法直接调用提供者，注册中心，服务提供者，
服务消费者三者之间均为长连接，监控中心除外，注册中心通过长连接感知服务提供者的存在，服务提供者宕机，注册中心将立即推送事件通知消费者 
注册中心和监控中心全部宕机，不影响已运行的提供者和消费者，消费者在本地缓存了提供者列表 
注册中心和监控中心都是可选的，服务消费者可以直连服务提供者
（3、Dubbo在安全机制方面是如何解决的 
Dubbo通过Token令牌防止用户绕过注册中心直连，然后在注册中心上管理授权。Dubbo还提供服务黑白名单，来控制服务所允许的调用方。
（4、dubbo通信协议？？？
Dubbo缺省协议采用单一长连接和NIO异步通讯，适合于小数据量大并发的服务调用，以及服务消费者机器数远大于服务提供者机器数的情况。
缺省协议，使用基于mina1.1.7+hessian3.2.1的tbremoting交互。

RPC（Remote Procedure Call）―远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。
在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。

连接个数：单连接
连接方式：长连接
传输协议：TCP
传输方式：NIO异步传输
序列化：Hessian二进制序列化
适用范围：传入传出参数数据包较小（建议小于100K），消费者比提供者个数多，单一消费者无法压满提供者，尽量不要用dubbo协议传输大文件或超大字符串。
适用场景：常规远程服务方法调用

为什么要消费者比提供者个数多：
因dubbo协议采用单一长连接，
假设网络为千兆网卡(1024Mbit=128MByte)，
根据测试经验数据每条连接最多只能压满7MByte(不同的环境可能不一样，供参考)，
理论上1个服务提供者需要20个服务消费者才能压满网卡。

为什么不能传大包：
因dubbo协议采用单一长连接，
如果每次请求的数据包大小为500KByte，假设网络为千兆网卡(1024Mbit=128MByte)，每条连接最大7MByte(不同的环境可能不一样，供参考)，
单个服务提供者的TPS(每秒处理事务数)最大为：128MByte / 500KByte = 262。
单个消费者调用单个服务提供者的TPS(每秒处理事务数)最大为：7MByte / 500KByte = 14。
如果能接受，可以考虑使用，否则网络将成为瓶颈。

为什么采用异步单一长连接：
因为服务的现状大都是服务提供者少，通常只有几台机器，
而服务的消费者多，可能整个网站都在访问该服务，
比如Morgan的提供者只有6台提供者，却有上百台消费者，每天有1.5亿次调用，
如果采用常规的hessian服务，服务提供者很容易就被压跨，
通过单一连接，保证单一消费者不会压死提供者，
长连接，减少连接握手验证等，
并使用异步IO，复用线程池，防止C10K问题。

（5、dubbo的作用：使用方便 全spring配置 没有任何api的接入，就像调用本地方法一样，调用远程方法。
                  高级应用：软负载均衡 容错机制
				  服务自动注册与发现，不再需要写死服务提供方地址，注册中心基于接口名查询服务提供者的IP地址，并且能够平滑添加或删除服务提供者。

（6、dubbo序列化方式？？？默认使用Hessian序列化，还有Duddo、FastJson、Java自带序列化。

dubbo单一长连接的原理????
http://blog.csdn.net/zgliang88/article/details/75440043
1.client一个线程调用远程接口，生成一个唯一的ID（比如一段随机字符串，UUID等），Dubbo是使用AtomicLong从0开始累计数字的
2.将打包的方法调用信息（如调用的接口名称，方法名称，参数值列表等），和处理结果的回调对象callback，全部封装在一起，组成一个对象object
3.向专门存放调用信息的全局ConcurrentHashMap里面put(ID, object)
4.将ID和打包的方法调用信息封装成一对象connRequest，使用IoSession.write(connRequest)异步发送出去
5.当前线程再使用callback的get()方法试图获取远程返回的结果，在get()内部，则使用synchronized获取回调对象callback的锁，再先检测是否已经获取到结果，如果没有，然后调用callback的wait()方法，释放callback上的锁，让当前线程处于等待状态。
6.服务端接收到请求并处理后，将结果（此结果中包含了前面的ID，即回传）发送给客户端，客户端socket连接上专门监听消息的线程收到消息，分析结果，取到ID，再从前面的ConcurrentHashMap里面get(ID)，从而找到callback，将方法调用结果设置到callback对象里。
7.监听线程接着使用synchronized获取回调对象callback的锁（因为前面调用过wait()，那个线程已释放callback的锁了），再notifyAll()，唤醒前面处于等待状态的线程继续执行（callback的get()方法继续执行就能拿到调用结果了），至此，整个过程结束。

当前线程怎么让它“暂停”，等结果回来后，再向后执行？
先生成一个对象obj，在一个全局map里put(ID,obj)存放起来，再用synchronized获取obj锁，再调用obj.wait()让当前线程处于等待状态，然后另一消息监听线程等到服 
务端结果来了后，再map.get(ID)找到obj，再用synchronized获取obj锁，再调用obj.notifyAll()唤醒前面处于等待状态的线程。
正如前面所说，Socket通信是一个全双工的方式，如果有多个线程同时进行远程方法调用，这时建立在client server之间的socket连接上会有很多双方发送的消息传递，前后顺序也可能是乱七八糟的，server处理完结果后，将结果消息发送给client，client收到很多消息，怎么知道哪个消息结果是原先哪个线程调用的？
答：使用一个ID，让其唯一，然后传递给服务端，再服务端又回传回来，这样就知道结果是原先哪个线程的了。

容错模式：
1.Failover Cluster（默认缺省）：
失败就自动切换，重试其他服务器.
用于读操作，可重试会带来延迟，如果写操作，接口一定要保持幂等性。
通过retries=“2”来设置重试次数（不含第一次）
2.Failfast Cluster
快速失败，只发起一次调用，失败立刻报错。――报错最好捕获，方便自身后续分析
用于非幂等性的写操作，比如新增记录
3.Failsafe Cluster
失败自动回复，后台记录失败请求，定时重发。――避免陷入服务器宕机，不断失败，不断定时重发
通常用作消息通知操作
4.Forking Cluster
并行调用多个服务器，成功一个即返回
用于实时性高的读操作，但需要浪费更多服务资源。―― 典型的 性能和硬件 之争，协调好二者，本人还是比较推荐这个模式的。
可通过forks=“2”设置最大的并行数
5.Broadcast Cluster
广播所有提供者，逐一调用，任意一台报错则报错
用于通知各个提供者更新缓存或日志等本地资源

tcp和udp
tcp和udp都是传输协议，主要区别是tcp协议连接需要3次握手，断开需要四次握手，是通过流来传输的，就是确定连接后，一直发送信息，传完后断开。udp不需要进行连接，直接把信息封装成多个报文，直接发送。
所以udp的速度更快写，但是不保证数据的完整性和数据顺序，（这个是可以在实现时通过验证手段来手动确定完整性）。
HTTP
http协议是建立在TCP协议之上的一种应用，是Web联网的基础，最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接
RCP
一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。
SOCKET
socket并不是一种协议，是在程序员层面上对TCP/IP协议的封装和应用。其实是一个调用接口，方便程序员使用TCP/IP协议栈而已。程序员通过socket来使用tcp/ip协议。但是socket并不是一定要使用tcp/ip协议，
Socket编程接口在设计的时候，就希望也能适应其他的网络协议。

谈谈你对dubbo的理解,使用dubbo有什么好处?难道仅仅是解耦和方便开发吗?
Dubbo是Alibaba开源的分布式服务框架，它最大的特点是按照分层的方式来架构，使用这种方式可以使各个层之间解耦合（或者最大限度地松耦合）。
从服务模型的角度来看，Dubbo采用的是一种非常简单的模型，要么是提供方提供服务，要么是消费方消费服务，所以基于这一点可以抽象出服务提供方（Provider）和服务消费方（Consumer）两个角色。关于注册中心、协议支持、服务监控等内容，详见后面描述。

所有开发人员共用一个zookeeper服务的时候,如何保证服务能正确的被调用?
(同时有两个相同的服务发布在zookeeper上)
配置文件中会有一个version，只要consumer和provider的version是同一个，就会进行调用。


　首先前文里也说了，引入服务治理是为了对整体的RPC调用进行集中化管理。对我们来说其核心价值在于，减少重复劳动、避免手动配置物理文件产生的问题、降低开发人员的技术运用成本。下面针对其中的功能点进行分别讲解。
分布式系统怎么做服务治理？？？
服务的自动注册：
　　这是一个服务治理框架的基础功能。大家运用WCF的时候应该感受更加明显，我们要配置一个WCF服务端的时候需要在config文件中做很多配置，甚至大部分公司其实配置都是一模一样的到处复制黏贴，整个这个过程其实是价值较低的重复性劳动。
　　解决这个问题需要通过动态的感知到服务端的地址信息，然后针对该地址信息进行自动化配置或者模板化配置，让其快速可用。那么这些额外的信息保存在哪，就需要引入一个注册中心的概念来进行集中化管理。
客户端的自动发现：
　　当我们在config文件中指定具体的IP和端口来定义远程服务的地址，或者直接在程序里硬编码远程服务地址时，本身就是一个端到端的访问方式。无法灵活的在程序运行过程中去增加或减少后端的服务节点。
　　解决这个问题需要和服务注册的实现方式配套。还可以针对于不同类型的应用制定一些负载均衡的策略进行切换。
变更下发：
　　客户端的自动发现就依赖于此下发的数据，需要及时把提供服务的节点信息变化下发到各个客户端。它面向的场景如：当我们进行一个发布的时候，先将需要发布的节点从负载均衡列表中移除，然后再进行更新，最后再添加到负载均衡列表中。这个时候避免了访问到正在发布中的程序。
　　当然这点也可以基于状态检测模块去做，这样可以对服务节点的健康状态感知能力得到更好的加强。
监控中心：消费者调用的次数和时间。

接口的幂等性的概念？？？
http://www.360doc.com/content/18/0227/11/52975396_732821644.shtml
打个比方，订单系统生成了订单，调用支付系统，进行支付，支付成功，因为网络原因，订单系统未收到确切的结果，因为订单系统需要进行重试，
如果支付系统并没有做到接口的幂等性，订单系统第一次调用和第二次调用，用户分别被扣了两次钱，不符合幂等性原则（同一个订单，无论是调用了多少次，用户都只会扣款一次）。

DUBBO超时重试??超时时间设置???
Dubbo超时和重连机制
dubbo启动时默认有重试机制和超时机制。
超时机制的规则是如果在一定的时间内，provider没有返回，则认为本次调用失败，重试机制在出现调用失败时，会再次调用。如果在配置的调用次数内都失败，则认为此次请求异常，抛出异常。

如果出现超时，通常是业务处理太慢，可在服务提供方执行：jstack PID > jstack.log 分析线程都卡在哪个方法调用上，这里就是慢的原因。
如果不能调优性能，请将timeout设大。

某些业务场景下，如果不注意配置超时和重试，可能会引起一些异常。

1.超时设置
DUBBO消费端设置超时时间需要根据业务实际情况来设定，
如果设置的时间太短，一些复杂业务需要很长时间完成，导致在设定的超时时间内无法完成正常的业务处理。
这样消费端达到超时时间，那么dubbo会进行重试机制，不合理的重试在一些特殊的业务场景下可能会引发很多问题，需要合理设置接口超时时间。
比如发送邮件，可能就会发出多份重复邮件，执行注册请求时，就会插入多条重复的注册数据。

（1）合理配置超时和重连的思路

1.对于核心的服务中心，去除dubbo超时重试机制，并重新评估设置超时时间。
2.业务处理代码必须放在服务端，客户端只做参数验证和服务调用，不涉及业务流程处理

（2）Dubbo超时和重连配置示例

<!-- 服务调用超时设置为5秒,超时不重试--> 
<dubbo:service interface="com.provider.service.DemoService" ref="demoService"  retries="0" timeout="5000"/>

2.重连机制
dubbo在调用服务不成功时，默认会重试2次。
Dubbo的路由机制，会把超时的请求路由到其他机器上，而不是本机尝试，所以 dubbo的重试机器也能一定程度的保证服务的质量。
但是如果不合理的配置重试次数，当失败时会进行重试多次，这样在某个时间点出现性能问题，调用方再连续重复调用，
系统请求变为正常值的retries倍，系统压力会大增，容易引起服务雪崩，需要根据业务情况规划好如何进行异常处理，何时进行重试。


dubbo 容错机制和负载均衡？？？？https://www.cnblogs.com/juncaoit/p/7691411.html
Failover Cluster

　　　　失败自动切换，当出现失败，重试其它服务器 。通常用于读操作，但重试会带来更长延迟。可通过 retries="2" 来设置重试次数(不含第一次)。

　　　　<dubbo:service retries="2" />

　　　　<dubbo:reference retries="2" />

　　　　<dubbo:reference> <dubbo:method name="findFoo" retries="2" /> </dubbo:reference>

 　　Failfast Cluster

　　　　快速失败，只发起一次调用，失败立即报错。通常用于非幂等性的写操作，比如新增记录。

　　Failsafe Cluster

　　　　失败安全，出现异常时，直接忽略。通常用于写入审计日志等操作。

　　Failback Cluster

　　　　失败自动恢复，后台记录失败请求，定时重发。通常用于消息通知操作。

　　Forking Cluster

　　　　并行调用多个服务器，只要一个成功即返回。通常用于实时性要求较高的读操作，但需要浪费更多服务资源。可通过 forks="2" 来设置最大并行数

　　Broadcast Cluster

　　　　广播调用所有提供者，逐个调用，任意一台报错则报错 。通常用于通知所有提供者更新缓存或日志等本地资源信息。

 

4.配置

　　提供方：

　　　　<dubbo:service cluster="failsafe" />

　　消费方：

　　　　<dubbo:reference cluster="failsafe" />

 

二：负载均衡

1.机制

　　在集群负载均衡时，Dubbo 提供了多种均衡策略，缺省为 random 随机调用。

　　可以自行扩展负载均衡策略

 

2.负载均衡策略

　　Random LoadBalance　　
随机，按权重设置随机概率。
在一个截面上碰撞的概率高，但调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。
　　RoundRobin LoadBalance

轮循，按公约后的权重设置轮循比率。
存在慢的提供者累积请求的问题，比如：第二台机器很慢，但没挂，当请求调到第二台时就卡在那，久而久之，所有请求都卡在调到第二台上。
　　LeastActive LoadBalance

最少活跃调用数，相同活跃数的随机，活跃数指调用前后计数差。
使慢的提供者收到更少请求，因为越慢的提供者的调用前后计数差会越大。
　　ConsistentHash LoadBalance

一致性 Hash，相同参数的请求总是发到同一提供者。
当某一台提供者挂时，原本发往该提供者的请求，基于虚拟节点，平摊到其它提供者，不会引起剧烈变动。
算法参见：http://en.wikipedia.org/wiki/Consistent_hashing
缺省只对第一个参数 Hash，如果要修改，请配置 <dubbo:parameter key="hash.arguments" value="0,1" />
缺省用 160 份虚拟节点，如果要修改，请配置 <dubbo:parameter key="hash.nodes" value="320" />
 

3.配置

　　服务端服务级别
　　　　<dubbo:service interface="..." loadbalance="roundrobin" />

　　客户端服务级别
　　　　<dubbo:reference interface="..." loadbalance="roundrobin" />

　　服务端方法级别
　　　　<dubbo:service interface="..."> <dubbo:method name="..." loadbalance="roundrobin"/> </dubbo:service>

　　客户端方法级别
　　　　<dubbo:reference interface="..."> <dubbo:method name="..." loadbalance="roundrobin"/> </dubbo:reference>


dubbo的一致性hash
https://blog.csdn.net/Revivedsun/article/details/71022871    源码分析
https://cloud.tencent.com/developer/news/220874   有参考价值  通俗易懂 深入浅出
在解决分布式系统中负载均衡的问题时候可以使用hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡的作用。
但是普通的余数hash（hash(比如用户id)%服务器机器数）算法伸缩性很差，当新增或者下线服务器机器时候，用户id与服务器的映射关系会大量失效。一致性hash则利用hash环对其进行了改进。
虚拟节点的出现，解决一致性hash的倾斜，分配更加均匀。
将数据进行hash计算后，顺时针找对应的node,该node即为该数据的服务node。增加节点或者删除节点影响的范围只是一小部分。

那些年在dubbo中踩得坑？？https://www.jianshu.com/p/ec7bbc3f925b
1、超时重试问题；数据库新增三条记录；
2、规范问题：开发自测的时候，自己写的未自测的代码的服务，容易被开发环境的其他小伙伴调用到，需要规定dubbo版本号；
3、规范问题：配置属性的问题，消费者端和提供者端很多配置会重复，比方说：超时时间，重试次数，这个时候该如何抉择，作为提供终端比消费者端更加清楚服务的性能参数，Provider 端尽量多配置 Consumer 端的属性，
   让 Provider 的实现者一开始就思考 Provider 端的服务特点和服务质量等问题。


-------------------------------------------------------------------------------------------------------------------------------


8、为 何使用mongodb？？？
https://www.zhihu.com/question/32071167?sort=created
应用特征Yes / No应用不需要事务及复杂 join 支持必须 Yes新应用，需求会变，数据模型无法确定，想快速迭代开发？
应用需要2000-3000以上的读写QPS（更高也可以）？应用需要TB甚至 PB 级别数据存储?应用发展迅速，需要能快速水平扩展?
应用要求存储的数据不丢失?应用需要99.999%高可用?应用需要大量的地理位置查询、文本查询？如果上述有1个 Yes，可以考虑 MongoDB，
2个及以上的 Yes，选择MongoDB绝不会后悔。
灵活文档模型(内嵌式数据模型 ) + 高可用复制集(仲裁节点:不参与复制和读写 只投票    超过大多数同意  从节点即可变成主节点) + 可扩展分片集群(mongos进行路由)
如果程序频繁获取address数据中的名称信息，那么需要多次查询以解析引用。一个较优的方案是嵌入address数据到patron数据中
{
   _id: "joe",
   name: "Joe Bookreader",
   addresses: [
                {
                  street: "123 Fake Street",
                  city: "Faketon",
                  state: "MA",
                  zip: "12345"
                },
                {
                  street: "1 Some Other Street",
                  city: "Boston",
                  state: "MA",
                  zip: "12345"
                }
              ]
 }
社群发的朋友圈  地理位置查询(附近人) 直播用户信息+礼物  聊天消息  日志记录
MongoDB性能高的原因？
首先：Mongo使用了内存映射技术 － 写入数据时候只要在内存里完成就可以返回给应用程序，而保存到硬盘的操作则在后台异步完成，所以数据暂时会保存在内中，提高了查询的效率。
其次：没有事务，没有join操作，文档性模式设计一般会是的你所需要的数据都相对集中在一起（内存或硬盘），大家知道硬盘读写耗时最多是随机读写所产生的磁头定位时间，数据集中在一起则减少了关系性数据库需要从各个地方去把数据找过来（然后Join）所耗费的随机读时间
 
-------------------------------------------------------------------------------------------------------------------------------

9、rocketmq使用场景 ？？？  rabbitmq!!!
http://blog.csdn.net/he90227/article/details/50800646
消息队列中间件是分布式系统中重要的组件，主要解决应用耦合，异步消息，流量削锋等问题。实现高性能，高可用，可伸缩和最终一致性架构。
是大型分布式系统不可缺少的中间件。
解决rocketmq消息重复和消息顺序????
http://blog.csdn.net/lovesomnus/article/details/51776942

rabbitmq！！！：
符合满足AMQP协议，生产者发送msg的时候带上routing key(fanout不用带上),发送到指定的exchange，exchange根据routing key进行选择队列queue，此时consumer会监听某个队列，会打开一个消息通道channel，如果有多个，便打开多个channel。
虚拟主机vhost：每一个vhost本质上就是一个mimi版的rabbitmq服务器，拥有自己的队列、绑定、交换机，有自己的权限控制（每个vhost下面都有自己可以访问的用户），一个rabbitmq服务器来服务众多的应用程序。
binding:绑定，将exchange和queue根据某种路由规则绑定起来。
channel 信道：信道是生产消费者与rabbit通信的渠道，生产者publish或是消费者subscribe一个队列都是通过信道来通信的。信道是建立在TCP连接上的虚拟连接，什么意思呢？就是说rabbitmq在一条TCP上建立
             成百上千个信道来达到多个线程处理，这个TCP被多个线程共享，每个线程对应一个信道，信道在rabbit都有唯一的ID ,保证了信道私有性，对应上唯一的线程使用。
RabbitMQ 为什么需要信道？为什么不是TCP直接通信？
1. TCP的创建和销毁，开销大，创建需要三次握手，销毁需要四次分手
2. 如果不使用信道，那么应用程序就会使用TCP的方式连接到rabbitmq，高峰时每秒成千上万条连接会造成资源的巨大浪费(一条tcp消耗资源，成千上万的tcp会非常消耗资源)，而且操作系统每秒处理TCP连接数量也是
   有限的，必定会造成性能瓶颈
3. 信道的原理是一条线程一条信道，多条线程多条信道共同使用一条TCP连接。一条TCP连接可以容纳无限的信道，及时每秒造成成千上万的请求也不会造成性能瓶颈
三种交换机：
直连交换机Direct Exchange:交换机根据某个routing key直接找到queue；声明一个队列的时候，会自动绑定到默认交换机（空字符串名称），并以队列名称作为路由键。
         一般情况可以使用rabbitMQ自带的Exchange：””(该Exchange的名字为空字符串，下文称其为default Exchange)。
         这种模式下不需要将Exchange进行任何绑定(binding)操作
         消息传递时需要一个“RouteKey”，可以简单的理解为要发送到的队列名字。
         如果vhost中不存在RouteKey中指定的队列名，则该消息会被抛弃。
         rabbitTemplate.convertAndSend("helloQueue", sendMsg);//  这里helloQueue是routing key 没有指定exchange 有一个默认的exchange 发送到了名称为helloQueue的队列中
主题交换机Topic Exchange:比较直连交换机，可以使用通配符，routing key可以带 * 只能匹配一个单词  # 匹配零个或者多个。
广播交换机Fanout Exchange:不需要指定routing key  交换机绑定几个队列，消息就会发送到几个队列。补充：fanout可以配置routing key不过自测没啥软用。
消费者还是生产者声明队列：一般来说是消费者声明队列，如果你的应用程序可以承担起消息的丢失，队列不存在的情况下，生产者会发送失败，最好的方式是消费者和生产者都需要声明队列。
exclusive(专用的、排外的)：申明队列的时候，设置为true，队列将设置为私有，只有你的应用程序才能消费该队列。RPC场景下，申明回调队列的时候，exclusive=true，可以防止其他客户端读到应答消息。
                        Queue.DeclareOk queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete,
                                 Map<String, Object> arguments) throws IOException;
auto_delete：设置为true，当最后一个消费者取消订阅的时候，队列会自动移除。
basic.reject()：允许消费者拒绝rabbitmq发送的消息，参数requeue设置为true，重新入队列，让下一个订阅者消费，如果设置成false，就会把这条消息丢弃。
auto_ack：消费者接收到每条消息是否自动确认，默认情况下是自动确认，即消息者一接收到消息，rabbitmq会自动视其确认了消息，然后就会把此消息删除；如果设置为false，需要手动应答，channel.basicAck()，
          如果忘记应答，队列中的消息会一直存在，重新连接的时候会重复消费(必须要重新连接，才会让自己重新消费，或者其他订阅者消费)。
队列对应多个消费者：一轮训的方式进行下发消息，round-robin，每条消息只会发送给一个订阅者。
durable：是否持久化队列或者交换机，默认为false，即重启或者rabbitmq发生崩溃后，队列或者交换机会消失，如果设置为true，就不会。
消息持久化：必须满足三点，把消息标记为持久化（delivery<投递> mode设置为2），交换机必须是持久化，队列必须是持久化。
          当发布一条消息到持久化的交换机上面，rabbitmq会把消息提交到持久化日志文件才发送响应。
生产者确认confirm模式：确保消息的持久化，最大的好处是异步的，基本上没有性能的开销，比起事务模式（吸干rabbitmq的性能，不建议使用）。如果消息和队列是可持久化的，那么确认消息只会在队列将消息写入
                    到磁盘后才发出，如果rabbitmq发生了内部错误从而导致了消息的丢失，rabbitmq会发送一条nack消息。
TTL：可以设置队列的过期时间或者单条消息的过期时间，配合死信队列可以做延迟队列。业务场景：下订单30分钟后把订单改成失效。
死信队列：三种情况可以进入死信交换机（声明队列的时候，加上arguments.put("x-dead-letter-exchange","DLX_EXCHANGE");）
        1、消息被拒绝（basic.reject/ basic.nack）并且requeue=false
        2、消息过期 (不消费这些消息，让其过期)
        3、队列达到最大长度(先入队的消息会被发送到DLX)
优先级队列： 优先级高的消息会优先消费，当消息堆积的时候，优先级才有意义。
           Map<String, Object> argss = new HashMap<String, Object>(); 
           argss.put("x-max-priority",10); // 队列最大优先级
           channel.queueDeclare("ORIGIN_QUEUE", false, false, false, argss);
延迟队列：配合死信队列即可实现，过期的消息，转发到延时队列，让订阅者进行消费。或者使用插件。https://github.com/rabbitmq/rabbitmq-delayed-message-exchange
RPC：服务端处理消息后，把响应消息发送到一个响应队列里面，客户端再从响应队列里面取到结果。
服务端流控：rabbit启动的时候会检测自己占用的内存达到40%以上的话，会主动抛出一个预警并阻塞所有的连接。默认情况下，如果磁盘空间在1GB以下，rabbit会主动阻塞生产者。
消费端限流：假设一个场景，由于我们的消费端突然全部不可用了，导致 rabbitMQ 服务器上有上万条未处理的消息，这时候如果没做任何处理，随便开启一个消费端客户端，就会导致巨量的消息瞬间全部推送过来，
          但是我们单个客户端无法同时处理这么多的数据，就会导致消费端变得巨卡，有可能直接崩溃不可用了。所以在实际生产中，限流保护是很重要的。
          rabbitMQ 提供了一种 qos (服务质量保证)功能，即在非自动确认消息的前提下，如果一定数目的消息(通过基于 consume 或者 channel 设置 QOS 的值)未被确认即ack前，不进行消费新的消息(rabbit不会推送新消息)。
          关键代码就是在声明消费者代码里面的 void basicQos(unit prefetchSize , ushort prefetchCount, bool global )。
          channel.basicQos(2);
immediate（立即的）:
当immediate标志位设置为true时，如果exchange在将消息路由到queue(s)时发现对于的queue上没有消费者，那么这条消息不会放入队列中。当与消息routeKey关联的所有queue（一个或者多个）都没有消费者时，
该消息会通过basic.return方法返还给生产者。rabbitmq3.0后的版本，去掉了immediate参数的支持，会报错。immediate标记会影响镜像队列性能，增加代码复杂性，并建议采用“TTL”和“DLX”等方式替代。
    void basicPublish(String exchange, String routingKey, boolean mandatory, boolean immediate, BasicProperties props, byte[] body)
            throws IOException;
            
rabbitmq消费者push模式和pull模式：一般是push模式注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对用户而言，感觉消息是被推送过来的。优点是效率高。缺点是很容易造成
                               consumer来不及处理消息。
                               pull模式：可以根据消费者自身的处理情况来取消息，实时性不太高，不需要ack，因为消费者取下一批数据时就可以认为上一批数据已经处理完毕。

以下是rabbitmq的可靠性投递！！！！
确保消息发送到rabbitmq队列中？？？(confirm模式代表，消息发送到exchange是否成功，而不关心exchange是否路由到正确的queue上面)。
补充：当消息发送到的交换机不存在，直接抛出异常，no find exchange。另外当exchange为“”空字符串，routingKey为一个不存在的路由，此时消息依然会异步回调handleAck，因为exchange为“”空字符串是存在的。
事务模式、confirm模式（异步回调）。。。
怎么保证消息百分之100投递成功：消息提前持久化 + 定时任务
怎么确保消息路由到正确的队列？？？ 当消息无法匹配到列队时，会发回给生产者。或者发到备份交换机上面。
  mandatory（强制的）标志位
当mandatory标志位设置为true时，如果exchange根据自身类型和消息routeKey无法找到一个符合条件的queue，那么会调用basic.return方法将消息返还给生产者；当mandatory设为false时，出现上述情形
broker会直接将消息扔掉，不会将消息返回给生产者。
  设置备份交换机
给指定的交换机备份一个交换机，无法路由的消息会发送到备份交换机上面，如下的ALTERNATE_EXCHANGE。
Map<String,Object> arguments = new HashMap<String,Object>(); 
arguments.put("alternate-exchange","ALTERNATE_EXCHANGE"); // 指定交换机的备份交换机  alternate交替的
channel.exchangeDeclare("TEST_EXCHANGE","topic", false, false, false, arguments);
消息持久化？？？
略！
确保消息从队列正确地投递到消费者？？？
autoAck参数为false的时候，会显示ack......basic.reject(),参数requeue........
消费者回调？？？
发送一条消息给生产者，或者调用生产者的api，告知消息已经成功消费。
补偿机制？？？
对于一定时间没有得到响应的消息，可以设置一个定时重发的机制，但要控制次数，比如最多重发3次，否则会造 成消息堆积，具体来说就是将消息持久化到数据库并设置状态值，收到消费端的应答就改变当前记录的状态。
再用轮询去重新发送没接收到应答的消息，注意这里要设置重试次数。
消息幂等性？？？
发送端无法做到，只能在消费端进行控制。引申出来的问题，如果避免消息重复消费？造成重复消费的原因：
        1、生产者的问题，环节1重复发送消息，比如在开启了Confirm模式但未收到确认。
        2、环节4出了问题，由于消费者未发送ACK或者其他原因，消息重复投递。 
        对于重复发送的消息，可以对每一条消息生成一个唯一的业务ID，通过日志或者建表来做重复控制。
消息的顺序性？？？
一个队列有多个消费者时，无法做到顺序消费，除非一个队列只有一个消费者。或者用业务的方式进行控制。

以下是rabbitmq的高可用架构！！！
rabbitmq集群：主要是为了实现高可用与负载均衡。通过cookie的方式来验证身份，需要在所有节点上保持一致。磁盘节点和内存节点，集群中至少要有一个是磁盘节点以实现元数据的持久化，集群通过25672端口两两通讯。
rabbitmq镜像队列：集群方式下，队列和消息是无法在节点之间同步，因为需要用到rabbitmq的镜像队列的方式。https://blog.csdn.net/u013256816/article/details/71097186
HAproxy负载+Keepalived高可用：准备两个内存节点和一个磁盘节点
在两个内存节点上面安装HA，并且进行监听15673和5673两个端口，8.40和8.45为内存节点。
listen rabbitmq_admin
bind 0.0.0.0:15673
server node1 192.168.8.40:15672 server node2 192.168.8.45:15672
#监听端口 #统计页面自动刷新时间 #统计页面url #统计页面密码框上提示文本 #统计页面用户名和密码设置
listen rabbitmq_cluster 0.0.0.0:5673 mode tcp
balance roundrobin
timeout client 3h
timeout server 3h
timeout connect 3h
server node1 192.168.8.40:5672 check inter 5s rise 2 fall 3 // check定义每隔多长时间检查后台服务器是否可用（健康监测）
server node2 192.168.8.45:5672 check inter 5s rise 2 fall 3 // rise后台服务器发生故障后，需要多少次健康监测，才能再次确认可用
                                                            // fall需要经过多少次失败的健康监测，HA才会停止使用后台服务器

利用keepalived做主备，避免单点问题(如果HA挂了怎么办？)，实现高可用，为了心跳检测HA，在两个内存节点上面安装一个MASTER和BACKUP，并且提供一个虚拟ip，对外提供服务。
vrrp_script chk_haproxy {
    script "pidof haproxy"
    interval 2
}
vrrp_instance VI_1 {
    interface ens192
    state MASTER
    priority 200
    virtual_router_id 10
    unicast_src_ip 192.168.8.40
    authentication {
        auth_type PASS
        auth_pass password
    }
    virtual_ipaddress {
        192.168.8.201 //虚拟ip，对外提供服务
    }
    track_script {
        chk_haproxy
    }
    notify_master /loadbtify_master.sh
}
启动keepalived即可，192.168.8.201是对外提供的统一地址。通过192.168.8.201:5673就可以访问rabbitmq服务。

以下是note题目？？？
1、消息队列的作用与使用场景? 
2、创建队列和交换机的方法? 
3、多个消费者监听一个生产者时，消息如何分发? 
4、无法被路由的消息，去了哪里? 可以设置备份交换机，无法路由的消息会发送到此。如果没有设置备份交换机，看参数mandatory，true会返回生产者，false会丢弃。
5、消息在什么时候会变成Dead Letter(死信)? 
6、RabbitMQ如何实现延迟队列? 
7、如何保证消息的可靠性投递? 
8、如何在服务端和消费端做限流? 
9、如何保证消息的顺序性? 
   一个队列只有一个消费者的情况下才能保证顺序。
   否则只能通过全局ID来实现（业务端进行实现）。
   1、每条消息有一个msgId，关联的消息拥有同一个parentMsgId。
   2、可以在消费端实现前一条消息未消费，不处理下一条消息;也可以在生产端实现前一条消息未处理完毕，不发 布下一条消息。
10、RabbitMQ的节点类型?
   普通模式：消息实体只会存在于一个节点，如果consumer在不存在这个消息实体的节点消费时，rabbitmq会临时在两个节点中进行消息传输。如果存在消息实体的那个rabbit挂了，那只能等它恢复了，才能进行消费。
   镜像模式：消息实体会在节点间进行同步。解决了普通模式的问题。
   
rabbitmq解耦：异步解耦（分离接收请求request和请求处理process），秒杀的业务，前端接收请求，生产一条AMQP的消息，发送至rabbit，process应用监听并且处理request，请求处理完了，发送一条消息到rabbit
             request应用监听并且处理，告诉消费者是否抢到商品。提供负载均衡：为process应用增加处理能力，通过增加新process服务的方法。语言无关性，request服务和process使用不同的语言编写，也没有关系
             AMQP是语言无关的。

书籍上面的补充知识？？？
集群中的队列：在一个集群中，创建队列只会在一个节点上进行创建，这个节点包含此队列的元数据、状态和内容，所有其他节点只知道此队列的元数据和指向该队列存在的那个节点的指针。（rabbitmq2.6.0之后出现镜像队列！！）
            为什么默认情况下，rabbit不会把队列的状态和内容同步到其他节点呢？
            1、磁盘空间：新节点的加入不会带来更多的存储空间；
            2、性能：消息的发布，需要复制到每一个节点；
            其他节点会将接收到的消息，传递给该消息所属队列的所属节点。因此在集群中添加更多的节点，会帮助传播消息到指定的节点，带来性能上的提升。
集群中的交换机：只是一个名称和队列绑定列表，当你发送消息到交换机，实际上是由你所连接到的信道（channel）将消息上的路由键同交换器的绑定列表进行比较，然后路由消息。实际上是信道是消息路由器，交换机仅仅
             是一张查询表。当在集群中新建一个交换机的时候，rabbit会将查询表添加到所有节点上面，此时，每个节点上面的每条信道都可以访问到这个新的交换机，也就是说每个节点都拥有每个交换机的信息，不用
             担心节点故障时重新声明交换机了，重新连接到集群发送消息即可。
             如果消息发布到信道上面，在路由之前节点发生崩溃怎么办？？？事务模式和发放方确认模式。
磁盘节点和内存节点：顾名思义，队列、交换机、绑定关系、用户、权限、vhost的元数据一个存磁盘，一个存内存（重启后丢失，好处是申明队列和交换机的时候，效率非常高，特别是在rpc的场景下），
                 一个集群里面必须有一个磁盘节点，用于恢复元数据用，当节点加入或者离开集群后，必须将这个变更通知到至少一个磁盘节点，如果只有一个磁盘节点，而且崩溃了，那么集群还是可以继续
                 工作的，只是创建队列、交换机、绑定、更改用户等等变更元数据的操作是不被允许的。保险的话可以设置两个磁盘节点，只有一个需要所有磁盘节点必须在线的操作是添加或者删除集群节点。
故障相关：代码实践，消费端while(true){try{连接rabbit 声明queue exchange等等}catch(){打印一条日志}} 如果连接的一个节点挂了，会打印一条日志 然后因为while的存在重新走一遍逻辑，继而连接到其他可用的一个节点，会发生以下的问题：
        在rabbit1.8.0之前的版本，当含有持久化队列的节点发生故障，客户端如果尝试连接其他节点并且重新创建此持久化队列，那么当节点恢复的时候，旧队列里面的数据就会丢失。
        在rabbit1.8.0之后的版本，当含有持久化队列的节点发生故障，客户端如果尝试连接其他节点并且重新创建此持久化队列，那么客户端会收到一个404 NOT_FOUND AMQP的错误。当故障节点恢复了，持久化队列及其内容也就恢复
        了（假设消息设置了deliveryMode=2）。在故障节点恢复之前，投递到该队列的消息，要么丢失了，要么设置了mandatory=true把消息返回给了客户端，或者设置了备份交换机，会把消息转发到备份交换机上面。
        以上两个版本的策略，都不是合理的：第一个消息丢失，第二个节点挂了的这些时间，客户端均无法发消息到这个队列（崩溃节点所属的持久化队列）。
        解决以上的问题，rabbit出现了warren主/备方式：主服务器和备服务器两者之间是隔离的，当主服务器挂了，HA会启动备服务器（在HA中配置backup），可以让消费者在备服务器上重新创建各种元数据，生产者往备服务器发送
        消息供消费者消费，新的消息不会丢失，并且业务不会断！老消息也不会丢失，等崩溃的那台主服务器恢复后，可以对老消息进行消费了。
消息的远距离通信和复制：使用shovel插件，思想是如下：不使用shovel的情况，消息要发送到两个数据中心（一个是内网，一个是外网），影响速度。使用shovel的话，可以在内网的rabbit新建一个队列，消息发送一份到此队列，
                    用shovel进行消费，然后把消息转发外网的rabbit，这样子不会影响之前逻辑的效率。

rabbitmq监控？？?1、监控是否能够接收TCP连接并且可以成功获取信道；2、监控是否可以成功路由消息；3、监控配置是否被修改；4、监控集群状态；5、监控消费者（消息总数）(适当加机器、线程)。
                PS：最重要的是监控消息是否有堆积，有的话及时增强下游处理能力（加机器，加线程），当然做的更好点可以以热点拓扑图绘制所有消息的流向流速一眼就可以看到目前哪些消息有压力。
监控rabbit不仅仅是确保5672是开启的能够接收TCP连接而已（代表rabbit守护进程在运行，但是不知道是否正常运作，比方说内存不足怎么办？没有足够的内存来响应AMQP命令），需要模拟AMQP客户端确保连接之后能够获取信道。
使用AMQP模拟检测来确认rabbit是否运行：我们利用nagios配合代码，简单的检测rabbit是否可以接受到新的请求和构造AMQP信道，可以来验证rabbit服务器是否健康。
使用REST API来检测：以上是验证rabbit是否能够接收连接，api/aliveness-test(rabbit management API 提供) 发送/接收消息，可以检测是否可以成功路由消息。
监控配置文件修改：比方说一个队列本来是持久化的，后来一不小心改成非持久化，这样的动作需要被监控到，不然损失会是非常大的。rabbit management API 提供 api/queues/<vhost>/<queue>方法
监控集群状态：api/nodes 返回节点的信息，就可以知道，节点是否缺失，使用内存有没有超过阈值。
监控消费者：（最主要监控消息总数）
        利用AMQP，申明队列的时候，参数passive=true 当队列存在的时候，会返回此队列的消息总数，然后验证消息总数，确保消费者是否在正常消费，当消息达到一个高度的时候，有可能消费者崩溃了，也有可能消费者处理了逻辑，
        忘记ack(autoack=false)
        利用rest api的方式，api/queues/<vhost>/<queue_name>可以详细的知道数据统计信息，messages_ready未消费的消息、messages_unack未确认的消息、messages消息总数。

提升性能，保障安全？？？
硬件配置：网络配置、磁盘配置、内存配置、CPU核数配置
消息持久化：设置方式delivery_mode=2  是否持久化，对性能影响很大。如果你能忍受消息丢失，可以设置消息非持久化，大大提高每秒消息的投递量。
消息确认：如果订阅队列的时候auto_ack=true，那么你处理完消息之后就无须再发送确认消息回服务器，优点是大大加快了消费者消费消息的速度，缺点是当客户端崩溃了，发送给客户端还未处理的消息就丢失了。
消息投递：详细得看书籍上面的图。。。。如果队列为空，并且消费者已经做好准备，会直接投递给消费者，然后消费者是否设置了auto-ack，为true，流程结束，这是效率最高的方式。
        接上面，为false，根据消息投递模式、队列是否持久化、内存是否有压力，选择把消息存磁盘还是内存中。
内存使用率：队列、交换机、绑定占用的内存非常小。
erlang进程计数：默认是每个erlang节点是1048576（2的20次方），对大多数用户来说足够多了，超过这个限制的话，erlang会崩溃，rabbit也会崩溃。新建连接-4个进程数 新建信道-4个进程数 队列声明-1个进程数。
        


消息队列中间件的比较
RabbitMQ：
优点：支持很多协议如：AMQP，XMPP，STMP，STOMP；灵活的路由；成熟稳定的集群方案；负载均衡；数据持久化等。
缺点：速度较慢；比较重量级，安装需要依赖Erlang环境。
Redis：
优点：比较轻量级，易上手
缺点：单点问题，功能单一
Kafka：https://mp.weixin.qq.com/s/KbQeseskB0YIfNLdSp0urg
优点：高吞吐；分布式；快速持久化；负载均衡；轻量级
缺点：极端情况下会丢消息

如何保障消息中间件100%消息投递成功？如何保证消息幂等性？
https://mp.weixin.qq.com/s/ymMGwIc37dSMjj48k6N0fA

MQ中间件、微服务、定时任务整合
https://mp.weixin.qq.com/s/iefJdMzfd7ZUQ9AEOGlPbg

-------------------------------------------------------------------------------------------------------------------------------

10、http协议 https 
https://www.cnblogs.com/wqhwe/p/5407468.html
https://mp.weixin.qq.com/s/pEVRjHNFYxYnqz3p7vPI7w


-------------------------------------------------------------------------------------------------------------------------------

11、JVM？？？
JVM垃圾回收机制，何时触发MinorGC等操作？？？
垃圾回收机制：
1、为什么要进行垃圾回收？
随着程序的运行，内存中存在的实例对象、变量等信息占据的内存越来越多，如果不及时进行垃圾回收，必然会带来程序性能的下降，甚至会因为可用内存不足造成一些不必要的系统异常。
2、哪些“垃圾”需要回收？
在我们上面介绍的五大区中，有三个是不需要进行垃圾回收的：程序计数器、JVM栈、本地方法栈。因为它们的生命周期是和线程同步的，随着线程的销毁，
它们占用的内存会自动释放，所以只有方法区和堆需要进行GC。具体到哪些对象的话，简单概况一句话：如果某个对象已经不存在任何引用，那么它可以被回收。
通俗解释一下就是说，如果一个对象，已经没有什么作用了，就可以被当废弃物被回收了。
3、什么时候进行垃圾回收？判断改对象是否改回收的条件?
根据一个经典的引用计数算法，每个对象添加一个引用计数器，每被引用一次，计数器加1，失去引用，计数器减1，当计数器在一段时间内保持为0时，
该对象就认为是可以被回收得了。但是，这个算法有明显的缺陷：当两个对象相互引用，但是二者已经没有作用时，按照常规，应该对其进行垃圾回收，但是其相互引用，
又不符合垃圾回收的条件，因此无法完美处理这块内存清理，因此Sun的JVM并没有采用引用计数算法来进行垃圾回收。而是采用一个叫：根搜索算法
基本思想就是：从一个叫GC Roots的对象开始，向下搜索，如果一个对象不能到达GC Roots对象的时候，说明它已经不再被引用，即可被进行垃圾回收
触发minorGC：当新生成的对象放入Eden区中，发现Eden空间不够，这个时候会发生minorgc，释放Eden空间。
补充当Eden空间还不够的时候，会把一些活跃对象放入到survivor区，old区空间足够，会把survivor区的对象移到old区。
4、如何进行垃圾回收？
根据一些算法：1.Mark-Sweep（标记-清除）算法：扫描对象-标记-清除  会产生内存碎片 效率不高
           2.复制算法（适用于新生代）：内存划分为相同的两块，活着的对象移到另一块空闲内存上面，上一块内存清除掉，优点：没有内存碎片，
			       缺点：对内存空间要求高，感觉有点浪费了一块内存空间。
			     3.标记整理（适用于老年代）：先标记 再整理  把存活对象整理到一边  把另外一边死的对象清除  没有内存碎片 充分利用空间。
			     4.分代回收策略：对新生代较高频率GC，对年老代较低频率GC。


新生代和老生代的内存回收策略？？？？
新生代GC（Minor GC）：指发生在新生代的垃圾收集动作，因为Java对象大多都具备朝生夕灭的特性，所以MinorGC非常频繁，一般回收速度也比较快。
老年代GC（Major GC/Full GC）：指发生在老年代的GC，出现了Major GC，经常会伴随至少一次的Minor GC。Major GC的速度一般会比Minor GC慢10倍以上。

触发GC（Garbage Collector）的条件： 
1)GC在优先级最低的线程中运行，一般在应用程序空闲即没有应用线程在运行时被调用。 
2)Java堆内存不足时，GC会被调用。

 jvm中一次完整的GC流程（从ygc到fgc）是怎样的，重点讲讲对象如何晋升到老年代等?
答：对象优先在新生代区中分配，若没有足够空间，Minor GC； 
大对象（需要大量连续内存空间）直接进入老年态；长期存活的对象进入老年态。如果对象在新生代出生并经过第一次MGC后仍然存活，年龄+1，若年龄超过一定限制（15），则被晋升到老年态。

Eden和Survivor的比例分配等？？？
http://blog.csdn.net/paul_wei2008/article/details/46444803
默认比例是8:1，可以进行配置！！！

JVM内存分代？？？
Java 8的内存分代改进？？？
为什么要将永久代 (PermGen) 替换为元空间 (MetaSpace) 呢?
永久代受限于固定大小的内存，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，基本上不会发生OOM。
已经取消永久代了，元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。
因此，默认情况下，元空间的大小仅受本地内存限制，
但可以通过以下参数来指定元空间的大小：可见在jdk8中：
1.字符串常量由永久代转移到堆中。
2.持久代已不存在，PermSize MaxPermSize参数已移除。
为什么转移到元空间：
    1、字符串存在永久代中，容易出现性能问题和内存溢出。
　　2、类及方法的信息等比较难确定其大小，因此对于永久代的大小指定比较困难，太小容易出现永久代溢出，太大则容易导致老年代溢出。
　　3、永久代会为 GC 带来不必要的复杂度，并且回收效率偏低。
　　4、Oracle 可能会将HotSpot 与 JRockit 合二为一。

运行时常量池在JDK1.6及之前版本的JVM中是方法区的一部分，而在HotSpot虚拟机中方法区放在了”永久代(Permanent Generation)”。所以运行时常量池也是在永久代的。 
但是JDK1.7及之后版本的JVM已经将运行时常量池从方法区中移了出来，在Java 堆（Heap）中开辟了一块区域存放运行时常量池。


深入分析了Classloader，双亲委派机制
类加载器的双亲委派加载机制（重点）：当一个类收到了类加载请求，他首先不会尝试自己去加载这个类，而是把这个请求委派给父类去完成，每一个层次类加载器都是如此，
因此所有的加载请求都应该传送到启动类加载其中，只有当父类加载器反馈自己无法完成这个请求的时候（在它的加载路径下没有找到所需加载的Class），
子类加载器才会尝试自己去加载。

JVM的编译优化
对Java内存模型的理解，以及其在并发中的应用
指令重排序，内存栅栏等

类加载机制？？？
定义：虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验、转换解析和初始化，最终形成可以被虚拟机直接使用的java类型。
类加载和连接的过程都是在运行期间完成的。
加载器：启动类加载器、标准扩展类加载器、系统类加载器、自定义加载器
双亲委派机制：JVM在加载类的时候，默认采用的是双亲委派机制，当一个类加载器接到类的加载请求的时候，首先先会把这个任务委派给父类加载器，如果父类加载器不能够加载这个类，只有自己的去加载。
//首先判断该类型是否已经加载
//如果没有被加载，就委托给父类加载器或者启动类加载器
//如果存在父类加载器，就委派给父类加载或者委派给启动类
//如果不存在父类加载器，就检查是否是由启动类加载器加载的类，如果是就成功返回!!!
//如果以上都不能加载，才调用自身的加载功能
//结论：如果父加载器为null，则会调用本地方法进行启动类加载尝试。!!!
继承关系：启动类加载器《--标准扩展类加载器《--系统类加载器
ClassLoader抽象类的两个构造方法：
（1没有带参数，没有指定父类，默认为系统类加载器
（2有参数，指定父类，强制设置父类加载器
在代码中直接调用Class.forName（String name）方法，到底会触发那个类加载器进行类加载行为？？？
Class.forName(String name)默认会使用调用类的类加载器来进行类加载。
在编写自定义类加载器时，如果没有设定父加载器，那么父加载器是？？？
默认使用系统类加载器。所以，我们现在可以相信当自定义类加载器没有指定父类加载器的情况下，默认的父类加载器即为系统类加载器。同时，我们可以得出如下结论：
即时用户自定义类加载器不指定父类加载器，那么，同样可以加载如下三个地方的类：
1.   <Java_Runtime_Home>/lib下的类
2.   < Java_Runtime_Home >/lib/ext下或者由系统变量java.ext.dir指定位置中的类
3.   当前工程类路径下或者由系统变量java.class.path指定位置中的类

在编写自定义类加载器时，如果将父类加载器强制设置为null，那么会有什么影响？如果自定义的类加载器不能加载指定类，就肯定会加载失败吗？
父类加载器是启动类加载器，可以加载java_home/lib下的类，不能加载java_home/lib/ext下面的类，当然也可以使用自己类的加载器加载（启动类加载器不能加载的情况下）

最好不要复写loadclass中的委派逻辑？？？
这样做极有可能引起系统默认的类加载器不能正常工作。

JVM面试题？？？？
http://blog.csdn.net/zd836614437/article/details/64126826
1. 类的实例化顺序，比如父类静态数据，构造函数，字段，子类静态数据，构造函数，字段，他们的执行顺序
答：先静态、先父后子。 
先静态：父静态 > 子静态 
优先级：父类 > 子类 静态代码块 > 非静态代码块 > 构造函数 
一个类的实例化过程： 
1，父类中的static代码块，当前类的static 
2，顺序执行父类的普通代码块 
3，父类的构造函数 
4，子类普通代码块 
5，子类（当前类）的构造函数，按顺序执行。 
6，子类方法的执行，


Full GC和Minor GC区别，及各自的触发条件
minor gc: 当 JVM 无法为一个新的对象分配空间时会触发 Minor GC，比如当 Eden 区满了。每次 Minor GC 会清理年轻代的内存。
Major GC 是清理老年代。
Full GC 是清理整个堆空间―包括年轻代和老年代。
full GC：当准备要触发一次young GC时，如果发现统计数据说之前young GC的平均晋升大小比目前old gen剩余的空间大，则不会触发young GC而是转为触发full GC

说说CMS垃圾回收器，及其适用场景
Concurrent Mark Sweep。

看名字就知道，CMS是一款并发、使用标记-清除算法的gc。

CMS是针对老年代进行回收的GC。 
在一些对响应时间有很高要求的应用或网站中，用户程序不能有长时间的停顿，CMS 可以用于此场景,强调的是性能

优点：
1).将停顿时间降到最低，能给电商网站用户带来最好的体验。               
2).尽管CMS的GC线程对CPU的占用率会比较高，但在多核的服务器上还是展现了优越的特性，目前也被部署在国内的各大电商网站上。
缺点：
1).对CMS在单核和多核机器上做测试。发现CMS在收集过程中会大量占用CPU的时间。所以在第二个阶段会比较漫长，所以一般将其设置在多核机器上。并且对于CMS在单核机器上的表现设计了一套启发式控制。这种控制将收集器看作一个掠夺者，而收集器会尽量赶在用户线程分配新的对象之前完成收集的工作。同样也有可能会出现用户线程希望分配对象，但目前空间不够，则需要停下收集器，这样会让整个收集时间大大加长。所以这时候一搬会选择扩张堆的大小。
2).Mark Sweep算法一直令人诟病的碎片问题，造成了堆空间的浪费以及利用率的下降。(CMS的解决方案是使用UseCMSCompactAtFullCollection参数(默认开启)，在顶不住要进行Full GC时开启内存碎片整理。)
3).需要较大的内存空间去运行，因为在很多并行的阶段，要考虑到用户程序运行时也要分配空间。所以一般选择在堆利用率达到一个常数的时候就开启CMS的收集。可以在VM argument里来设置这个阀值。(CXX:CMSInitiatingOccupancyFraction =n，n=0~100)
4).会产生浮动垃圾，由于CMS并发清理阶段用户线程还在运行着，伴随程序自然就还会有新的垃圾不断产生，这一部分垃圾出现在标记过程之后，CMS无法在当次收集中处理掉它们，只好等到下一次GC去处理。

G1收集器

G1收集器的优势：

（1）并行与并发

（2）分代收集

（3）空间整理 （标记整理算法，复制算法）

（4）可预测的停顿（G1处处理追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为M毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经实现Java（RTSJ）的来及收集器的特征）

补充！！！
程序计数器：线程私有，存蓄的是正在执行的虚拟机字节码指令地址。
虚拟机栈：（一个线程对应一个虚拟机栈，描述的是Java方法执行的内存模型）执行方法时会创建一个栈帧，用于存储局部变量数据、参数信息、记录出栈入栈操作等等。一个方法a执行产生栈帧F1，然后压入到栈中，然后a方法继续调用b方法，产生栈帧F2，
         然后压入到栈中，执行完毕后，先弹出F2，在弹出F1，先进后出。
方法区：线程共享，可以理解为永久代（hotspot虚拟机），用户存储虚拟机加载的类信息、常量、静态变量等等。jdk1.6常量池放在方法区（永久代），1.7放在了堆内存，jdk1.8已经没有永久代，将方法区直接放在一个
       与堆不相连的本地内存区域，元空间。
运行时常量池（class常量池加载进内存）：class文件除了放类的信息，接口，方法等描述信息外，还存放着常量池，用于存放编译期生成的各种字面量和符号引用，这部分内容将在类加载后，进入方法区。当然运行期间也是可以将新的常量放入池中，
            比方说string的intern()方法。
字符串常量池：1.6在永久代，1.7（以后包括1.8）在堆内存，可能因为方法区的永久代内存空间太小了，容易导致溢出。1.6放字符串常量，1.7放字符串常量或者存放堆内的字符串对象的引用。
常量池（属于方法区，1.7之后放入了堆内存，但还是属于方法区的）：为了避免频繁的创建对象和销毁对象而影响系统性能，其实现了对象的共享。例如字符串常量池，在编译阶段就把所有的字符串文字放到一个常量池中。
       ・节省内存空间：常量池中所有相同的字符串常量被合并，只占用一个空间。
       ・节省运行时间：比较字符串时，==比equals()快。对于两个引用变量，只用==判断引用是否相等，也就可以判断实际值是否相等。
对象在内存中的存储：对象头、实例数据、填充信息；对象头：自身的运行时数据（hashcode、gc分代年龄、锁状态标志、线程持有的锁），类型指针（指向他的类元数据，虚拟机通过这个指针知道这个对象是哪个类的实例）。
对象访问方式：操作栈上面的reference来操作实例对象，使用句柄的方式访问（java堆中有个句柄池，记录着对象实例的地址，好处是，实例对象被移动的时候（发生GC），只需要更改句柄就行，不需要更改reference）、
            使用直接指针访问。
逃逸分析和栈上分配：对象一般在堆上分配空间，如果没有发生逃逸那么在栈上分配空间。jvm根据对象是否发生逃逸，会分配到不同（堆或栈）的存储空间。栈上分配的对象，GC不需要进行回收，性能上不错。消除同步，逃逸分析可以判断出
                某个对象是否始终只被一个线程访问。能在方法内创建对象，就不要再方法外创建对象。毕竟这是为了GC好，也是为了提高性能。
                概念：Java Hotspot编译器能够分析出一个新的对象的引用的使用范围从而决定是否要将这个对象分配到堆上。
                1、快速 GC；
                2、消除同步，避免了多线程的竞争；
                https://mp.weixin.qq.com/s/gtKysPvVoTvtu_cApvZHAg
TLAB：JVM在eden区开辟了一小块线程私有的区域，称作TLAB（threadlocal allocation buffer）,适合存储是小对象，用过即丢弃，不存在线程共享，快速GC。线程私有不存在任何的锁进行同步，相比其他对象在堆中分配内存必须
      要进行同步（虚拟机采用CAS配上失败重试的方式保证更新操作的原子性），而在竞争激烈的场合分配的效率又会进一步下降。
TLAB和逃逸分析最佳详情：https://blog.csdn.net/zyc88888/article/details/80361635 尝试栈上分配-尝试tlab分配-是否直接进入老年代-进入Eden。

接解决oom：Java heap space：Java堆内存溢出，一般的手段是通过内存映像分析工具（比如ecplise memory analyzer）对dump出来的堆转储快照进行分析，重点是确认内存中的对象是否是必要的，
         也就是分清楚到底是出现了内存泄漏，还是内存溢出。如果是内存泄漏，进一步通过工具查看泄漏对象到GC ROOTS的引用链，就能找到泄漏对象是通过怎么样的路劲与GC ROOTS相关联，并导致垃圾回收器无法回收。
         掌握了对象类型并且GC ROOTS引用链的信息，就可以精确的定位到泄漏代码的位置。如果不存在泄漏，换句话说这些对象确实是存活状态，这个时候得检查机器内存，-Xmx -Xms，是否有增大的必要，从代码中
         检查有些对象是否存在生命周期过长，占用着内存，迟迟不释放。
stackOverFlowError: -Xss设置小一点，采用递归调用，会抛出这样子的错误，单个线程下，无论是栈帧太大或者虚拟机栈太小，当内存无法分配的时候，虚拟机抛出的都是stackOverFlowError。
                    如果使用多线程测试，可以产生oom，32G的Windows对每个进程分配的内存最多是2个G，减去xmx,减去方法区容量，剩下的被本地虚拟机栈瓜分了。
虚拟机进程的内存：等于最大堆内存 最大方法区内存 程序计数器 虚拟机栈和本地方法栈，如果每个线程分配到的栈容量越大，可以建立的线程数量自然就少。
Jstat是JDK自带的一个轻量级小工具。全称“Java Virtual Machine statistics monitoring tool”，它位于java的bin目录下，主要利用JVM内建的指令对Java应用程序的资源和性能进行实时的命令行的监控，
包括了对Heap size和垃圾回收状况的监控。https://www.jianshu.com/p/213710fb9e40
jps：列出正在运行的虚拟机进程。
jinfo：实时查看和调整虚拟机的各项参数。
jstack：生成虚拟机当前时刻的线程快照，为了定位线程出现长时间停顿的原因，比如死锁、死循环、等等。

判断哪些对象存活，哪些对象死去？
引用计数法、GC ROOTS的对象作为起始点，向下搜索，搜索所走过的路劲成为引用链，当一个对象到GC ROOTS没有任何引用链相连，即GC roots 到这个对象不可达，证明对象是不可以用的、
GC ROOTS对象: 虚拟机栈（栈帧的本地变量）中引用的对象、方法区中类静态属性引用的对象、方法区中常量引用的对象等等.
垃圾收集算法：
标记清除：产生内存碎片
复制算法：（适用于存活率较低的新生代，老年代并不适合）内存划分两块，使用时只用一块，当一块用完了，将存活对象复制到另外一块上面，优点没有内存碎片，缺点浪费一块内存空间。新生代使用复制算法作为垃圾回收，
三块区域Eden、from survivor、to survivor，eden：survivor=8:1(默认)，每次使用Eden区和from，当发生垃圾回收的时候Eden和from中存活对象复制到to中，最后清理掉Eden和from，10%的空间是被”浪费“，
当to survivor不够的时候，需要老年代进行分配担保（一般来说新生代的对象百分之98可以被回收）。
标记整理清除：试用于老年代（对象存活率比较高，也没有其他内存对其进行分配担保，故不能使用复制算法），不会产生内存碎片。
分代收集算法：新生代和老年代

垃圾收集器：
serial收集器：单线程收集器，单个线程去完成垃圾回收的工作，更重要的是，会发生stop the world（暂停其他线程，直到收集完成），虚拟机运行在client模式下默认的新生代收集器，优点简单高效，没有线程交互的开销，
             桌面应用中新生代也就几十兆，一两百兆，停顿时间也就几十毫秒，这点停顿时间完全可以接受的。
ParNew收集器（和cms进行配合收集）：serial收集器的多线程版本，除了可以进行多线程收集，其他跟serial收集器一模一样，包括stop the world，虚拟机运行在server模式下首选的新生代收集器，因为除了serial收集器只有它可以跟cms进行
             配合收集工作。
Parallel Scavenge收集器（和Parallel Old配合收集工作）：新生代吞吐量优先的收集器，复制算法，并行的多线程收集器，看上去跟ParNew收集器一样。不同之处：它的关注点是可控制的吞吐量（cpu运行代码的时间/cpu总消耗的时候）（尽快完成程序的运算任务，
                        适合在后台运算而不需要太多交互的任务），手工优化存在困难的情况下，有个参数可以自适应调节策略，不需要设置新生代的大小，设置-Xmx，设定一个目标（最大停顿时间或者吞吐量）即可。
                        垃圾收集停顿时间设置的小，GC次数会增加，吞吐量会降低。两个重要参数 -XX:MaxGcpausemillis垃圾收集停顿时间   设置的越小 回收次数增多 吞吐量降低。-XX:GCtimeratio吞吐量大小。
Serial old收集器：单线程收集器，标记-整理算法。
Parallel Old收集器：Serial old的多线程版本，它的出现弥补了，Parallel Scavenge收集器只能与Serial old搭配，在注重吞吐量优先与cpu资源敏感的场合，可以优先考虑Parallel Scavenge和Parallel Old.
CMS（老年代）: 获取最短回收停顿时间为目标，重视响应速度，希望系统停顿时间最短，给用户带来较好的体验。算法：标记-清除。并发收集，低停顿。初始标记的时候stop the world。jdk1.8默认老年代的垃圾回收
     缺点：对cpu资源非常敏感，占用CPU资源，导致应用程序变慢，总的吞吐量会降低；
         产生浮动垃圾，因为回收的时候，用户线程还在运行中，会伴有新的垃圾产生；
         产生内存碎片，当无法找到足够大的连续空间来分配当前对象，不得不提前触发一次FULL GC。也可以设置参数，当触发FULL GC时，开启内存碎片整理.
CMS出现promotion（担保失败） failed&concurrent mode failure：https://www.jianshu.com/p/ca1b0d4107c5 出现concurrent mode failure的情况下需要进行stop-the-world的Serial Old收集器！
G1: 并行（利用多核CPU来缩短stop-the-world停顿时间）和并发（并发的方式让Java程序继续运行）、分代收集、标记-整理（没有内存碎片）、追求低停顿外，还可以建立可预测的停顿时间模型。jdk1.8建议用G1。jdk1.9默认G1。

内存分配与回收策略：
对象优先在Eden分配，当没有足够空间，会发生一次minor gc;
大对象直接进入老年代，最典型的大对象是很长数组，经常出现大对象容易导致还有内存的情况下就提前触发了垃圾收集以获取足够的连续空间来安置他们，有个参数可以进行设置，超过多大的对象直接进去老年代，避免Eden和survivor发生大量的内存复制；
长期存活的对象进入老年代，每个对象都有一个age，经过一次minor gc，age+1，当age增大到15岁（默认）就会晋升到老年代。另外，如果survivor区中相同age的对象总和大于survivor的一半，那么超过这个age的对象直接进入老年代；
空间分配担保：新对象会首先生成在eden区，如果eden区满了，在发生minor gc之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象的空间，如果成立，minor  gc 是安全的。如果不成立，虚拟机就会去
            查看HandlePromotionFailure设置是否允许担保失败。如果允许，那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次minor gc，尽管是有风险的，
            如果小于，或者HandlePromotionFailure设置不允许冒险，那这时候改为一次full  gc。
            如果出现了HandlePromotionFailure失败，那就只好在失败后重新发起一次Full GC。虽然担保失败时绕的圈子是最大的，但大部分情况下都还是会将HandlePromotionFailure开关打开，避免Full GC过于频繁。
            JDK 6 Update 24之后的规则变为只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小就会进行Minor GC，否则将进行Full GC。
            担保失败的GC日志如下：先进行了一次minorGC，发现survivor放不下，只能放到old区，old区没有连续的空间存在这部分对象，导致concurrent mode failure。
            106.641: [GC 106.641: [ParNew (promotion failed): 14784K->14784K(14784K)<整个young区>, 0.0370328 secs]106.678: [CMS106.715: [CMS-concurrent-mark: 0.065/0.103 secs] [Times: user=0.17 sys=0.00, real=0.11 secs]
            (concurrent mode failure): 41568K->27787K(49152K)<整个old区>, 0.2128504 secs] 52402K->27787K(63936K)<整个heap>, [CMS Perm : 2086K->2086K(12288K)], 0.2499776 secs] [Times: user=0.28 sys=0.00, real=0.25 secs]



java类的加载机制？？？参考：http://www.cnblogs.com/ityouknow/p/5603287.html
类的生命周期：
加载：1、获取类的二进制字节流；2、二进制字节流根据虚拟机所需要的格式存蓄在方法区；3、在堆中（方法区？）生成一个Class对象，用于访问方法区的数据；可以从class文件加载，也可以从网络等等途径加载。
连接：包括验证、准备、解析。
验证：确保被加载类的正确性，分为文件格式验证、元数据验证、字节码验证、符号引用验证。
准备：为类的静态变量分配内存，并将其初始化为默认值。
解析：符号引用（student）转换成直接引用（地址访问）。
初始化：为类的静态变量赋予正确的初始值。执行<clinit>
      类初始化时机：只有当对类的主动使用的时候才会导致类的初始化，类的主动使用包括以下六种：
       C 创建类的实例，也就是new的方式
       C 访问某个类或接口的静态变量，或者对该静态变量赋值
       C 调用类的静态方法
       C 反射（如Class.forName(“com.shengsiyuan.Test”)）
       C 初始化某个类的子类，则其父类也会被初始化
       C Java虚拟机启动时被标明为启动类的类（Java Test），直接使用java.exe命令来运行某个主类

什么是类的加载：
类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的Class对象，
Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。
        ClassLoader classLoader = Allocation.class.getClassLoader();
           // sun.misc.Launcher$AppClassLoader@18b4aac2
        System.out.println(classLoader);
           // 只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容,只有在newInstance才会去执行static块。
        Class<?> aClass = classLoader.loadClass("com.hxm.JVM.Test");
        Object o = aClass.newInstance();
           // com.hxm.JVM.Test@610455d6
        System.out.println(o);
           // 将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块；
        Class.forName("com.hxm.JVM.Test");
        
JVM调优？？？
JVM 32BIT OR 64BIT
在项目实施中，数据库服务器可以选择部署在64bit操作系统上；
而对于应用服务器，就不能一概而论了，如果要获得应用的最佳性能，就要在应用的部署环境中进行测试，以评价转换到64位所带来的潜在性能提升，再决定是部署在JVM 32bit还是JVM 64bit。
但一般情况下，除非应用需要较大的内存，并超过了2G，并且对应的服务器物理内存超过了4G，否则应该以JVM 32bit优先。
可视化工具visualVM：运行监控和故障处理程序。虚拟机进程和进程的配置、信息。监视cpu、堆、GC、方法区和线程的信息。dump以及分析堆转储快照、方法级的程序运行性能分析。。。
                  最精华的部分就是给visualVM安装插件
                  生成和浏览堆转储文件，右击左边的应用程序-堆dump-应用程序下边就会生成一个以headump开头的文件-可以进行保存，可以进行浏览
                  Profile:
                  一个是CPU，可以跟踪每个方法占用CPU的时长；比如你在发现CPU持续走高的时候可以通过Profile的CPU跟踪来确定是哪些函数耗费了性能；
                  一个是内存，内存的Profile通常可以检测到现存的对象都有哪些，占用了多少内存，对象存在多久等信息；通过内存Profile可以发现可疑的内存泄漏对象，进行分析；还可以针对某一个具体的对象进行内存跟踪，
                  只要右键Class-name列下一行，有一个“log class library”，点击即可切换到跟踪指定对象的内存使用情况。
                  插件BTRACE:
                  解决问题：经常遇到程序出现问题，但排查错误的一些必要信息，譬如方法参数、返回值等，在开发时并没有打印到日志之中，以至于不得不停掉服务，通过调试增量加入日志代码以解决问题。
                  作用：不停止服务的前提下，进行调试代码（日志不全的情况下）。
                  插件Visual GC：
                  可以看到GC的一些信息、包括GC次数、gc时间、各个区的内存变化。
                  Threads:
                  可以看到各个线程的运行情况：running sleeping wait park monitor(在锁上面等待)。

这个案例可以进行了解下。 concurrent mode failure。https://www.jianshu.com/p/ca1b0d4107c5   总结一句话：使用标记整理清除碎片和提早进行CMS操作。

线上出现了cpu报警、进而内存报警，观察内存新增特别快，新生代Eden区回收效率特别低，放到survivor放不下，就直接进了老年代，老年代内存也在激增，fullgc次数频率非常高。jstat -gcutil 999 1000
第一步：让运维帮忙dump堆转存快照；mat分析，byte数组占用量特别多，继续查询，byte数组到GC roots的引用链，发现是通过okhttp引用着，迟迟不释放，发生了内存泄漏。
第二步：二话不说，让运维先增加配置，最大堆从1.6个G增大到6个G； // 治标不治本，内存还在不断增大
第三步：怀疑发生了线程阻塞。
ps -mp pid -o THREAD,tid,time  // 先显示线程列表  找到耗时最高的线程，一直占用着CPU资源
printf “%x\n” tid // 把线程id转化成16进制格式
jstack pid |grep tid -A 30  // 最后打印线程的堆栈信息 得到以下信息：我们程序中使用了okhttp请求第三方资源。
"OkHttp ConnectionPool" #4858 daemon prio=5 os_prio=0 tid=0x00007f8199496800 nid=0x20a8 in Object.wait() [0x00007f812691c000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
at java.lang.Object.wait(Native Method)
at java.lang.Object.wait(Object.java:460)
at com.squareup.okhttp.ConnectionPool$1.run(ConnectionPool.java:101)
- locked <0x000000068dba8960> (a com.squareup.okhttp.ConnectionPool)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
参考文章：https://blog.csdn.net/chenaini119/article/details/80000949
结论：这段时间内，业务量暴涨，身份证认证的接口请求次数比起之前有量级别的增长，最重要的是这个接口需要传身份证的照片（传的byte数组），但是调用第三方的时候速度很慢，身份证照片byte数组不能及时释放，导致内存暴增，
线程也不能释放，一直占用着CPU资源，应用变得巨卡。
解决方案：
1、部署双节点做负载均衡。
2、图片信息进行压缩。
3、服务降级，控制这个接口的访问量，超过一定的访问量或者超过一定的访问时间，失败返回。
4、借助mq进行异步处理。


性能调优补充？？？
虚拟机工具：jps 
jstat监控jvm使用信息 jstat -gcutil 999 1000 监控进程为999的gc信息，每隔1秒钟进行打印一次。
jstack
jmp
gc日志
                

类加载沽泡补充笔记？？？
类加载是懒加载的
变量表 javap 字节码？
沙箱？
ecplise memory anylise

clinit和init的区别？	
clinit：JVM第一次加载class文件是调用，包括静态变量初始化语句和静态代码块的执行；
init：实例创建的时候调用，包括普通代码块和构造函数。

双亲委派机制的好处？JVM区分类的方式：1、类名和路径；2、是否是相同的类加载器加载（相同文件的类被不同的类加载器加载产生的是两个不同的类）。如果不使用双亲委派机制，每个类加载器都加载自己的话，
                我们编写一个类java.lang.object，那么程序运行的话，就会有多个object。
不使用双亲委派机制？自定义一个类加载器，继承java.lang.classloader，重写loadClass方法。


-----------------------------------------------------------------------------------


12、分布式事务？？
同理转账服务也是如此，当用户A账户扣除1万后，
我们只要生成一个凭证（消息）即可，这个凭证（消息）上写着“让用户B账户增加 1万”，只要这个凭证（消息）能可靠保存，
我们最终是可以拿着这个凭证（消息）让用户B账户增加1万的，即我们能依靠这个凭证（消息）完成最终一致性。
经典的 X/OpenDTP 事务模型：
                        AP: application, 应用程序，也就是业务层。哪些操作属于一个事务，就是 AP 定义 的；
                        RM: Resource Manager，资源管理器。一般是数据库，也可以是其他资源管理器， 比如消息队列，文件系统；
                        TM: Transaction Manager ，事务管理器、事务协调者，负责接收来自用户程序 (AP)发起的 XA 事务指令，并调度和协调参与事务的所有 RM(数据库)，确保 事务正确完成；
                        在分布式系统中，每台机器节点虽然都知道自己在进行事务操作过程中的结果是否成功或者失败，但是却无法知道其他机器节点的操作结果，所以引入了协调者（TM）来调度所有分布式节点的执行逻辑。
                        \1. 参与分布式事务的应用程序(AP)先到 TM 上注册全局事务 
                        \2. 然后各个 AP 直接在相应的资源管理器(RM)上进行事务操作 
                        \3. 操作完成以后，各个 AP 反馈事务的处理结果给到 TM
                        \4. TM 收到所有 AP 的反馈以后，通过数据库提供的 XA 接口进行数据提交或者回 滚操作
2pc：
    第一阶段（投票阶段）：1、事务询问 2、执行事务（Undo 和 Redo 信息记录到事务日志中） 3、各个AP向TM反馈事务询问的响应
    第二阶段（执行事务提交）：1、发送提交请求 2、事务提交 3、反馈事务提交结果 4、完成事务
                          1、发送回滚请求 2、事务回滚 3、反馈事务回滚结果 4、中断事务
    优点：原理简单，实现方便；
    缺点：每个阶段都是同步阻塞，容易产生性能问题；
         协调者产生单点问题，如果协调者在第二阶段出现故障，那么其他参与者会一直处理锁定状态；
         太过保守，任意一个节点失败都会导致数据回滚；
         数据不一致，在阶段二协调者往参与者们发送commit请求，在完全没有发送完之前协调者挂了，可能会导致部分参与者收到commit，剩下的参与没有收到commit将无法进行提交，也就可能导致数据不一致；

3pc：
     3PC 协议主要用来解决 2PC 的同步阻塞问题的一种优化方案，最大的优点就是降低了参与者的阻塞范围，并且在出现协调者单点故障的时候继续达成一致，3pc 分为 3 个阶段 分别为:cancommit、Precommit、doCommit。
     和 2 阶段提交的区别在于:
     (1) 在协调者和参与者中引入了超时机制，{2pc 只有在协调者拥有超时机制,协调者在一定时间内没受到参与者的信息则默认为失败}; 
     (2) 把 2 阶段提交的第一个阶段拆分成了两个步骤。
     缺点：参与者在接收到precommit消息后，如果出现了网络分区，协调者所在的节点和参与者无法进行通信，在这种情况下，参与者（在等待超时后）依然会进行事务的提交，可能导致数据不一致。

幂等？？？https://mp.weixin.qq.com/s/vUFhl4-V7AmOGFHHELuQHA
   数据库唯一约束报错捕捉错误（catch住唯一约束异常，然后可以进行查询一次，查到就返回成功）   
   查询（会有竞争问题比方说同一时间消费两条一模一样的消息，查询不到，都会进行消费）得加锁，如果是Redis呢，需要加锁吗？
   状态机实现？？？其实就是表中的一个字段，比方说订单状态，where status = 1 才会更新成功  有点类似于乐观锁的方式
   token机制，防止页面重复提交：防止重复点击或者网络重发，或者nginx重发等情况导致数据被重复提交。数据提交前要向服务的申请token，token放到redis或jvm内存，token有效时间。
                            提交后后台校验token，同时删除token，生成新的token返回。
                            注意：redis要用删除操作来判断token，删除成功代表token校验通过，如果用select+delete来校验token，存在并发问题，不建议使用。
   select + insert：必须要加锁（分布式锁、悲观锁、乐观锁）。



基于消息的最终一致性方法实践？？？
两种方案：一种是基于可靠消息中间件来实现异步的最终一致性，另外一种就是通过MQ来实现最大努力通知型（比如支付宝支付成功后，回调我们系统）。
参考：https://www.jianshu.com/p/eb7a36d25b2a

TCC事务？？？https://blog.csdn.net/u010412301/article/details/78410933
try:尝试执行业务
confirm:确认执行业务
cancel：取消执行业务

主事务表 分事务表 下单流程  主事务表初始状态-1  下单--使用优惠券(生成分事务表,状态1)--扣库存(生成分事务表,状态1)   主事务表状态变成0   然后变成1然后 分事务表状态变成-1  事务完成
扫描主事务表 状态值<1的  找到事务id 查找分事务表  进行相应的回滚操作.




-----------------------------------------------------------------------------------------------------------------------


13、设计模式？？？
在 Spring 框架中，设计模式是怎么用的？ https://mp.weixin.qq.com/s/eAlPRqScG3-Acvi3HwYK3A
单例模式：
对于频繁使用的对象，可以节省new操作花费的时间，对于重量级对象来说，是一笔非常可观的系统开销。
对系统内存的使用率降低，减轻GC压力，缩短GC停顿时间。
饿汉式：性能最佳，并且简单，缺点是无法控制实例创建的时间点。
最佳实践如下：私有静态内部类的方式，弥补饿汉式和懒汉式（线程不安全，需要进行加锁）的缺点。
public Singleton{
	private Singleton(){};
	private static SingletonHolder{
		  private static Singleton instance= new Singleton();
		
	}
	public static getInstance(){
		return SingletonHolder.instance;
		}
	
}

不变模式:
对多线程友好，不会产生线程安全问题，String类和所有元数据的包装类（Boolean、Double、Long。。。。。。）都是使用不变模式实现的，当使用String所有实例方法均不需要考虑同步操作，提高系统的并发性能和并发量。
public final class Product { // 确保没有子类
    private final String no; // 私有属性，不会被获取
                             // final保证属性不会被第二次赋值
    private final double price;
    public Product(String no, double price) { // 必须指定数据，创建之后，无法进行修改
        super();
        this.no = no;
        this.price = price;
    }
    public String getNo() {
        return no;
    }
    public double getPrice() {
        return price;
    }
}

消费者和生产者模式：
多线程设计模式，共享内存缓冲区，作为消费者和生产者的通信桥梁，避免了消费者和生产者的直接通信，进行了解耦。生产者不需要知道消费者的存在，消费者不需要知道生产者的存在，另外由于缓冲区的存在，可以允许
生产者和消费者在执行速度上面存在时间差。
内存缓冲区的选择，BlockingQueue并不是高性能的实现，它完全使用了锁和阻塞等待来实现多线程间的同步。可以选择无锁的内存缓冲区的实现，比方说ConcurrentLinkedQueue。
无锁的缓冲框架disruptor了解一下，利用CAS，并且消除了伪共享（如果两个不同的并发变量位于同一个缓存行，则在并发情况下，会互相影响到彼此的缓存有效性，进而影响到性能，这叫着‘伪共享’。为了避开‘伪共享’，
Disruptor3.0在Sequence.java中使用多个long变量填充，从而确保一个序号独占一个缓存行。）

工厂模式
适配器模式


future模式：核心思想是异步调用，当调用者调用一个耗时很大的函数时，我们可能先不急于要结果，可以让被调用者立即返回一个凭证，让它在后台慢慢计算，对于调用者来说，可以先处理其他事情，处理完了，再用这个
           凭证去获取计算结果。参考jdk的实现。

代理模式：为其他对象提供一种代理以控制对这个对象的访问。也可以说，在出发点到目的地之间有一道中间层，意为代理。
静态代理：一个代理类只能对一个业务接口和实现类进行代理，如果有多个业务接口就必须定义很多代理类和实现接口。如果代理类对业务方法的预处理或者调用后操作都是一样的，那这些代理类就会有重复代码。编译期间生成代理类。
jdk动态代理：弥补了静态代理的缺点，目标对象必须实现接口。代码运行过程中，生成代理对象。
        // 参数：类的加载器（和目标对象一样的加载器）、目标类的所有接口、InvocationHandler子类(this)实现invoke方法，在这个方法里面加入需要增强的代码
        return Proxy.newProxyInstance(targetObject.getClass().getClassLoader(),
                targetObject.getClass().getInterfaces(), this);
                
        // InvocationHandler子类所实现的方法
        @Override
        public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        // agrs 参数
        if (null != args) {
            for (int i = 0; i < args.length; i++) {
                System.out.println(args[i]);
            }
        }
        System.out.println("满30减5活动进行中");
        // 调用目标方法targetObject：目标实例  args：参数  invoke:返回值
        Object invoke = method.invoke(targetObject, args);
        System.out.println("骑手正在奔跑中");
        System.out.println(invoke);
        return invoke;
        }
                
        // 代理对象的源码： 某个需要增强的方法 h指的是InvocationHandler子类 里面会有一个子类自己实现的invoke方法（增强的代码写在这里） m3指的是目标类的choose方法 var1方法参数
        public final String choose(String var1) throws  {
        try {
            return (String)super.h.invoke(this, m3, new Object[]{var1});
        } catch (RuntimeException | Error var3) {
            throw var3;
        } catch (Throwable var4) {
            throw new UndeclaredThrowableException(var4);
        }
        }

cglib:解决了有的类由于没有实现接口而无法被动态代理的问题。
        //JDK的动态代理是通过接口来强制转换的
        //生成以后的代理对象是可以强制转化为我们的接口

        //CGLib的动态代理是通过生成一个被代理对象的子类，然后重写了父类的方法
        //生成的对象，可以强制为被代理的对象（也就是用户自己写的类）
        //子类引用赋值给父类


        //此处的 Object o对象是CGLib帮我么new出来子类的对象
        //Java OOP,在new子类的同时，实际上默认先调用了我们super()的方法
        //new了父类的同时，必须向new出来父类，这也就是间接持有了我们父类的引用
        //我们改变了子类对象的某些属性，是可以间接的操作父类的属性的



观察者模式，发布订阅模式（Redis的pub sub）
手写单例模式。分别问了多线程情况如何处理，什么时候可以破坏单例模式，怎么防止单例模式被破坏
反射,序列化可以破坏单例模式
防止被破坏:私有构造方法中,加个判断如果已经创建过实例了,就不能让它再创建。
双重检查锁
public class Singleton {  
    private volatile static Singleton singleton;  
    private Singleton (){}  
    public static Singleton getSingleton() {  
    if (singleton == null) {  
        synchronized (Singleton.class) {  
        if (singleton == null) {  
            singleton = new Singleton();  
        }  
        }  
    }  
    return singleton;  
    }  
}  
public class Singleton {  
    private static Singleton instance = new Singleton();  
    private Singleton (){}  
    public static Singleton getInstance() {  
    return instance;  
    }  
}  

其实现非常简单，如下：可以防止被破坏。每个枚举实例都是static final类型的，也就表明只能被实例化一次

public enum Singleton {
    INSTANCE;
    private Singleton() {}
}

下面我们用一个枚举实现单个数据源例子来简单验证一下：
声明一个枚举，用于获取数据库连接。

public enum DataSourceEnum {
    DATASOURCE;
    private DBConnection connection = null;
    private DataSourceEnum() {
        connection = new DBConnection();
    }
    public DBConnection getConnection() {
        return connection;
    }
}  
使用：
DataSourceEnum.DATASOURCE.getConnection()
-----------------------------------------------------------------------------------


14、多线程？？？
http://blog.csdn.net/a511596982/article/list/2
AbstractQueuedSynchronizer(AQS):JUC并发包的锁机制都是基于AQS框架上构建的。AQS留给实现者的5个方法，比方说tryAcquire,tryRelease。AQS留给子类的int state状态，getState方法,setState方法。对于ReentrantLock
来说，state代表是重入次数。对于Semaphore来说state可以存蓄许可数量。对于countdownlatch来说state可以存蓄被countdown的次数。

Synchronized和Lock的区别？？？
http://blog.csdn.net/u012403290/article/details/64910926?locationNum=11&fps=1
java关键字     java类 
自动释放锁    在finally中必须释放锁，不然容易造成线程死锁
B线程一直等待     分情况而定，Lock有多个锁获取的方式，具体下面会说道，大致就是可以尝试获得锁，线程可以不用一直等待（trylock）
无法判断锁的状态     可以判断
可重入 不可中断  非公平    可重入 可中断  可以公平

早期的Synchronized是重量级锁，性能比较差，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的，挂起和唤醒线程需要底层操作系统的帮助。

怎么选择Synchronized(JVM的内置属性，具备进一步优化的可能性)和Lock（纯Java代码编写）？？？https://www.jianshu.com/p/1ea87c152413    https://mp.weixin.qq.com/s/CKZfytG_PNshXWWfVVAu0Q
Synchronized：映射成字节码指令就是增加来两个指令：monitorenter和monitorexit。当一条线程进行执行的遇到monitorenter指令的时候，它会去尝试获得锁（监视器），如果获得锁那么锁计数+1（为什么会加一呢，因为它是一个
              可重入锁，所以需要用这个锁计数判断锁的情况），如果没有获得锁，那么阻塞。当它遇到monitorexit的时候，锁计数器-1，当计数器为0，那么就释放锁。
              如果 Monitor 的进入数为 0，则该线程进入 Monitor，然后将进入数设置为 1，该线程即为 Monitor 的所有者。https://mp.weixin.qq.com/s/vSxEJMLdgjQHz3Bb0aPkyw
Lock：CAS乐观锁的体现，底层是主要靠volatile和CAS操作实现的，基于CAS尝试将state（锁数量）从0设置为1。
优先使用Synchronized，因为在jdk6之后对Synchronized进行了大部分的优化，比方说偏向锁、线程自旋、锁消除、锁粗化等等，除非需要用到高级功能：中断、定时、读写锁、公平锁。
对象监视器的理解：https://blog.csdn.net/qq_29842929/article/details/80999836

Thread 类中的start() 和 run() 方法有什么区别？？？
start()方法是启动一个新线程，而且start()内部调用了run()方法，如果直接调用run方法，没有启动新线程，而是在原来的线程中，执行run方法中的逻辑。

Java中Runnable和Callable有什么不同？？？
Runnable和Callable都代表那些要在不同的线程中执行的任务。Runnable从JDK1.0开始就有了，Callable是在JDK1.5增加的。
它们的主要区别是Callable的 call() 方法可以返回值和抛出异常，而Runnable的run()方法没有这些功能。

Java中如何停止一个线程？？？
jdk1.0本来有stop(释放锁)、suspend(不释放锁)、resume方法，已经被废弃，会引发死锁的。可以用interrupted()状态的判断出当前线程是否是停止状态，手动进行停止，
也可以volatile修饰一个布尔值变量，退出run（）方法的循环。或者使用线程阻塞类LockSupport。

如何在两个线程间共享数据？？？
利用wait()和notify()方法实现消费者和生成者模型实现共享数据。

多线程死锁的产生以及避免死锁？？？
死锁：因争夺资源，而造成相互等待的现象，若无外力，他们都将无法推进下去。
死锁必须满足以下条件：
互斥条件：一个资源每次只能被一个进程使用。
请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。
不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。
循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。
如何避免死锁：
打破互相等待的局面来避免死锁。为了达到这一点，你需要在代码中合理地安排获取和释放锁的顺序。如果获得锁的顺序是固定的，并且获得的顺序和释放的顺序刚好相反的话，
就不会产生出现死锁的条件了。
加锁时限、死锁检测；

Thread类中的yield方法有什么作用？？？
Yield方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃CPU占用而不能保证使其它线程一定能占用CPU，
执行yield()的线程有可能在进入到暂停状态后马上又被执行。

Thread类中的join方法有什么作用？？？
主线程创建并且启动了子线程,如果子线程需要进行大量的耗时运算,主线程往往将早于子线程结束之前结束,如果主线程想要等待子线程执行完毕后,获得子线程中处理的某个数据
就要用到join方法,方法join是等待线程对象被销毁(等待子线程执行完毕),其他线程才可以执行.

同步集合 和 并发集合？？？
不管是同步集合还是并发集合他们都支持线程安全，他们之间主要的区别体现在性能和可扩展性，还有他们如何实现的线程安全。同步HashMap, Hashtable, HashSet, Vector, 
ArrayList 相比他们并发的实现（比如：ConcurrentHashMap, CopyOnWriteArrayList, CopyOnWriteHashSet）会慢得多。造成如此慢的主要原因是锁， 同步集合会把整个Map
或List锁起来，而并发集合不会。并发集合实现线程安全是通过使用先进的和成熟的技术像锁剥离。比如ConcurrentHashMap 会把整个Map 划分成几个片段，
只对相关的几个片段上锁，同时允许多线程访问其他未上锁的片段。

CountDownLatch（闭锁）？？？
CountDownLatch这个类能够使一个线程等待其他线程完成各自的工作后再执行。CountDownLatch是通过一个计数器来实现的，计数器的初始值为线程的数量。
每当一个线程完成了自己的任务后，计数器的值就会减1。当计数器值到达0时，它表示所有的线程已经完成了任务，然后在闭锁上等待的线程就可以恢复执行任务。
使用场景：比如对于马拉松比赛，进行排名计算，参赛者的排名，肯定是跑完比赛之后，进行计算得出的，翻译成Java识别的预发，就是N个线程执行操作，
主线程等到N个子线程执行完毕之后，在继续往下执行。

Semaphore（信号量）？？？
在java中，使用了synchronized关键字和Lock锁实现了资源的并发访问控制，在同一时间只允许唯一了线程进入临界区访问资源(读锁除外)，
这样子控制的主要目的是为了解决多个线程并发同一资源造成的数据不一致的问题。在另外一种场景下，一个资源有多个副本可供同时使用，
比如打印机房有多个打印机、厕所有多个坑可供同时使用，这种情况下，Java提供了另外的并发访问控制--资源的多副本的并发访问控制。
原理：Semaphore是用来保护一个或者多个共享资源的访问， 内部维护了一个计数器，其值为可以访问的共享资源的个数。一个线程要访问共享资源，先获得信号量，
如果信号量的计数器值大于1，意味着有共享资源可以访问，则使其计数器值减去1，再访问共享资源。如果计数器值为0,线程进入休眠。当某个线程使用完共享资源后，
释放信号量，并将信号量内部的计数器加1，之前进入休眠的线程将被唤醒并再次试图获得信号量。


写出3条你遵循的多线程最佳实践？？？
给你的线程起个有意义的名字、避免锁定和缩小同步的范围、多用同步类少用wait 和 notify、多用并发集合少用同步集合；

Threadlocal用法???及其内存泄露???   参考性强！https://mp.weixin.qq.com/s/SNLNJcap8qmJF9r4IuY8LA
1、保存线程上下文信息，在任意地方都可以获取；2、线程私有，消除同步，线程安全。
1.1、打印日志：整个请求可以串起来；spring事务管理，用threadlocal存储connection，各个DAO都可以获取同一个connection，用于事务回滚和提交。
1.2、
https://www.cnblogs.com/dolphin0520/p/3920407.htmlThreadLocal在每个线程中对该变量会创建一个副本，即每个线程内部都会有一个该变量，且在线程内部任何地方都可以使用，线程之间互不影响，这样一来就不存在线程安全问题，也不会严重影响程序执行性能。
http://www.importnew.com/22039.html
ThreadLocal为什么会内存泄漏
ThreadLocalMap使用ThreadLocal的弱引用作为key，如果一个ThreadLocal没有外部强引用来引用它，那么系统 GC 的时候，这个ThreadLocal势必会被回收，这样一来，ThreadLocalMap中就会出现key为null的Entry，
就没有办法访问这些key为null的Entry的value，如果当前线程再迟迟不结束的话，这些key为null的Entry的value就会一直存在一条强引用链：Thread Ref -> Thread -> ThreaLocalMap -> Entry -> value永远无法回收，
造成内存泄漏。
其实，ThreadLocalMap的设计中已经考虑到这种情况，也加上了一些防护措施：在ThreadLocal的get(),set(),remove()的时候都会清除线程ThreadLocalMap里所有key为null的value。
为什么ThreadLocalMap使用ThreadLocal的弱引用作为key，而不是强引用？
如果是强引用：引用的ThreadLocal的对象被回收了，但是ThreadLocalMap（生命周期与线程一样长）还强引用着ThreadLocal，故ThreadLocal对象不能回收，需要手动删除，不然造成内存泄漏。
如果是弱引用：引用的ThreadLocal的对象被回收了，由于ThreadLocalMap持有ThreadLocal的弱引用，即使没有手动删除，ThreadLocal也会被回收。value在下一次ThreadLocalMap调用set,get，remove的时候会被清除。
static class ThreadLocalMap {
	      // 继承 WeakReference 所以map中的key对于threadlocal是弱引用
        static class Entry extends WeakReference<ThreadLocal<?>> {
            /** The value associated with this ThreadLocal. */
            Object value;

            Entry(ThreadLocal<?> k, Object v) {
                super(k);
                value = v;
            }
        }
threadlocal最佳实践：threadlocal对象声明建议使用static，也就是说在类第一次使用的时候装载，只分配一块空间。
try{}finaly{threallocal.remove()}


线程自旋和适应性自旋:
我们知道，java’线程其实是映射在内核之上的，线程的挂起和恢复会极大的影响开销。并且jdk官方人员发现，很多线程在等待锁的时候，在很短的一段时间就获得了锁，所以它们在线程等待的时候，并不需要把线程挂起，
而是让他无目的的循环，一般设置10次。这样就避免了线程切换的开销，极大的提升了性能。 
而适应性自旋，是赋予了自旋一种学习能力，它并不固定自旋10次一下。他可以根据它前面线程的自旋情况，从而调整它的自旋，甚至是不经过自旋而直接挂起。
如果自旋开销小于线程上下文切换，则使用自旋。竞争不激烈，使用自旋；竞争激烈，线程等待时间长，挂起更高效。

note：
1.  多线程实现同步的方式、互斥同步、非阻塞同步???
同步:多个线程并发访问共享数据时,保证共享数据在同一时刻只被一个线程使用.
互斥:是实现同步的一种方式,临界区\互斥量\信号量都是主要的互斥实现方法
sychronized 和 ReentrantLock实现(高级特性:可中断\公平锁\绑定多个条件:一个ReentrantLock对象可以同时绑定多个Condition对象)
非阻塞同步????   http://greemranqq.iteye.com/blog/2031806
synchronized  等锁机制是一种阻塞同步，虽然它完成了我们的原子性操作，和线程安全，但是这种阻塞同步机制是比较耗费性能的，因为在阻塞和唤醒等状态转换中，是需要CPU指令进行帮忙实现，这要的调度是比较耗时的，因此这种策略是一种悲观策略，当然我们需要线程安全，又要高效，在一定情况下我们会采用非阻塞同步机制。
同步,无非就是一个线程访问共享资源的时候,其他线程不能访问,非阻塞同步,比较乐观,简单的理解就是先干了再说,如果期间没有线程访问,我们的操作表示成功,如果期间有线程访问,酱紫就产生了冲突,那么就开始解决冲突,这样就不用把其他线程阻塞住.
 2.1 常见的非阻塞同步有：
      a.volatile 变量：轻量级的线程同步，不会引起线程调度，提供可见性，但是不提供原子性
      b.CAS 原子指令：轻量级线程同步，不会引起线程调度，提供可见性和原子性。
	  
	  我们知道提高并发性，应使得串行部分达到最大程度的并行，与锁相比，非阻塞算法机制，是直接操作机器指令，从指令层面协调多线程的竞争，使得线程在竞争公共资源的时候不会发生阻塞，减少了线程调度的开销，因此速度是由于锁的。
	  
	  在CAS 指令执行时，当V 的值符合A的预期的时候，新值B 才会更新，否则不会更新，这是一个原子操作。比如：A 地址指向V = 1,当一个线程准备更新V的值为B=2 额时候，检测到A 还是指向 V= 1的，那么允许修改，如果修改期间，检测到 A 已经指向 V = 其他值的时候，也就是说被其他线程改了就不会执行更新了。
	  
	    从上面CAS 的原理分析，假设变量i 的原始值是i=5,A 线程通过get() 方法，获取值等于V，然后这时候B线程用同样的方式获得V，然后改成了6,(中途可能被其他使用)，然后又改回成5.这时候 A线程去判断的时候，发现内存值还是5，说明没有改变，就执行更新。但是我们发现 在中途其实已经改变过了，又改变回来了而已，这就是ABA 问题。
          当然ABA 问题，表面上上不会影响你的业务逻辑，但是在有些情况下，发生这种中途 “调包” 的事情，还是会有影响。解决类似的问题的办法一般是加个版本号，更新了版本加1，每次比较的之后还要对版本进行比较，在JDK 里面是通过院子引用类“AtomicStampedReference” 进行处理的，具体的原理，我也没去看！
		  

2.   解释Synchronize关键字的锁优化技术，偏向锁，轻量级锁，重量级锁，这些锁是如何存储的，偏向锁撤销升级为轻量级锁的过程，结合源码解释
轻量锁能提升性能的原因是 ：认为大多数锁在整个同步周期都不存在竞争，所以使用 CAS 比使用互斥开销更少。但如果锁竞争激烈，轻量锁就不但有互斥的开销，还有 CAS 的开销，甚至比重量锁更慢。简单来说,没有判断是否上锁,直接就干,如果冲突就解决.
偏向锁
为了进一步的降低获取锁的代价，JDK1.6 之后还引入了偏向锁。
偏向锁的特征是:锁不存在多线程竞争，并且应由一个线程多次获得锁。
当线程访问同步块时，会使用 CAS 将线程 ID 更新到锁对象的 Mark Word 中，如果更新成功则获得偏向锁，并且之后每次进入这个对象锁相关的同步块时都不需要再次获取锁了。
这些锁是如何存储的???
锁存在Java对象头里。如果对象是数组类型，则虚拟机用3个Word（字宽）存储对象头，如果对象是非数组类型，则用2字宽存储对象头。在32位虚拟机中，一字宽等于四字节，即32bit。Java对象头里的Mark Word里默认存储对象的HashCode，分代年龄和锁标记位。32位JVM的Mark Word的默认存储结构如下：

3.volatile关键字语义，内存屏障如何实现，JMM对内存屏障做了哪些优化，volatile的语义增强???
　可见性，是指线程之间的可见性，一个线程修改的状态对另一个线程是可见的。也就是一个线程修改的结果。另一个线程马上就能看到。比如：用volatile修饰的变量，就会具有可见性。volatile修饰的变量不允许线程内部缓存和重排序，
即直接修改内存。所以对其他线程是可见的。但是这里需要注意一个问题，volatile只能让被他修饰内容具有可见性，但不能保证它具有原子性。比如 volatile int a = 0；之后有一个操作 a++；这个变量a具有可见性，但是a++ 依然是
一个非原子操作，也就是这个操作同样存在线程安全问题。
当对非 volatile 变量进行读写的时候，每个线程先从内存拷贝变量到CPU缓存中。如果计算机有多个CPU，每个线程可能在不同的CPU上被处理，这意味着每个线程可以拷贝到不同的 CPU cache 中。
而声明变量是 volatile 的，JVM 保证了每次读变量都从内存中读，跳过 CPU cache 这一步。


书本上的补充？？？
指令重排序：指令运行是个流水线工作（比方说 c= a+b 第一步加载a，第二步加载b，第三步add操作，这个操作是有个中断，因为执行这个的时候b的数据还没准备好），最怕的是流水线中断，指令重排就是为了减少中断
          的一个技术。有些指令不能重排：重排后的指令不能改变原有的串行语义，比如 a = 1;b=a+1;
          是指CPU采用了允许将多条指令不按照程序规定的顺序交给各相应电路单元处理。JVM能够根据处理器的特性（CPU的多级缓存系统、多核处理器等）适当的重新排序机器指令，使机器指令更符合CPU的执行特点，最大限度的发挥机器的性能。
终止线程：stop已经被废弃，因为使用stop会立刻终止线程，并且释放锁，导致数据不一致。
中断线程：interrupt()置上中断标志位，然后进行判断 isInterrupted(不会清除中断标志位)，补充，线程的sleep操作，由于中断会抛出异常，并且会清除中断标志位，可以在catch中使用interrupt置上标志位。
wait和notify：这两个方法都在Object中，如果一个线程调用obj.wait()方法，那么线程就会进入obj对象的等待队列中。当obj.notify()调用时，会去等待队列中随机挑选一个线程，并且将其唤醒。注意，wait和notify方法不是随时都可以进行调用的
              必须在对应的synchronzied语句中，必须要获得目标对象的监视器，wait会释放锁。另外注意，当notify之后唤醒一个等待线程，比方说T1，T1被唤醒之后，第一件事情并不是执行后续的代码，而是尝试重新获得锁，如果暂时无法获得，那
              只能继续等待！
              另外补充一点：wait后线程被中断，那么线程必须先获得锁，然后再抛出异常。线程中断的源码：设置中断标志位为true，并且通过parkevent的UNpark方法来唤醒线程。在object.wait阻塞的线程被唤醒之后，继续
              尝试获得锁，获取不到再park。在调用parkevent的park方法之前，会先判断线程的中断状态，如果为true，会清除当前线程的中断标志。
suspend和resume：线程挂起和继续执行，已经被废弃的方法，suspend不会释放锁，一旦resume执行比suspend早，那挂起的线程无法继续执行，并且它占用的那把锁无法释放，造成死锁。
join：当前线程等待目标线程执行完毕，main线程等待T1线程计算完毕，然后main线程拿到计算结果进行打印。t1.join()。main线程在t1线程上wait，t1线程执行完毕会notifyall(),while(isAlive){wait(0)};
yield：让出cpu，让其他线程一些工作机会。
volatile: 加了volatile修饰的变量可以保证线程的可见性（修改了，对其他线程立即可见，普通变量做不到这一点，普通变量的值在线程之间传递均需要通过主内存完成），但是不能保证线程安全，不能代替锁。
          保证有序性，禁止指令重排序，依靠内存屏障（不能把后面的指令重排序到内存屏障之前的位置）
线程组:相同功能的线程可以放置到一个线程组内。编码习惯，给线程和线程组取一个好听的名字，当系统出现问题，利于排查。
守护线程daemon: 与用户线程（真正执行业务逻辑的线程）相对应，顾名思义就是守护用户线程，如果用户线程全部结束，守护线程要守护的对象不存在了，那应用程序就自然而然的结束了。如果一个Java应用中，只有守护线程，Java虚拟机就会自然退出.
synchronized:指定对象加锁（进入同步代码块时，需要获得对象的锁），实例方法上面加锁（实际上对当前实例加锁），静态方法上加锁（对整个类进行加锁）。可以保证可见性和有序性，可以替换volatile。
ReentrantLock：重入锁，一个线程可以连续获得同一把锁，获得多少把锁，就必须释放多少把锁。
              与synchronized的区别：
              ReentrantLock可以中断响应，也就是在等待锁的过程中，程序可以根据需要取消对锁的请求，api是lock.lockInterruptibly()，这是一个可以对中断进行响应的锁申请动作。
              锁申请等待限时，lock.tryLock( 等待的时长 )，超过时长，不会继续阻塞直接返回false，时长以内获得锁，返回true，好处是避免了死锁。不带参数的tryLock()，可以获得锁返回true，获取不到锁则返回false。
              公平锁，不会产生线程饥饿，synchronized无法做到公平锁，ReentrantLock默认是非公平的，通过构造方法传入true参数，就可以变成公平锁，要维持一个有序队列，性能相对要低一点。
Condition条件：lock.newCondition(),condition.await()要求线程持有相关的重入锁,condition.singal()一般执行完之后，还要执行lock.unlock()方法，不然被唤醒的那个线程无法获得锁，无法继续执行。
Semaphore信号量：指定多个线程，访问同一个资源。acquire()获得许可，若无法获得，则会进行等待，直到线程release()释放一个许可或者当前线程被中断。tryAcquire(),立刻返回false or true.
ReadWriteLock：读写锁，读与读不互斥，读与写互斥，用锁分离的机制来进行提升性能。
CountDownLatch倒计时器：等到指定的线程个数（构造方法中指定）执行完任务后countdown()，在await()等待的线程才能继续执行。
CyclicBarrier循环栅栏zhà lan：阻止线程继续执行，要求在栅栏处进行等待，当凑齐指定的线程个数后（会进行计数），再继续执行，当然可以反复使用（取名为循环栅栏）。比如：司令要求10个士兵去执行一个任务，10个士兵先进行集合报道，接着一起去执行任务，当士兵
                     把手头的任务执行完毕后，司令就会对外宣布任务完成。构造方法参数（int , Runnable）一个是指定的线程个数，一个是计数器完成一个计数后，系统会执行的动作。
                     await()等待所有士兵集合，可以响应中断.
LockSupport：线程阻塞工具，可以在线程里面的任何位置进行阻塞，弥补了resume发生在前，导致无法继续执行的情况，和obj.wait()相比，不需要获得某个对象锁，也不会抛出InterruptedException。
             LockSupport.park();不会释放锁，LockSupport.unpark(t1);park()的时候，被中断，不会抛出异常，会默默返回，继续执行下面的代码。https://www.cnblogs.com/qingquanzi/p/8228422.html
fork/join：ForkJoin是Java7提供的原生多线程并行处理框架，其基本思想是将大任务分割成小任务，最后将小任务聚合起来得到结果。它非常类似于HADOOP提供的MapReduce框架，只是MapReduce的任务可以针对集群内的所有计算
           节点，可以充分利用集群的能力完成计算任务。ForkJoin更加类似于单机版的MapReduce。
           算法：分割的子任务会添加到当前工作线程的双端队列中，进入队列的头部。当一个工作线程中没有任务时，会从其他工作线程的队列尾部获取一个任务。

并发容器补充：
CopyOnWriteArrayList：适用于读多写少的场景，远远好于Vector。读读不阻塞，写入也不会阻塞读操作，写写是阻塞。copyonwrite写入的时候，进行了一次自我复制，将修改的东西写入副本中，写完之后，再将
                      修改完的副本替换原本的数据。这样子就可以保证写的时候，不影响读。
BlockingQueue：ArrayBlockingQueue数组结构适合做有界队列 LinkedBlockingQueue链表结构适合做无界队列。
ConcurrentLinkedQueue：高效的并发队列，链表结构。在高并发条件下，性能最好的队列。不使用锁而单纯的使用cas，不过需要在应用层面保证线程安全。
                       offer(E e)将指定元素插入此队列的尾部。poll() 获取并移除此队列的头，如果此队列为空，则返回 null。peek() 获取但不移除此队列的头；如果此队列为空，则返回 null
                                                       
锁优化补充：
减小锁的持有时间
减小锁粒度：比如jdk1.7的concurrenthashmap采用分段锁的思路，它内部细分了许多小的hashmap，称之为段（SEGMENT），默认情况下是16段，当进行新增的时候，并不是将整个hashmap进行加锁，而是先根据hashcode
          得到该表项应该存放在哪个段中，然后对该段进行加锁，并完成put操作。默认16个分段，幸运的话，concurrenthashmap可以接受16个线程同时插入，大大增加吞吐量。
读写分离锁来代替独占锁
锁分离：LinkedBlockingQueue基于链表的，take()作用于队列的前端，put()作用于队列的尾端，两者并不冲突，所以定义了两把锁takeLock 和 putLock。
锁粗化：如果对同一个锁不断的进行请求、同步和释放，其本身也会消耗系统的资源。为此，虚拟机会在遇到一连串连续的对同一把锁进行请求和释放的操作时，便会把所有的锁操作整合成对锁的一次请求，从而减少对锁的
       请求同步次数，这个动作叫做锁的粗化。比如：for循环里面有申请锁和释放锁的动作，改成在外层只请求一次锁。
锁消除：Java虚拟机在JIT编译时，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过锁消除，可以节省毫无意义的请求锁时间。比如定义一个局部变量StringBuffer，栈独有，不可能被其他线程访问，
       StringBuffer的内部加锁是没有必要的，虚拟机检测到，会把这些无用的锁操作进行去除。
锁偏向：（会使用 CAS 将线程 ID 更新到锁对象的 Mark Word 中）Java虚拟机对加锁操作的优化，一个线程获得了一把锁，那么锁就进入了偏向模式，当这个线程再次请求同一把锁时，无须再做任何同步操作，这样子大大节省了锁申请的操作，从而提高了性能。对于几乎没啥竞争的场合，
       偏向锁有比较好的优化效果。如果锁竞争比较激烈，每次都是不同的线程请求同一把锁，偏向锁的优化效果不佳。
轻量级锁：如果偏向锁失败，虚拟机并不会挂起线程，会进行轻量级锁的操作，简单的将对象头部作为指针，CAS指向持有锁的线程堆栈的内部，更新是否成功，来判断一个线程是否持有对象锁。如果线程获取轻量级锁成功，那么进入临界区
         如果失败，则表示其他线程抢占了锁，那么当前线程的锁请求就会膨胀为重量级锁。轻量级锁利用CAS操作避免了使用互斥量的开销，但如果存在竞争，除了互斥量的开销还有CAS操作，因为在有竞争的情况下，
         轻量级锁比重量级锁更慢。轻量级锁利用了乐观的策略，大部分情况下是没有存在竞争的。
自旋锁：锁膨胀后，虚拟机为了避免线程挂起，做了最后的努力--自旋锁，虚拟机会让线程做几次循环（自旋的含义），如果可以得到锁，那么进入临界区，如果不能，那么挂起。
自适应自旋锁：赋予锁一种学习能力，会根据前几次自旋次数获得锁，调整自旋的次数。
ThreadLocal：线程的局部变量，只有当前线程可以访问。SimpleDateFormat.parse()线程不安全，一种方式是加锁，另外一种方式是利用ThreadLocal为每个线程分配一个SimpleDateFormat。
             Thread内部有个成员变量ThreadLocal.ThreadLocalMap threadLocals = null;ThreadLocalMap的key是ThreadLocal实例(弱引用)，ThreadLocalMap的value是ThreadLocal.set的value,
             失去ThreadLocal实例的引用（ThreadLocal=null），有可能会导致value的回收。有时候我们使用了固定大小的线程池，线程总是存在的，回收value对象，最好的实践是，当你不用ThreadLocal里的value对象，
             必须要进行清理，不然容易导致内存泄漏，使用ThreadLocal.remove()。如果线程会退出，那不用使用remove，因为线程退出前，系统会进行回调，执行 threadLocals = null。

JDK并发包：atomic包、locks包、concurrenthashmap、blockingqueue、线程池相关、countdownlatch、cyclicbarrier等等。             

无锁补充：锁是一种悲观策略，假设每一次临界区操作都会发生冲突。无锁是一种乐观的策略，它会假设对资源的访问是没有冲突的，于是不需要等待，不停顿，假如遇到冲突了，就进行重试，比方说用cas（比较交换）的方式进行鉴别冲突。
        CAS不会发生阻塞，即不可能发生死锁，更重要的是，没有锁竞争带来的系统开销，也没有线程间频繁调度带来的开销。
        CAS算法：CAS(V,E,N)。V表示要更新的变量，E表示预期值，N表示新值。仅当V=E的时候，V的值才会更新为N，如果V!=E，说明其他线程抢先做了更新，当前线程可以放弃操作，也可以不断重试。
        JDK并发包里面有个atomic包里面的类有CAS的操作。
        Unsafe类内部的方法 public final native boolean compareAndSwapInt(Object var1, long var2, int var4, int var5);第一个参数是给定的对象，比方说AtomicInteger实例，第二个参数是对象内的偏移量（
        其实就是一个字段到对象头部的偏移量，通过这个偏移量可以快速定位到字段）比方说是AtomicInteger实例内部的private volatile int value;var4就是期待值，var5要更新成为的值，如果指定字段的值等于var4,那么这个字段更新成var5.
        我们的应用程序无法直接使用Unsafe类，它是一个JDK内部使用的专属类。
        带时间戳的对象引用能够解决ABA，AtomicStampedReference。
        让普通变量享受到原子操作：AtomicIntegerFieldUpdater，场景：由于前期考虑不周，后期的需求中，一些变量需要用到线程安全，简单的修改程序中每一个使用或者读取这个变量的地方，不符合开闭原则。
        死锁：jps   jstack查看堆栈。。。。。避免死锁：使用无锁的函数，使用重入锁，利用其中断或者限时等待。。。。。
        
解决伪共享问题：CPU cache的优化，为了提高cpu的速度，CPU有一个高速缓存cache，读写数据的最小单位是缓存行，如果两个变量在同一个缓存行，在多线程访问的时候，可能会相互影响彼此的性能。比方说，X和Y在
             同一个缓存行，cpu1更新了X，那么cpu2上的那一缓存行也会失效，导致cache无法命中，如果cpu经常无法命中cache，那系统的吞吐量就会下降。解决方案：X变量周围进行填充空间，使缓存行只有X一个
             变量。

disruptor框架：高并发下blockingqueue替换，使用了无锁cas加上解决伪共享问题加上各种消费策略的选择。应用场景自然就是“生产者-消费者”模型的应用场合了。环形队列。https://blog.csdn.net/qq_19558705/article/details/77247912

函数式结果@functionalinterface：只有一个抽象方法！Java1.8后可以在接口中定义实例方法：default void run(){......} 
lambda表达式可以访问局部变量，但是这个局部变量必须是final，可以不用定义为final，编译也能通过。
方法引用，静态方法 Classname::methodname 实例方法 instancereference::methodname 超类上的实例方法 super::methodname 类型上的实例方法 Classname::methodname
        构造方法 Class::new  数组构造方法 TypeName[]::new
一般来说，静态方法-》流内的元素会自动作为参数，实例方法-》流内的元素会自动作为调用目标。
jdk1.8串行方式改成并发方式非常简单，list.stream()  改成  list.parallel.parallelStream()
        int[] arraySort = {14, 3, 51, 7 , 5};
        Arrays.parallelSort(arraySort);// 并发的方式进行排序
jdk1.8新出的增强future：CompletableFuture 支持异步执行任务  流式调用  组合多个CompletableFuture
jdk1.8新出的读写锁的改进StampedLock，读写锁虽然使读和读之间可以并发，但是读和写之间依然是冲突的，读锁会完全阻塞写锁,它使用的依然是悲观的锁策略.如果有大量的读线程,他也有可能引起写线程的饥饿
     而StampedLock则提供了一种乐观的读策略,这种乐观策略的锁非常类似于无锁的操作,使得乐观锁完全不会阻塞写线程。
        StampedLock的小陷阱，挂起线程时，遇到中断，会直接返回（不会跟sleep方法一样，抛出异常），waiting状态编程runnable状态，一直存在并且耗尽cpu资源，直到自己抢占了锁。
        先尝试使用乐观读，如果在这期间，有线程申请到了写锁，乐观读锁就会失效，一方面可以再次尝试，另一方面可以把乐观读升级成悲观读。
        写锁和悲观读的策略：自旋尝试、加入等待队列、直到成功获取锁。
jdk1.8对原子类的增强：LongAdder。仿造concurrenthashmap，将热点数据进行分离，比方说将atomicInteger的value，分离成一个数组，每个线程访问，通过hash等算法映射到其中一个数字进行计算，
                    而最终的计算结果，是这个数组的求和累加。
                    LongAdder一开始并不会动用数组进行处理，所有数据记录在一个base的变量，如果多线程条件下，大家修改base没有冲突，那么没有必要扩展cell数组，但是一旦base修改发生冲突
                   ，就使用新的策略，如果cell数组依旧发生冲突，那么系统尝试创建新的cell，或者将cell的数组加倍，来减少冲突。
                   LongAccumulator是LongAdder的功能增强版本。

JMM内存模型补充？？？
java内存模型来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java在各个平台上都能达到一致性的内存访问效果。定义将变量（不包括局部变量和方法参数，因为这是线程私有的，不存在共享问题，即线程安全问题）存储到内存和从内存中
取出来这样的底层细节。
每条Java线程都有自己的工作内存（可以和CPU的高速缓存类比），线程的工作内存保存了该线程使用到的变量的主内存副本拷贝，线程的所有操作都必须在其工作内存中进行。不同线程之间无法访问对方的工作线程，只能通过主内存进行传递变量。
内存之间的交互操作：lock锁定、 unlock解锁、 read读取、 load载入到工作内存的变量副本中、 use使用、 assign赋值、 store存储到工作内存、 write写到主内存。
原子性、可见性和有序性：
                    原子性：read,load,assgin,use,store,write这些操作保证原子性的，基本类型数据的访问是具有原子性的。
                    可见性：普通变量和volatile都是通过主内存作为传递媒介的方式来实现可见性，但是volatile的特殊规则（内存屏障）保证了新值能够立即同步到主内存，以及每次使用前立即从主内存刷新。另外synchronized
                           和final这两个关键字也是可以实现可见性。
                    有序性：如果在本线程内观察，所有的操作都是有序的；如果一个线程观察另外一个线程，所有的操作都是无序的。volatile和synchronized可以保证有序性，前者因为禁止指令重排序，后者因为
                           一个变量在同一时刻允许一条线程对其进行lock操作。
先行发生原则happens-before：有序性不仅仅靠volatile和synchronized，Java语言中有一个先行发生原则，Java语言无需任何同步手段就能成立的先行发生规则。比如对volatile的写操作先行发生于后面对这个变量的读操作，Thread.start()的调用会happens-before于启动线程里面的动作。等等。
                          一个操作“时间上的先发生”不代表这个操作会是“先行发生”。我们在衡量并发问题的时候不要受到时间顺序的干扰，一切必须以先行发生原则为准。
内存屏障：volatile实现可见性的一项技术！！！！
   通过 Synchronized关键字包住的代码区域,当线程进入到该区域读取变量信息时,保证读到的是最新的值.这是因为在同步区内对变量的写入操作,在离开同步区时就将当前线程内的数据刷新到内存中,而对数据的读取也不能从缓存读取,
   只能从内存中读取,保证了数据的读有效性.这就是插入了StoreStore屏障
   使用了volatile修饰变量,则对变量的写操作,会插入StoreLoad屏障.             
                    
-----------------------------------------------------------------------------------


15、Java基础
接口 抽象区别????
1）抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法；jdk1.8接口中可以定义实例方法。
2）抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的；
3）接口中不能含有静态代码块以及静态方法，而抽象类可以有静态代码块和静态方法；jdk1.8接口中可以定义静态方法。
4）一个类只能继承一个抽象类，而一个类却可以实现多个接口。
抽象类是对一种事物的抽象，即对类抽象，而接口是对行为的抽象。抽象类是对整个类整体进行抽象，包括属性、行为，但是接口却是对类局部（行为）进行抽象。举个简单的例子，飞机和鸟是不同类的事物，但是它们都有一个共性，
就是都会飞。那么在设计的时候，可以将飞机设计为一个类Airplane，将鸟设计为一个类Bird，但是不能将 飞行 这个特性也设计为类，因此它只是一个行为特性，并不是对一类事物的抽象描述。此时可以将 飞行 设计为一个接口Fly，
包含方法fly( )，然后Airplane和Bird分别根据自己的需要实现Fly这个接口。然后至于有不同种类的飞机，比如战斗机、民用飞机等直接继承Airplane即可，对于鸟也是类似的，不同种类的鸟直接继承Bird类即可。从这里可以看出，
继承是一个 “是不是”的关系，而 接口 实现则是 “有没有”的关系。如果一个类继承了某个抽象类，则子类必定是抽象类的种类，而接口实现则是有没有、具备不具备的关系，比如鸟是否能飞（或者是否具备飞行这个特点），
能飞行则可以实现这个接口，不能飞行就不实现这个接口。

抽象类实现某个接口，可以不实现所有接口的方法，可以由它的子类实现。而普通类即非抽象类则必须实现接口里的全部方法。
普通类继承抽象类，要么实现抽象类的所有抽象方法，要么标记该普通类也是抽象类。


java多态??
就可以让引用变量绑定到各种不同的类实现上，从而导致该引用调用的具体方法随之改变，即不修改程序代码就可以改变程序运行时所绑定的具体代码，让程序可以选择多个运行状态，这就是多态性。
多态的优点
1. 消除类型之间的耦合关系
2. 可替换性
3. 可扩充性
4. 接口性
5. 灵活性
6. 简化性
多态存在的三个必要条件
继承
重写
父类引用指向子类对象
父类类型的引用可以调用父类中定义的所有属性和方法，对于只存在与子类中的方法和属性它就望尘莫及了!!!!
阐述一下快排基本原理。???
快速排序之所比较快，因为相比冒泡排序，每次交换是跳跃式的。每次排序的时候设置一个基准点，将小于等于基准点的数全部放到基准点的左边，将大于等于基准点的数全部放到基准点的右边。这样在每次交换的时候就不会像冒泡排序一
样每次只能在相邻的数之间进行交换，交换的距离就大的多了。因此总的比较和交换次数就少了，速度自然就提高了。当然在最坏的情况下，仍可能是相邻的两个数进行了交换。因此快速排序的最差时间复杂度和冒泡排序是一样的都是O(N2)，
它的平均时间复杂度为O(NlogN)。其实快速排序是基于一种叫做“二分”的思想。我们后面还会遇到“二分”思想，到时候再聊。
冒泡排序:
每次循环把最大的数移到最后,重复这样的步骤
public void bubbleSort(int array[]) {
        int t = 0;
        for (int i = 0; i < array.length - 1; i++)
            for (int j = 0; j < array.length - 1 - i; j++)
                if (array[j] > array[j + 1]) {
                    t = array[j];
                    array[j] = array[j + 1];
                    array[j + 1] = t;
                }
    }
选择排序:
第一次大循环把最小的数放到第一个,第二次大循环把最小的数放到第二个,依次重复.....


类的执行顺序！
(1)父类静态变量和静态代码块（先声明的先执行）;
(2)子类静态变量和静态代码块（先声明的先执行）;
(3)父类的非静态属性（变量）和非静态代码块（先声明的先执行）；
(4)父类构造函数
(5)子类的非静态属性（变量）和非静态代码块（先声明的先执行）；
(6)子类构造函数

关于String拼接字符串的性能问题？
在jdk5开始，用javac生成字节码时，在编译阶段就进行了优化。但是循环外和循环内优化方式不一样。
1、无变量的字符串拼接，在编译期间值都确定了，所以 javac 工具帮我们把它直接编译成一个字符常量。String s = "f" + "d" + "d"; javap反编译：String s = "fdd";
2、有变量的字符串拼接，在编译期间变量的值无法确定，所以运行期间会生成一个StringBuilder 对象。String i = "dd";String s = "f" + "d" + "d" + i; javap反编译：String s = new StringBuilder.append(..).... 
3、循环中使用字符串拼接，循环内，每循环一次就会产生一个新的 StringBuilder 对象，对资源有一定的损耗。必须要避免，使用4这种情况。
4、循环外使用 StringBuilder，循环内再执行 append() 方法拼接字符串，只会成一个 StringBuilder 对象。

因此，对于有循环的字符串拼接操作，建议使用 StringBuilder 和 StringBuffer，对性能会有一定的提升。

Java spi机制（服务提供发现机制）？？？Service Provider Interface 服务提供接口 例如：有个接口想在运行时才发现具体的实现类，那么你只需要在程序运行前添加一个实现即可，并把新加的实现描述给JDK即可。
https://www.jianshu.com/p/e4262536000d
案例：
jdbc4.0以前，开发人员还需要基于Class.forName("xxx")的方式来装载驱动，jdbc4也基于spi的机制来发现驱动提供商了，可以通过META-INF/services/java.sql.Driver文件里指定实现类的方式来暴露驱动提供者。

-----------------------------------------------------------------------------------

16、mybatis源码分析
回忆JDBC步骤
// 加载驱动
Class.forName("com.mysql.jdbc.Driver");
// 获取数据库连接
conn = DriverManager.getConnection(URL, USER, PASSWORD);
PreparedStatement ps = null;
ResultSet rs = null;
    try {
      ps = connection.prepareStatement("select * from user_info where id = ?");
      ps.setLong(1, id);
      rs = ps.executeQuery();
      while (rs.next()) {
        Long roleId = rs.getLong("id");
。。。。。
      catch(){}
      // 释放资源
      finally{ps.close;rs.close}
主角 Mybatis，主要做的事情，就是封装了上面的除SQL语句之外的重复代码，为什么说是重复代码呢？因为这些代码，细想一下，都是不变的。
实际上，Mybatis 只做了两件事情： 
1. 根据 JDBC 规范 建立与数据库的连接。 
2. 通过反射打通Java对象和数据库参数和返回值之间相互转化的关系。

阅读mybatis源码之前首先得知道：Mybatis 的运行可以分为2个部分,第一部分是读取配置文件创建 Configuration 对象, 用以创建 SqlSessionFactroy, 第二部分是 SQLSession 的执行过程.
mybatis几个重要类的生命周期：
1. SqlSessionaFactoryBuilder 该类主要用于创建 SqlSessionFactory, 并给与一个流对象, 该类使用了创建者模式, 如果是手动创建该类(这种方式很少了,除非像楼主这种测试代码), 那么建议在创建完毕之后
   立即销毁.
2. SqlSessionFactory 该类主要是用于创建SqlSession，生命周期跟mybatis保持同步，工厂模式和单例模式。
3. SqlSession是个会话，就像 HTTP 请求中的会话一样，每次访问数据库都需要这样一个会话，每次创建完之后，必须在finally里面关闭它，另外它是一个线程不安全对象，操作数据库需要注意隔离级别和数据库锁。
4. Mapper 映射器， 正如我们编写的那样, Mapper 是一个接口, 没有任何实现类, 他的作用是发送 SQL, 然后返回我们需要的结果. 或者执行 SQL 从而更改数据库的数据, 因此它应该在 SqlSession 的事务方
   法之内, 在 Spring 管理的 Bean 中, Mapper 是单例的。
   
源码走起：第一篇源码入口是sqlSession.selectOne(“org.apache.ibatis.mybatis.UserInfoMapper.selectById”, parameter) 
1、初始化Configuration 创建SqlSessionFactory
2、SqlSession的创建过程                               // 默认的执行器 “SIMPLE”，自动提交没有配置的话，默认是false
   调用DefaultSqlSessionFactory的openSession方法  return openSessionFromDataSource(configuration.getDefaultExecutorType(), null, false);
    Transaction tx = null;
    try {
    	// 获取配置文件中的环境
      final Environment environment = configuration.getEnvironment();
      // 根据环境的配置，获得事务工厂  没配置默认ManagedTransactionFactory   我们配置了type="JDBC"  所以返回JdbcTransactionFactory
      final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment);
      // 创建事务对象 level是TransactionIsolationLevel事务隔离级别 本文为null  本文是JdbcTransaction 该类都是有关连接和事务的方法，比如commit，openConnection，rollback，和
      // JDBC 的connection 功能很相似
      tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit);
      // 获取一个执行器类 Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。
      // SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。
      // ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map
      final Executor executor = configuration.newExecutor(tx, execType);
      // 生成SqlSession对象
      return new DefaultSqlSession(configuration, executor, autoCommit);
    } catch (Exception e) {
      closeTransaction(tx); // may have fetched a connection so lets call close()
      throw ExceptionFactory.wrapException("Error opening session.  Cause: " + e, e);
    } finally {
      ErrorContext.instance().reset();
    }
3、SqlSession 执行过程
   调用DefaultSqlSession的selectOne方法
   public <T> T selectOne(String statement, Object parameter) {
    // 本文中的statement是一个key，它是由mapper接口的全路径再加上方法名称，比如“com.xavier.mapper.UserInfoMapper.selectById”
    List<T> list = this.<T>selectList(statement, parameter);
    if (list.size() == 1) {
      return list.get(0);
    } else if (list.size() > 1) {
    	// 这个异常是不是很常见的呀！！
      throw new TooManyResultsException("Expected one result (or null) to be returned by selectOne(), but found: " + list.size());
    } else {
      return null;
    }
  }
  
  @Override
  public <E> List<E> selectList(String statement, Object parameter, RowBounds rowBounds) {
    try {
    	// 根据statement这个key获得MappedStatement 获取不到会抛出异常 
      MappedStatement ms = configuration.getMappedStatement(statement);
      return executor.query(ms, wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER);
    } catch (Exception e) {
      throw ExceptionFactory.wrapException("Error querying database.  Cause: " + e, e);
    } finally {
      ErrorContext.instance().reset();
    }
  }
  
  @Override
  public <E> List<E> query(MappedStatement ms, Object parameter, RowBounds rowBounds, ResultHandler resultHandler) throws SQLException {
    // 参数绑定对象，里面包括sql语句、参数
    BoundSql boundSql = ms.getBoundSql(parameter);
    // 创建一个缓存key 注意：mybatis 一级缓存默认为true，二级缓存默认false。创建缓存的过程很简单，就是将所有的参数的key或者id构造该 CacheKey 对象，使该对象唯一。最后执行query方法：
    CacheKey key = createCacheKey(ms, parameter, rowBounds, boundSql);
    return query(ms, parameter, rowBounds, resultHandler, key, boundSql);
  }
  
  @Override
  public <E> List<E> query(MappedStatement ms, Object parameterObject, RowBounds rowBounds, ResultHandler resultHandler, CacheKey key, BoundSql boundSql)
      throws SQLException {
    Cache cache = ms.getCache();
    if (cache != null) {
      flushCacheIfRequired(ms);
      if (ms.isUseCache() && resultHandler == null) {
        ensureNoOutParams(ms, parameterObject, boundSql);
        @SuppressWarnings("unchecked")
        List<E> list = (List<E>) tcm.getObject(cache, key);
        if (list == null) {
          list = delegate.<E> query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);
          tcm.putObject(cache, key, list); // issue #578 and #116
        }
        return list;
      }
    }
    // 执行SimpleExecutor的query方法
    return delegate.<E> query(ms, parameterObject, rowBounds, resultHandler, key, boundSql);
  }
  
  @Override
  protected Statement instantiateStatement(Connection connection) throws SQLException {
    // 是不是很眼熟哇 回忆JDBC编程的 connection.prepareStatement 代码，由此证明mybatis 就是封装了 JDBC
    String sql = boundSql.getSql();
    if (mappedStatement.getKeyGenerator() instanceof Jdbc3KeyGenerator) {
      String[] keyColumnNames = mappedStatement.getKeyColumns();
      if (keyColumnNames == null) {
        return connection.prepareStatement(sql, PreparedStatement.RETURN_GENERATED_KEYS);
      } else {
        return connection.prepareStatement(sql, keyColumnNames);
      }
    } else if (mappedStatement.getResultSetType() != null) {
      return connection.prepareStatement(sql, mappedStatement.getResultSetType().getValue(), ResultSet.CONCUR_READ_ONLY);
    } else {
      return connection.prepareStatement(sql);
    }
  }
  
    @Override
  public void setNonNullParameter(PreparedStatement ps, int i, Integer parameter, JdbcType jdbcType)
      throws SQLException {
    // 是不是很熟悉哇   回忆JDBC编程的设置参数步骤
    ps.setInt(i, parameter);
  }
  
    @Override
  public <E> List<E> query(Statement statement, ResultHandler resultHandler) throws SQLException {
    // 是不是很熟悉哇   回忆JDBC编程的 ps.executeQuery();
    PreparedStatement ps = (PreparedStatement) statement;
    ps.execute();
    return resultSetHandler.<E> handleResultSets(ps);
  }
   
   @Override
  public void close(boolean forceRollback) {
    // 最后一步进行close资源
    try {
      //issues #499, #524 and #573
      if (forceRollback) { 
        tcm.rollback();
      } else {
        tcm.commit();
      }
    } finally {
      delegate.close(forceRollback);
    }
  }

4、总结
还记得我们刚开始说的吗？mybatis 做的什么工作?
1. 根据 JDBC 规范 建立与数据库的连接。 
2. 通过反射打通Java对象和数据库参数和返回值之间相互转化的关系。
还有Mybatis 的运行过程？
1. 读取配置文件创建 Configuration 对象, 用以创建 SqlSessionFactroy.
2. SQLSession 的执行过程.
// 我们也知道了其实在mybatis 层层封装下，真正做事情的是 StatementHandler
public abstract class BaseStatementHandler implements StatementHandler {
  protected final Configuration configuration;// 配置
  protected final ObjectFactory objectFactory;// 对象工厂
  protected final TypeHandlerRegistry typeHandlerRegistry;// 类型处理器
  protected final ResultSetHandler resultSetHandler;// 结果集处理器
  protected final ParameterHandler parameterHandler;// 参数处理器
  protected final Executor executor;// SQL执行器
  protected final MappedStatement mappedStatement;// 映射器
  protected final RowBounds rowBounds;// 分页对象
  protected BoundSql boundSql;// 绑定SQL和参数对象
当然，StatementHandler 只是 SqlSession 4 大对象的其中之一，还有Executor 执行器，他负责调度 StatementHandler，ParameterHandler，ResultHandler 等来执行对应的SQL，
而 StatementHandler 的作用是使用数据库的 Statement(PreparedStatement ) 执行操作，他是4大对象的核心，起到承上启下的作用。ParameterHandler 就是封装了对参数的处理，ResultHandler 
封装了对结果级别的处理。


第二篇源码走起。。。。
1、入口是UserInfoMapper userInfoMapper = sqlSession.getMapper(UserInfoMapper.class);
     UserInfo userInfo2 = userInfoMapper.selectById(1);
     
  // DefaultSqlSession 的 getMapper方法
  @Override
  public <T> T getMapper(Class<T> type) {
    // 其实调用了configuration 的get方法
    return configuration.<T>getMapper(type, this);
  }
  
  // 使用jdk动态代理，创建代理对象  MapperProxy该类实现了 InvocationHandler 接口，因此我们需要看看他的 invoke 方法：  
  protected T newInstance(MapperProxy<T> mapperProxy) {
  	// 参数：目标对象的类加载器、目标对象的所有接口、InvocationHandler子类
    return (T) Proxy.newProxyInstance(mapperInterface.getClassLoader(), new Class[] { mapperInterface }, mapperProxy);
  }

  public T newInstance(SqlSession sqlSession) {
    final MapperProxy<T> mapperProxy = new MapperProxy<T>(sqlSession, mapperInterface, methodCache);
    return newInstance(mapperProxy);
  }
  
  // mapperProxy的invoke方法
  @Override
  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
    try {
      if (Object.class.equals(method.getDeclaringClass())) {
        return method.invoke(this, args);
      } else if (isDefaultMethod(method)) {
        return invokeDefaultMethod(proxy, method, args);
      }
    } catch (Throwable t) {
      throw ExceptionUtil.unwrapThrowable(t);
    }
    // 走到这边：这个方法跟缓存相关 
    final MapperMethod mapperMethod = cachedMapperMethod(method);
    // 然后执行，
    return mapperMethod.execute(sqlSession, args);
  }
  
  // 这是上段代码的execute方法的片段，是不是一目了然了，也就是说getMapper 最终还是调用 SqlSession 的 selectOne 方法，只不过通过动态代理封装了一遍，让mybatis 来管理这些字符串样式的key，
  // 而不是让用户来手动管理。
  case SELECT:
        if (method.returnsVoid() && method.hasResultHandler()) {
          executeWithResultHandler(sqlSession, args);
          result = null;
        } else if (method.returnsMany()) {
          result = executeForMany(sqlSession, args);
        } else if (method.returnsMap()) {
          result = executeForMap(sqlSession, args);
        } else if (method.returnsCursor()) {
          result = executeForCursor(sqlSession, args);
        } else {
          Object param = method.convertArgsToSqlCommandParam(args);
          result = sqlSession.selectOne(command.getName(), param);
        }
        break;
2、总结
文章不长，因为主要逻辑在上一篇，这里只不过将 Mybatis 如何封装代理的过程解析了一遍。


第三篇源码走起：如何整合Spring
1. SqlSessionFactory，SqlSession 如何生成?

@Bean(name = "sqlSessionFactory")
  public SqlSessionFactory sqlSessionFactoryBean() {
    // 实现FactoryBean
    SqlSessionFactoryBean bean = new SqlSessionFactoryBean();
    bean.setDataSource(dataSource());
    bean.setTypeAliasesPackage(TYPE_ALIASES_PACKAGE);
    // 添加插件
    bean.setPlugins(MybatisUtil.getInterceptor());
    // 添加XML目录
    ResourcePatternResolver resolver = new PathMatchingResourcePatternResolver();
    bean.setMapperLocations(resolver.getResources("classpath:mapper/*.xml"));
    // 获得SqlSessionFactory
    return bean.getObject();
  }
  
  @Bean
  public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) {
    // 有了sqlSessionFactory 创建 SqlSessionTemplate
    return new SqlSessionTemplate(sqlSessionFactory);
  }

// SqlSessionTemplate有成员变量private final SqlSession sqlSessionProxy;
// 很多方法select update insert delete  最后调用的是sqlSessionProxy.select update insert delete
public SqlSessionTemplate(SqlSessionFactory sqlSessionFactory, ExecutorType executorType,
      PersistenceExceptionTranslator exceptionTranslator) {
    this.sqlSessionFactory = sqlSessionFactory;
    this.executorType = executorType;
    this.exceptionTranslator = exceptionTranslator;
    // 生成代理对象  SqlSessionInterceptor是InvocationHandler的子类  重点看下这个类的invoke方法
    // this.sqlSessionProxy 就是SqlSession对象
    this.sqlSessionProxy = (SqlSession) newProxyInstance(
        SqlSessionFactory.class.getClassLoader(),
        new Class[] { SqlSession.class },
        new SqlSessionInterceptor());
  }
  
  private class SqlSessionInterceptor implements InvocationHandler {
    // 每次调用sqlSessionProxy的方法，都要执行invoke方法
    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
      SqlSession sqlSession = getSqlSession(
          SqlSessionTemplate.this.sqlSessionFactory,
          SqlSessionTemplate.this.executorType,
          SqlSessionTemplate.this.exceptionTranslator);
      try {
      	// sqlSession是目标对象
        Object result = method.invoke(c, args);
        // 检测事务是否由spring管理
        if (!isSqlSessionTransactional(sqlSession, SqlSessionTemplate.this.sqlSessionFactory)) {
          // force commit even on non-dirty sessions because some databases require
          // a commit/rollback before calling close()
          sqlSession.commit(true);
        }
        return result;
      } catch (Throwable t) {
        Throwable unwrapped = unwrapThrowable(t);
        if (SqlSessionTemplate.this.exceptionTranslator != null && unwrapped instanceof PersistenceException) {
          // release the connection to avoid a deadlock if the translator is no loaded. See issue #22
          closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory);
          sqlSession = null;
          Throwable translated = SqlSessionTemplate.this.exceptionTranslator.translateExceptionIfPossible((PersistenceException) unwrapped);
          if (translated != null) {
            unwrapped = translated;
          }
        }
        throw unwrapped;
      } finally {
        if (sqlSession != null) {
          closeSqlSession(sqlSession, SqlSessionTemplate.this.sqlSessionFactory);
        }
      }
    }
  }
  
2. Mapper 代理如何生成？如何运行？
// 启动类上面加上一个注解
@MapperScan(basePackages = "cn.think.in.java.mapper")
// 关注@Import注解 在 IOC 启动的时候，会加载该注解标识的类，也就是 MapperScannerRegistrar.class 并且会调用 registerBeanDefinitions方法
@Retention(RetentionPolicy.RUNTIME)
@Target(ElementType.TYPE)
@Documented
@Import(MapperScannerRegistrar.class)
public @interface MapperScan {

在registerBeanDefinitions方法中最关键的一点是执行 ClassPathMapperScanner 的 doscan 方法
  @Override
  public Set<BeanDefinitionHolder> doScan(String... basePackages) {
    Set<BeanDefinitionHolder> beanDefinitions = super.doScan(basePackages);

    if (beanDefinitions.isEmpty()) {
      logger.warn("No MyBatis mapper was found in '" + Arrays.toString(basePackages) + "' package. Please check your configuration.");
    } else {
    	// 重要：该方法会将制定包下的 Mapper 接口改成 mapperFactoryBean 的类型，也就是说，Spring getBean 返回的就是 mapperFactoryBean 类型
    	// definition.setBeanClass(this.mapperFactoryBean.getClass());
    	// 另外会加入一些和 SqlSession 相关的属性
      processBeanDefinitions(beanDefinitions);
    }
    return beanDefinitions;
  }
既然上文提到，接口的 Bean 的类型改为 mapperFactoryBean 类型。 那么 mapperFactoryBean 是什么呢？
  @Override
  public T getObject() throws Exception {
  	// 这个方法是不是很熟悉哇，我们终于知道了为什么在 ClassPathMapperScanner 的 doScan 方法中要将接口的 Bean 定义的改成 MapperFactorybean ，原来最终的目的就是调用 getObject 方法，
  	// 然后调用 getMapper 方法。并且设置的那些属性就是在对 MapperFactoryBean 的父类 SqlSessionDaoSupport 的属性进行赋值。
    return getSqlSession().getMapper(this.mapperInterface);
  }
首先implements FactoryBean<T>，那必然得关注getObject方法，通常会在这个方法中做一些手脚。
其次extends SqlSessionDaoSupport，拿到了 设置 SqlSessionFactory 和 SqlSessionTemplate，getSession 等方法。
其次MapperFactorybean 还间接实现了 InitializingBean 接口，也就是 Spring 留给我们的扩展接口。 需要重写 afterPropertiesSet 方法。
@Override
	public final void afterPropertiesSet() throws IllegalArgumentException, BeanInitializationException {
		// 调用 configuration 的 addMapper 方法，该方法底层调用了 MapperRegistry 的 addMapper 方法，将 namespace 属性和 配置文件关联。
		checkDaoConfig();
		try {
			// 空方法
			initDao();
		}
		catch (Exception ex) {
			throw new BeanInitializationException("Initialization of DAO failed", ex);
		}
	}
最后总结来一波：
1、IOC 通过注解扫描指定包名，在初始化的时候调用@MapperScan 注解中指定的类最终执行 doScan 方法，将所有的 Mapper 接口的 Bean 定义都改成 FactoryBean 的子类 MapperFactoryBean，
   并将该 SqlSessionFactory 和 SqlSessionTemplate 添加到该类中。
2、Spring IOC 在实例化该 Bean 的时候，需要传入接口类型，并将 SqlSessionFactory 和 SqlSessionTemplate 注入到该 Bean 中。并调用 configuration 的 addMapper 方法，解析配置文件。
3、当调用 MapperFactoryBean 的 getObject 方法的时候，事实上是调用 SqSession 的 getMapper 方法，而这个方法会返回一个动态代理对象。所有对这个对象的方法调用都是底层的 SqlSession 的方法。
而 Spring 和 MyBatis 的整合也和 AOP 相似，都是通过 Spring 留下的扩展接口 FactoryBean 来实现的。在 FactoryBean 中包装了 SqlSession ，而 SqlSession 则会返回代理。
在SqSessionTemplate 中，底层还是调用 Mybatis 自己的SqlSession 创建动态代理来实现的。可谓万变不离其宗。


第四篇源码走起：插件的原理和应用
1、插件的原理
mybatis 不像 Spring ，留了很多的接口给使用者扩展，只留了一个接口给开发者扩展，接下去开始讲插件。
mybatis四大对象在创建过程中，比如newExecutor的过程中，调用了 interceptorChain.pluginAll(xxx)方法，可见这个方法是非常重要的，该方法是拦截器链调用插件方法，传入一个对象，返回该对象。
public class InterceptorChain {

  private final List<Interceptor> interceptors = new ArrayList<Interceptor>();
  // 观察此方法的调动栈 四大对象 在newXxxx的创建的时候 会进行调用
  public Object pluginAll(Object target) {
    for (Interceptor interceptor : interceptors) {
      target = interceptor.plugin(target);
    }
    return target;
  }
  // 首先得看看拦截器插件是怎么添加进去的  观察该方法的调用栈  发现在解析XML配置文件的时候，将配置好的插件添加进集合的 pluginElement(root.evalNode("plugins"));
  public void addInterceptor(Interceptor interceptor) {
    interceptors.add(interceptor);
  }
  
  public List<Interceptor> getInterceptors() {
    return Collections.unmodifiableList(interceptors);
  }
}
// 然后再来看看interceptor.plugin(target);方法
   @Override
    public Object plugin(Object target) {
        return Plugin.wrap(target, this);
    }
// 静态方法
public static Object wrap(Object target, Interceptor interceptor) {
	  // 根据Interceptor的注解，获取map，key指的是需要拦截类型，value是需要拦截的方法
    Map<Class<?>, Set<Method>> signatureMap = getSignatureMap(interceptor);
    Class<?> type = target.getClass();
    // 目标对象是否匹配signatureMap中key的类型  满足的话即>0，生成代理对象
    Class<?>[] interfaces = getAllInterfaces(type, signatureMap);
    if (interfaces.length > 0) {
      return Proxy.newProxyInstance(
          type.getClassLoader(),
          interfaces,
          new Plugin(target, interceptor, signatureMap));
    }
    return target;
  }
// 承接上段代理  生成代理对象  我们得看下InvocationHandler子类Plugin的invoke方法
public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
    try {
      Set<Method> methods = signatureMap.get(method.getDeclaringClass());
      // 方法是否匹配 匹配的话进行拦截 intercept可以加上自己的逻辑
      if (methods != null && methods.contains(method)) {
        return interceptor.intercept(new Invocation(target, method, args));
      }
      // 调用目标类的方法
      return method.invoke(target, args);
    } catch (Exception e) {
      throw ExceptionUtil.unwrapThrowable(e);
    }
  }

2、插件的应用
首先编写 mybatis 插件需要遵守几个约定： 
1. 实现 Interceptor 接口并实现接口中的方法。 
2. 在配置文件中配置插件。 
3. 在实现 Interceptor 接口的类上加上 @Intercepts 注解。
比如国内著名的分页插件：
/**
 * Mybatis - 通用分页拦截器<br/>
 * 项目地址 : http://git.oschina.net/free/Mybatis_PageHelper
 *
 * @author liuzh/abel533/isea533
 * @version 5.0.0
 */
@SuppressWarnings({"rawtypes", "unchecked"})
@Intercepts(
    {
        @Signature(type = Executor.class, method = "query", args = {MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class}),
        @Signature(type = Executor.class, method = "query", args = {MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class, CacheKey.class, BoundSql.class}),
    }
)
public class PageInterceptor implements Interceptor {...}

@Documented
@Retention(RetentionPolicy.RUNTIME)
@Target({})
public @interface Signature {// 这是一个方法签名的注解
  Class<?> type();// 要拦截的类型
  String method();// 要拦截的类型下面的哪个方法
  Class<?>[] args();// 要拦截的类型下面的哪个方法，具体到参数（方法有可能会重载）
}
四大对象的执行顺序，对于开发插件来说，是非常重要的：
首先执行了 executor 的方法，然后执行了 StatementHandler 的拦截方法， 再执行 ParameterHandler 的方法，再执行 ResultSetHandler 的拦截器，最后执行 executor 真正的查询方法。

总结来一波吧！
我们分析了 mybatis 中常用的插件，知道了他的原理，就是每次创建4大对象的时候，都会将场景封装到对象中，如果有多个，就层层包装。这个是通过动态代理的技术实现的。然后在运行的时候会调用实现了动态代理 
InvocationHandler 接口的 Plugin 类的 invoke 方法，而该方法会调用拦截器器的 intercept 方法，并传入封装了目标对象，目标方法，目标方法参数的 Invocation 供使用者修改或加强。
修改 Sql 有多种方式，最终都是修改 StatementHandler 的 BoundSql 中的 sql 字段，无论是直接修改属性，还是重新创建一个 BoundSql 对象。还有一个 mybatis 的 MetaObject 类，该类是 mybatis 
提供的一个强大的通过反射修改对象属性的工具类，mybatis 中多次使用该类。
在我们的项目中，通过 mybatis 的拦截器可以实现很多功能，比如分页插件，再比如 分表插件，因为如果一张表中数据过大，会拆分为多个表，这个时候可以通过一些特定的参数，将表的后缀加上去，起到自动分表的效果。
而 XML 中的 SQL 是感知不到的。
总之，mybatis 插件可以实现很多功能。但使用他的时候请一定小心，毕竟这修改了 mybatis 底层的逻辑。


mybatis的两种分页方式:RowBounds和PageHelper
https://blog.csdn.net/peach_garden/article/details/80062793


-----------------------------------------------------------------------------------
18、NIO NETTY？？？
了解Unix网络编程的IO模型：
一。IO请求的两个阶段：
1.等待资源阶段(数据准备阶段)：IO请求一般需要请求特殊的资源（如磁盘、RAM、文件），当资源被上一个使用者使用没有被释放时，IO请求就会被阻塞，直到能够使用这个资源。
2.使用资源阶段(内核空间复制回用户进程缓冲区阶段)：真正进行数据接收和发生。

二。在等待数据阶段，IO分为阻塞IO和非阻塞IO。
1.阻塞IO： 资源不可用时，IO请求一直阻塞，直到反馈结果（有数据或超时）。
2.非阻塞IO：资源不可用时，IO请求离开返回，返回数据标识资源不可用

三。在使用资源阶段，IO分为同步IO和异步IO。
1.同步IO：应用阻塞在发送或接收数据的状态，直到数据成功传输或返回失败。
2.异步IO：应用发送或接收数据后立刻返回，数据写入OS缓存，由OS完成数据发送或接收，并返回成功或失败的信息给应用。

形象的例子来一波：https://mp.weixin.qq.com/s?__biz=Mzg3MjA4MTExMw==&mid=2247484746&idx=1&sn=c0a7f9129d780786cabfcac0a8aa6bb7&source=41#wechat_redirect 这个例子比较有代表性的
阻塞IO模型：A同学用杯子装水，打开水龙头装满水然后离开，如果水龙头没出来水，那她也必须等待水装满了才去做其他事情。同步IO。
非阻塞IO模型：B同学也用杯子装水，打开水龙头后发现没有水，它离开了，过一会他又拿着杯子来看看……在中间离开的这些时间里，B同学离开了装水现场(回到用户进程空间)，可以做他自己的事情。同步IO。
IO复用模型(reactor模型)：C同学来装水，发现有一排水龙头，舍管阿姨告诉他这些水龙头都还没有水，等有水了告诉他。于是等啊等(select调用中)，过了一会阿姨告诉他有水了，但不知道是哪个水龙头有水，自己看吧。
          于是C同学一个个打开，往杯子里装水(recv)。同步IO。
信号驱动IO模型：D同学让舍管阿姨等有水的时候通知他(注册信号函数)，没多久D同学得知有水了，跑去装水。是不是很像异步IO？很遗憾，它还是同步IO(省不了装水的时间啊)。
异步IO模型：调用aio_read，让内核等数据准备好，并且复制到用户进程空间后执行事先指定好的函数。E同学让舍管阿姨将杯子装满水后通知他。整个过程E同学都可以做别的事情(没有recv)，这才是真正的异步IO。

补充：
JavaIO阻塞模型：传统的IO流都是阻塞的，当一个线程调用read或者write时候，该线程被阻塞，直到有一些数据被读取或写入，该线程在此期间不能执行其他任务。因此，在完成网络通信进行 IO 操作时，由于线程会阻塞，
所以服务器端必须为每个客户端都提供一个独立的线程进行处理，当服务器端需要处理大量客户端时，性能急剧下降。注意：在阻塞IO操作的过程中，用来提高程序的解决方案一般是使用多线程来处理，但是开辟线程也是比较
耗费资源的。
JavaNIO非阻塞模型：当线程从某通道进行读写数据时，若没有数据可用时，该线程可以进行其他任务。线程通常将非阻塞 IO 的空闲时间用于在其他通道（channel）上执行 IO 操作，所以单独的线程可以管理多个输入和
输出通道。因此， NIO 可以让服务器端使用一个或有限几个线程来同时处理连接到服务器端的所有客户端。
使用NIO的步骤：1）创建一个channel注册到selector 2）selector会实时监控channel，channel准备就绪了，才会分配一个或者多个线程继续去执行channel的请求。
Reactor模式：是网络编程中的一种设计模式，Java NIO 就是采用了这种模型，selector.select(); 这个函数会监听channel，如果没有事件发生，则阻塞，有事件发生，则用执行相对应handle函数。
这项技术叫做多路io复用，所以Reactor是建立在多路io复用基础上的。https://blog.csdn.net/u010900754/article/details/77601289  https://mp.weixin.qq.com/s/dFumpp1wBO1NWes3AxVBRw
Reactor模式概念：首先是事件驱动的，有一个或多个并发输入源，有一个Service Handler，有多个Request Handlers；这个Service Handler会同步的将输入的请求（Event）多路复用的分发给相应的Request Handler。
从结构上，这有点类似生产者消费者模式，即有一个或多个生产者将事件放入一个Queue中，而一个或多个消费者主动的从这个Queue中Poll事件来处理；而Reactor模式则并没有Queue来做缓冲，每当一个Event输入到
Service Handler之后，该Service Handler会主动的根据不同的Event类型将其分发给对应的Request Handler来处理。
为什么出现Reactor模式：原先是采用线程池的方式，一个线程负责连接、读取和写入，比方说来了一个连接，我们分配了一个线程，连接执行完后，再执行读取，读取过程可能有点慢，就会一直占用这个线程，如果再多来几个连接，那么线程池
                    的线程，马上用完了，就会出现线程同步的粒度太大了，限制了吞吐量。这个时候，Reactor模式横空出世。在Reactor中，这些被拆分的小线程或者子过程对应的是handler，每一种handler会出处理一种event。
reactor模型的三种方式：
       单线程：它是由一个线程来接收客户端的连接，并将该请求分发到对应的事件处理handle中，整个过程完全是异步非阻塞的。
       多线程：将单线程的事件处理改成了多线程，可以利用Java线程池的实现。
       主从多线程：该模型将客户端连接的那一块的线程改成了多线程。
       netty模型：boss相当于reactor模型中处理客户端连接的线程池，work自然就是处理事件的线程池。

reactor模型总结：介绍了那么多，Reactor模式的优点很明显，解耦、提升复用性、模块化、可移植性、事件驱动、细力度的并发控制等。Reactor模式的缺点也很明显，模型复杂，因为涉及到内部回调，多线程处理，
不容易调试；需要操作系统底层支持，这就导致不同操作系统可能会产生不一样的结果。所以总的来说如果并发要求不是那么高，使用传统的阻塞线程池模型足够了，而且调试、查问题都会简单很多；如果我们的使用场景
是会产生瞬时大并发，可以使用Reactor模式来实现，目前大部分的NIO框架或者容器都是实现了Reactor模式，Tomcat、Jetty的NIO都是实现了Reactor模式，Netty和Mina是两套NIO的框架，也分别对Java NIO
进行了二次封装实现了Reactor模式。
总结就一句：“分而治之，将任务拆分开来，由专门的人负责专门的任务”，这不仅在计算机领域生效，在整个社会领域都生效。


JAVA NIO！！！
核心组件：channel、buffer、selector、unsafe
channel：FileChannel DatagramChannel SocketChannel（以TCP来向网络连接的两端读写数据） ServerSocketChannel（能够监听客户端发起的TCP连接） 。数据总是从缓冲区写入通道，并从通道读取到缓冲区。
buffer：缓存区，实际上是一个容器，是一个连续数组。比如从一个客户端向服务端发送数据，然后服务端接收数据的过程。客户端发送数据时，必须先将数据存入Buffer中，然后将Buffer中的内容写入通道。服务端这边
        接收数据必须通过Channel将数据读入到Buffer中，然后再从Buffer中取出数据来处理。
        api：写的时候，position写入一个，进一位，limit = capacity。读的时候，buffer.flip()转换成读模式，position变成第0位，limit = 前position位置。 buffer.rewind()使position=0。
             compact()作用：将 position 与 limit之间的数据复制到buffer的开始位置，复制后 position  = limit -position,limit = capacity，防止往里面put数据的时候，未读取的数据被覆盖。
             get()和put('x')会改变position的位置，get(2)和put(2,'x')不会改变position的位置。
Selector：选择器，是可以选择通道的多路复用器，可用作可以进入非阻塞模式的特殊类型的通道。它可以检查一个或多个NIO通道，并确定哪个通道准备好了可以进行通信，即读取或写入。选择器(Selector)用于使用单
         个线程处理多个通道。 因此，它需要较少的线程来处理这些通道。 线程之间的切换对于操作系统来说是昂贵的。 因此，使用它可以提高系统效率。

NETTY!!!
netty的理解：轻松快速开发网络应用程序的nio框架，简化了网络编程。reactor线程模型，netty对Java nio进行了封装，方便了我们的使用。
netty的启发：接口同步转异步处理、回调通知结果、多线程提高并发效率、带来的问题：异步之后事务如何保证、回调失败的情况、多线程带来的上下文切换、共享资源的问题。
netty的使用场景：构建高性能rpc基础通信组件，比如dubbo，跨节点通信。
netty的简单demo：.......




-----------------------------------------------------------------------------------


全局唯一id生成？？？
UUID：本地生成，不需要远程调用，缺点是32位，没有意义，占用空间比较大，mysql的B+树索引索引效率比较低。
数据库的自增id：优点是简单，缺点是高并发情况下性能不佳，并且要考虑单点问题。
第三方软件生成Redis和zookeeper：优点是比数据库性能好，缺点是需要引入新的组件。
snowflake：
优点
简单高效，生成速度快。
时间戳在高位，自增序列在低位，整个ID是趋势递增的，按照时间有序递增。
灵活度高，可以根据业务需求，调整bit位的划分，满足不同的需求。
缺点
依赖机器的时钟，如果服务器时钟回拨，会导致重复ID生成。
在分布式环境上，每个服务器的时钟不可能完全同步，有时会出现不是全局递增的情况。
| 41 bits: Timestamp | 3 bits: 区域 | 10 bits: 机器编号 | 10 bits: 序列号 |



-----------------------------------------------------------------------------------
遇到的BUG？？？
本接盘侠接手的一个服务使用RabbitMQ和其他服务进行消息传输。我们发现，有时候RabbitMQ中明明有元素，但是不会回调DefaultConsumer接口的handleDelivery函数，于是队列无法消化，队列越堵越长。
通过jstack查看，发现rabbitmq消费者线程堵塞在socketinputstream的socketRead0函数。
那是否是因为没有正确地ACK/NACK导致的队列堵塞问题呢？我们进一步通过rabbitmqctl  命令进行查看，注意name messages_unacknowledged 可以查看被取走但还没有ACK的消息数：
rabbitmqctl  list_queues -p xxx.host name messages_ready messages_unacknowledged 
unacknowledgedListing queues ...
xxx.queue 0 500 （我们的Qos设置为500，这时候没有ACK的消息数量已经达到上限，队列）
...done.
通过以上排查基本可以确定队列堵塞是由于消费者线程取走了消息，但是既没有ACK，也没有NACK，这样的消息个数到达Qos设置的值后，队列就会堵塞，不再回调handleDelivery函数。我查看原来的代码，发现逻辑十分混乱，在各种分支和异常处理中进行basicAck和basicNack操作，经过仔细分析发现存在分支遗漏这两个操作，这样就可能在长时间运行后，最终导致队列的堵塞。
解决办法应该是在finally语句中来执行这些操作，我分析从队列中取出消息后，会有三种处理结果：1、处理成功，这种时候应该用basicAck确认消息；2、可重试的处理失败，这时候应该用basicNack将消息重新入列；3、不可重试的处理失败，这时候应该使用basicNack将消息丢弃。


----------------------------------------------------------------------------------------
做过的项目：分库分表、rabbitMQ集群搭建、mq监控、rabbitmq实现分布式事务、rabbitmq实现日志跟踪、springboot\cloud封装。

-----------------------------------------------------------------------------------

分布式session设置？？？
https://www.cnblogs.com/study-everyday/p/7853145.html

-----------------------------------------------------------------------------------

时间复杂度？？？
https://blog.csdn.net/qq_41523096/article/details/82142747

-----------------------------------------------------------------------------------

数据结构？？？
红黑树：https://juejin.im/post/5a27c6946fb9a04509096248#comment
为了解决二叉树多次插入新节点而导致的不平衡！
1.节点是红色或黑色。
2.根节点是黑色。
3.每个叶子节点都是黑色的空节点（NIL节点）。
4 每个红色节点的两个子节点都是黑色。(从每个叶子到根的所有路径上不能有两个连续的红色节点)
5.从任一节点到其每个叶子的所有路径都包含相同数目的黑色节点。
这些规则就是保证了红黑树的平衡，当插入或者删除节点的时候，为了保证红黑树的规则，必须要做出一些调整（变色或者旋转），来继续维持我们的规则。







-----------------------------------------------------------------------------------

17、note

爱奇艺Java开发程序员7大note题
https://www.toutiao.com/a6529771972564353549/?tt_from=weixin&utm_campaign=client_share&timestamp=1520378903&app=news_article&utm_source=weixin&iid=27016189472&utm_medium=toutiao_ios&wxshare_count=1
A、int，double型怎么存储的?


B、常见设计模式、并发优缺点
Num1：单例模式
基本概念：保证一个类仅有一个实例，并提供一个访问它的全局访问点。
Num2：工厂模式
基本概念：为创建对象提供过渡接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。
Num4：观察者模式
基本概念：观察者模式定义了一种一对多的依赖关系，让多个观察者对象同时监听某一主题对象。这个主题对象在状态发生变化时，会通知所有观察者对象，使它们能够自动更新自己。观察者模式又叫发布-订阅(Publish/Subscribe)模式。
Num5：适配器(Adapter)模式
基本概念：适配器模式把一个类的接口变换成客户端所期待的另一种接口，从而使原本因接口不匹配而无法在一起工作的两个类能够在一起工作。
Num6：代理模式
基本概念：为其他对象提供一种代理以控制对这个对象的访问。也可以说，在出发点到目的地之间有一道中间层，意为代理。
并发优缺点???
优点:资源利用率更好\程序设计更简单\程序响应更快
缺点:安全性(主要是多个线程共享数据时可能会产生于期望不相符的结果)\死锁\性能问题(线程过多时会使得CPU频繁切换，花在调度上时间太多。增加资源消耗)

C、类加载原理、虚拟机、内存模型
类从被加载到虚拟机内存中开始，到卸载出内存为止，它的整个生命周期包括：加载（Loading）、验证（Verification）、准备(Preparation)、解析(Resolution)、初始化(Initialization)、使用(Using)和卸载(Unloading)7个阶段。其中准备、验证、解析3个部分统称为连接（Linking）
加载、验证、准备、初始化和卸载这5个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始，而解析阶段则不一定：它在某些情况下可以在初始化阶段之后再开始，这是为了支持Java语言的运行时绑定（也称为动态绑定或晚期绑定）。以下陈述的内容都已HotSpot为基准。
加载：查找并加载类的二进制数据
验证：确保被加载的类的正确性
准备：为类的静态变量分配内存，并将其初始化为默认值
解析：把类中的符号引用转换为直接引用
初始化: 初始化，为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。

内存模型???http://blog.csdn.net/suifeng3051/article/details/52611310
关于主内存与工作内存之间的具体交互协议，即一个变量如何从主内存拷贝到工作内存、如何从工作内存同步到主内存之间的实现细节，Java内存模型定义了以下八种操作来完成：
lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。

unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。

read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用

load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。

use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。

assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。

store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。

write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。
重排序
在执行程序时为了提高性能，编译器和处理器经常会对指令进行重排序

当一个变量定义为volatile之后，它将具备两种特性：

第一：保证此变量对所有线程的可见性，这里的可见性是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。普通变量的值在线程间传递需要通过主内存来完成

由于valatile只能保证可见性，在不符合一下两条规则的运算场景中，我们仍要通过加锁来保证原子性

1.运算结果并不依赖变量的当前值，或者能够确保只有单一的线程修改变量的值。

2.变量不需要与其他的状态变量共同参与不变约束

第二：禁止指令重排序，普通的变量仅仅会保证在该方法的执行过程中所有依赖赋值结果的地方都能获取到正确的结果，而不能保证变量赋值操作的顺序与程序代码中执行顺序一致，这个就是所谓的线程内表现为串行的语义


同步块可以保证在同一时刻仅有一个线程可以进入代码的临界区。同步块还可以保证代码块中所有被访问的变量将会从主存中读入，当线程退出同步代码块时，所有被更新的变量都会被刷新回主存中去，不管这个变量是否被声明为volatile。



D、多线程相关，比如jdk线程池种类，synchronize关键字，volatile关键字。



2、redis知道哪些数据类型?
String   list(列出最新的评论,队列)  set hash zset(有序集合.排行榜相关)  
缓存 发布和订阅 计数器
https://www.cnblogs.com/mrhgw/p/6278619.html

3、JDK和JRE的区别是什么?
JDK是Java的开发工具，它不仅提供了Java程序运行所需的JRE，还提供了一系列的编译，运行等工具，如javac，java，javaw等。JRE只是Java程序的运行环境，它最核心的内容就是JVM（Java虚拟机）及核心类库。



4、谈谈mybatis的原理实现过程
原理详解：   MyBatis应用程序根据XML配置文件创建SqlSessionFactory，SqlSessionFactory在根据配置，配置来源于两个地方，一处是配置文件，一处是Java代码的注解，获取一个SqlSession。SqlSession包含了执行sql所需要的所有方法，可以通过SqlSession实例直接运行映射的sql语句，完成对数据的增删改查和事务提交等，用完之后关闭SqlSession。

 
MyBatis的优缺点
 
优点：
1、简单易学
     mybatis本身就很小且简单。没有任何第三方依赖，最简单安装只要两个jar文件(mybaties\mybaties-spring)+配置几个sql映射文件易于学习，易于使用，通过文档和源代码，可以比较完全的掌握它的设计思路和实现。
2、灵活

       mybatis不会对应用程序或者数据库的现有设计强加任何影响。 sql写在xml里，便于统一管理和优化。通过sql基本上可以实现我们不使用数据访问框架可以实现的所有功能，或许更多。

3、解除sql与程序代码的耦合

       通过提供DAL层，将业务逻辑和数据访问逻辑分离，使系统的设计更清晰，更易维护，更易单元测试。sql和代码的分离，提高了可维护性。

4、提供映射标签，支持对象与数据库的orm字段关系映射
5、提供对象关系映射标签，支持对象关系组建维护
6、提供xml标签，支持编写动态sql。

缺点：

1、编写SQL语句时工作量很大，尤其是字段多、关联表多时，更是如此。

2、SQL语句依赖于数据库，导致数据库移植性差，不能更换数据库。
  
3、框架还是比较简陋，功能尚有缺失，虽然简化了数据绑定代码，但是整个底层数据库查询实际还是要自己写的，工作量也比较大，而且不太容易适应快速数据库修改。


4、二级缓存机制不佳


总结
mybatis的优点同样是mybatis的缺点，正因为mybatis使用简单，数据的可靠性、完整性的瓶颈便更多依赖于程序员对sql的使用水平上了。sql写在xml里，虽然方便了修改、优化和统一浏览，但可读性很低，调试也非常困难，也非常受限。

      mybatis没有hibernate那么强大，但是mybatis最大的优点就是简单小巧易于上手，方便浏览修改sql语句。

5、数据库事务，spring事务如何实现?
一、事务的四大特性（ACID）

1、原子性（atomicity）：组成事务的语句形成了一个逻辑单元，不能只执行一部分；

2、一致性（consistency）：在事务处理执行前后，数据库与理论值是一致的（数据库完整性约束）；

3、隔离性（isolcation）：一个事务处理和另一个事务处理相互间互不影响；

4、持续性（durability）：事务处理的效果能够被永久保存下来。

二、隔离级别

1、多线程并发执行可能会产生以下三个问题：

　　脏读（dirtyreads）：一个事务读取了另一个事务未提交的并行事务写的数据；

　　不可重复读（non-repeatablereads）：一个事务重新读取前面读取过的数据，发现该数据已经被另一个已提交的事务修改过；(同样的条件，你读取过的数据，再次读取出来发现值不一样了。)

　　幻读（phantomread）：一个事务重新执行一个查询，返回一套符合条件的行，发现这些行因为最近提交的事务而发生了改变(同样的条件，第1次和第2次读出来的记录数不一样。)

2、隔离级别

　　读未提交（Read uncommitted）：未解决

　　读已提交 （Readcommitted）：已解决：脏读

　　可重复读 （Repeatableread）：已解决：脏读，不可重复读

　　序列化 （Serializble）：已解决：脏读，不可重复读，幻读

3、设置隔离级别

　　connection.setTransactionlsolation(Connection.事务级别)

　　MySql默认为读已提交；

spring事务如何实现????http://blog.csdn.net/bazingaea/article/details/53133620

（1）编程式事务管理：需要手动编写代码，在实际开发中很少使用

（2）声明式事务管理：

（2.1）基于TransactionProxyFactoryBean的方式，需要为每个进行事务管理的类做相应配置

（2.2）基于AspectJ的XML方式，不需要改动类，在XML文件中配置好即可

（2.3）基于注解的方式，配置简单（<!-- 配置事务管理器 -->  <!-- 开启注解事务 -->  ），需要在业务层类中添加注解



（2.2）和（2.3）在开发中使用比较多，前者配置一目了然，可以在XML文件中得到所有信息，后者配置简单方便
(2.3)
<!-- 配置事务管理器 -->  
<bean id="transactionManager" class="org.springframework.jdbc.datasource.DataSourceTransactionManager">  
    <property name="dataSource" ref="dataSource"/>  
</bean>  
      
<!-- 开启注解事务 -->  
<tx:annotation-driven transaction-manager="transactionManager"/>  

6、String类为什么是final的?
final的出现就是为了为了不想改变，而不想改变的理由有两点：设计(安全)或者效率。
final 修饰的类是不被能继承的，所以 final 修饰的类是不能被篡改的。 
1、从设计安全)上讲， 
1)、确保它们不会在子类中改变语义。String类是final类，这意味着不允许任何人定义String的子类。
换言之，
如果有一个String的引用，它引用的一定是一个String对象，而不可能是其他类的对象。 
2)、String 一旦被创建是不能被修改的，
因为 java 设计者将 String 为可以共享的，下面这段是源码中的注释：
2、从效率上讲： 
1)、设计成final，JVM才不用对相关方法在虚函数表中查询，而直接定位到String类的相关方法上，提高了执行效率。 
2)、Java设计者认为共享带来的效率更高。

总而言之，就是要保证 java.lang.String 引用的对象一定是 java.lang.String的对象，而不是引用它的子孙类，这样才能保证它的效率和安全。

spring mvc?????https://www.cnblogs.com/cainiaoxb/p/5707730.html

SpringMVC运行原理
1. 客户端请求提交到DispatcherServlet

2. 由DispatcherServlet控制器查询一个或多个HandlerMapping，找到处理请求的Controller

3. DispatcherServlet将请求提交到Controller

4. Controller调用业务逻辑处理后，返回ModelAndView

5. DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图

6. 视图负责将结果显示到客户端

DispatcherServlet是整个Spring MVC的核心。它负责接收HTTP请求组织协调Spring MVC的各个组成部分。其主要工作有以下三项：

       1. 截获符合特定格式的URL请求。
       2. 初始化DispatcherServlet上下文对应的WebApplicationContext，并将其与业务层、持久化层的WebApplicationContext建立关联。
       3. 初始化Spring MVC的各个组成组件，并装配到DispatcherServlet中。
	   
	   
光云科技面试???
java内存模型??http://blog.csdn.net/suifeng3051/article/details/52611310
	   
	   内存屏障（Memory Barrier ）:
	   1. 保证特定操作的执行顺序。
       2. 影响某些数据（或则是某条指令的执行结果）的内存可见性。
编译器和CPU能够重排序指令，保证最终相同的结果，尝试优化性能。插入一条Memory Barrier会告诉编译器和CPU：不管什么指令都不能和这条Memory Barrier指令重排序。
Memory Barrier所做的另外一件事是强制刷出各种CPU cache，如一个Write-Barrier（写入屏障）将刷出所有在Barrier之前写入 cache 的数据，因此，任何CPU上的线程都能读取到这些数据的最新版本。
这和java有什么关系？上面java内存模型中讲到的volatile是基于Memory Barrier实现的。

dubbo服务搭建???http://shiyanjun.cn/archives/1075.html
首先运行zookeeper，然后启动dubbo-server程序，最后运行dubbo-client程序。dubbo-client中的服务接口路径需要和dubbo-server的一致，也就是说一个类在dubbo-server中的url和在dubbo-client中的url要一样，否则运行会出现Forbid consumer异常。


京东面试???
一、关于数据库事务说出你知道的所有???
数据库四大特性
原子性 一致性 隔离性  持久性（D）
造成的问题
脏读 不可重复读 幻读
隔离级别
读未提交 读已提交 可重复读 序列化
mysql默认的事务处理级别是'REPEATABLE-READ',也就是可重复读

把你知道的java的concurrent包的技术全部说出来（volatile、锁重入，LinkedTransferQueue字节追加提高并发度技术，ConcurrentHaspMap结合volatile的happen-before读取优化）
在java并发处理中，concurrent已成为毋庸置疑的核心标准。队列、线程池、atomic开头的原子性变量。
AtomicInteger 利用CAS保证原子性
ReentrantLock  锁重入  信号量 闭锁
ConcurrentHashMap  并发集合 
ThreadPoolExecutor 线程池
LinkedBlockingQueue  队列

二、redis的配置文件（AOF&&Snapshot&&主从复制）???
指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合
save <seconds> <changes>
Redis默认配置文件中提供了三个条件：
save 900 1
save 300 10
save 60 10000
分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。
指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no
appendonly no

指定更新日志文件名，默认为appendonly.aof
appendfilename appendonly.aof

指定更新日志条件，共有3个可选值：
no：表示等操作系统进行数据缓存同步到磁盘（快）
always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）
everysec：表示每秒同步一次（折衷，默认值）
appendfsync everysec
设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步
slaveof <masterip> <masterport>

三、WebSocket长连接问题???
前后端如何保持长连接？---WEBSOCKET
1. pc端的应用，一般会采用前端定时请求后台;

2. app定时去访问后台的话，对用户来说并不友好，会消耗大量的流量，移动端最好的方式就是后台主动向app推送信息;

3. H5提供了一种比较好的方式是websocket，打开app后，向后台发出请求，后台响应后，就可以实时向前端推送信息了，而无需app再次去访问；
为什么要用 WebSocket
使用传统的 HTTP 轮询或者长连接的方式也可以实现类似服务器推送的效果，但是这类方式都存在资源消耗过大或推送延迟等问题。而 WebSocket 直接使用 TCP 连接保持全双工的传输，可以有效地减少连接的建立，实现真正的服务器通信，对于有低延迟有要求的应用是一个很好的选择。



四、秒杀业务场景设计（事务，逻辑调整，行级锁，数据库并发度、mybatis调用存储过程）???
六、如何保障请求执行顺序？？？
七、分布式事务和分布式锁？？？

八、分布式session设置？？？
https://www.cnblogs.com/study-everyday/p/7853145.html


九、zokeeper哪些作用???
数据发布与订阅（配置中心）
负载均衡
命名服务(Naming Service)
分布式通知/协调
集群管理与Master选举
分布式锁
分布式队列
http://blog.csdn.net/tycoon1988/article/details/38866395
https://www.toutiao.com/a6512267582714675716/?tt_from=weixin&utm_campaign=client_share&timestamp=1521186721&app=news_article&utm_source=weixin&iid=28104235531&utm_medium=toutiao_ios&wxshare_count=1
十、JVM内存模型？？？
十一、数据库垂直和水平拆分？？？http://blog.csdn.net/jerome_s/article/details/52492616
十二、mybaties分页???
十三、IO 和 NIO  阻塞和非阻塞？？？NIO可让您只使用一个（或几个）单线程管理多个通道（网络连接或文件），但付出的代价是解析数据可能会比从一个阻塞流中读取数据更复杂。
在阻塞模式下，若从网络流中读取不到指定大小的数据量，阻塞IO就在那里阻塞着。比如，已知后面会有10个字节的数据发过来，但是我现在只收到8个字节，那么当前线程就在那傻傻地等到下一个字节的到来，对，就在那等着，啥事也不做，直到把这10个字节读取完，这才将阻塞放开通行。

在非阻塞模式下，若从网络流中读取不到指定大小的数据量，非阻塞IO就立即通行。比如，已知后面会有10个字节的数据发过来，但是我现在只收到8个字节，那么当前线程就读取这8个字节的数据，读完后就立即返回，等另外两个字节再来的时候再去读取。

从上面可以看出，阻塞IO在性能方面是很低下的，如果要使用阻塞IO完成一个Web服务器的话，那么对于每一个请求都必须启用一个线程进行处理。而使用非阻塞IO的话，一到两个线程基本上就够了，因为线程不会产生阻塞，好比一下接收A请求的数据，另一下接收B请求的数据，等等，就是不停地东奔西跑，直接到把数据接收完了。

虽然说，非阻塞IO比阻塞IO有更高的性能，但是对于开发来的，难度就成数倍递增了。由于是有多少数据就读取多少数据，这样在读取完整之前需要将已经读取到的数据保存起来，而且需要与其他地方来的数据隔离开来不能混在一起，否则就不知道这数据是谁的了

十四、分布式接口幂等性设计？？？https://blog.csdn.net/rickiyeat/article/details/78390897
随着分布式系统及微服务的普及，因为网络原因而导致调用系统未能获取到确切的结果从而导致重试，这就需要被调用系统具有幂等性。


聚光科技面试题目????
1、公司的整体架构画出来???
2、数据库数据源 包括监控？？？
DRUID是阿里巴巴开源平台上一个数据库连接池实现，它结合了C3P0、DBCP、PROXOOL等DB池的优点，同时加入了日志监控，可以很好的监控DB池连接和SQL的执行情况，可以说是针对监控而生的DB连接池(据说是目前最好的连接池,不知道速度有没有BoneCP快)。
3、MySQL读写分离
https://www.cnblogs.com/joylee/p/7513038.html
主从复制
http://blog.csdn.net/chengzi28/article/details/50960549

4、分布式session
https://www.cnblogs.com/study-everyday/p/7853145.html
spring session  web.xml中自定义了一个filter,这个filter继承了HttpServletRequestWrapper 、HttpServletResponseWrapper 类，重写getSession等相关方法(在这些方法里调用相关的 session存储容器操作类)。 

5、数据库访问慢检测
是否有慢查询SQL  是否有死锁 

6、mq介绍及其问题排查

7、分库分表规则、注意事项

8、redis使用过程中的注意事项、雪崩
搭建 Redis Cluster
搭建集群工作分为三步：

准备节点
节点握手
分配槽
什么是缓存穿透？
一般的缓存系统，都是按照key去缓存查询，如果不存在对应的value，就应该去后端系统查找（比如DB）。如果key对应的value是一定不存在的，并且对该key并发请求量很大，就会对后端系统造成很大的压力。这就叫做缓存穿透。
 
如何避免？
1：对查询结果为空的情况也进行缓存，缓存时间设置短一点，或者该key对应的数据insert了之后清理缓存。
2：对一定不存在的key进行过滤。可以把所有的可能存在的key放到一个大的Bitmap中，查询时通过该bitmap过滤。【感觉应该用的不多吧】
缓存雪崩
什么是缓存雪崩？
当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给后端系统(比如DB)带来很大压力。
如何避免？
1：在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。
2：不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。
3：做二级缓存，A1为原始缓存，A2为拷贝缓存，A1失效时，可以访问A2，A1缓存失效时间设置为短期，A2设置为长期（此点为补充）

9、分布式事务
主事务表 分事务表 下单流程  主事务表初始状态-1  下单--使用优惠券(生成分事务表,状态1)--扣库存(生成分事务表,状态1)   主事务表状态变成0   然后变成1然后 分事务表状态变成-1  事务完成
扫描主事务表 状态值<1的  找到事务id 查找分事务表  进行相应的回滚操作.



10、技术突破口



安恒信息面试题???
1.rpc协议 http协议 TCP UDP
2.MQ的规范
https://blog.csdn.net/zxjiayou1314/article/details/67634369

光云面试？？
SQL优化  索引（前缀索引）
慢SQL查询的SQL语句检查出来   SQL执行计划   索引  
JVM调优
OOM
https://www.jianshu.com/p/2fdee831ed03
https://blog.csdn.net/wfh6732/article/details/57422967?utm_source=itdadao&utm_medium=referral
https://www.cnblogs.com/jay36/p/7680008.html
VisualVM：JDK自带，功能强大，与JProfiler类似。推荐。
有了堆信息查看方面的功能，我们一般可以顺利解决以下问题：

  --年老代年轻代大小划分是否合理
  --内存泄漏
  --垃圾回收算法设置是否合理

千金顶面试？？？》
dubbo线程池？？？
<dubbo:protocolname="dubbo"dispatcher="all"threadpool="fixed"threads="100"/>
Dispatcher！！！
all 所有消息都派发到线程池，包括请求，响应，连接事件，断开事件，心跳等。
direct 所有消息都不派发到线程池，全部在IO线程上直接执行。
message 只有请求响应消息派发到线程池，其它连接断开事件，心跳等消息，直接在IO线程上执行。
execution 只请求消息派发到线程池，不含响应，响应和其它连接断开事件，心跳等消息，直接在IO线程上执行。
connection 在IO线程上，将连接断开事件放入队列，有序逐个执行，其它消息派发到线程池。

默认为all，为了减少在Provider线程池打满时整个系统雪崩的风险，建议将Dispatcher设置成message：

ThreadPool！！！
fixed 固定大小线程池，启动时建立线程，不关闭，一直持有。(缺省)
cached 缓存线程池，空闲一分钟自动删除，需要时重建。
limited 可伸缩线程池，但池中的线程数只会增长不会收缩。(为避免收缩时突然来了大流量引起的性能问题)。

默认是固定大小线程池，如果服务消费者太多时会出现等待，甚至超时：这时可以两种选择，增加线程的数量或者修改线程池为缓存线程池。不过我推荐第二种！！！

jvm调优？？？
mq消息丢失？？？
zk集群部署？？？

光云复试？？
优雅发布，分布式事务，并发包，mysql事务等级（https://blog.csdn.net/ochangwen/article/details/52558774），轮训实现，rocketmq重试机制？？(https://blog.csdn.net/zhanglianhai555/article/details/77162208?ref=myrecommend)

你会发现RECONSUME_LATER的策略：如果消费失败，那么1S后再次消费，如果失败，那么5S后，再次消费，……直至2H后如果消费还失败，那么该条消息就会终止发送给消费者了！ 
RocketMQ为我们提供了这么多次数的失败重试，但是在实际中也许我们并不需要这么多重试，比如重试3次，还没有成功，我们希望把这条消息存储起来并采用另一种方式处理，而且希望RocketMQ不要再重试呢，因为重试解决不了问题了！这该如何做呢？ 
比如由于网络原因导致消息压根就没有从MQ到消费者上，那么在RocketMQ内部会不断的尝试发送这条消息，直至发送成功为止！（比如集群中一个broker失败，就尝试另一个broker） 

JVM 调优  ？？？
观察oom：Java heap space  
1.流量/数据量峰值：  2.内存泄漏：特定的编程错误会导致你的应用程序不停的消耗更多的内存，每次使用有内存泄漏风险的功能就会留下一些不能被回收的对象到堆空间中，随着时间的推移，泄漏的对象会消耗所有的堆空间，最终触发java.lang.OutOfMemoryError: Java heap space错误。


java.lang.OutOfMemoryError:Permgen space
因此，我们可以得出出现java.lang.OutOfMemoryError: PermGen space错误的原因是：太多的类或者太大的类被加载到permanent generation（持久代）。


堆大小设置\回收器选择(吞吐量优先的并行收集器 响应时间优先的并发收集器)

dump文件：进程的内存镜像
调试工具：VisualVM：JDK自带，功能强大，与JProfiler类似。推荐。


一致性hash???


最关键的区别就是，对节点和数据，都做一次哈希运算，然后比较节点和数据的哈希值，数据取和节点最相近的节点做为存放节点。这样就保证当节点增加或者减少的时候，影响的数据最少。





四、调优总结

年轻代大小选择
响应时间优先的应用：尽可能设大，直到接近系统的最低响应时间限制（根据实际情况选择）。在此种情况下，年轻代收集发生的频率也是最小的。同时，减少到达年老代的对象。
吞吐量优先的应用：尽可能的设置大，可能到达Gbit的程度。因为对响应时间没有要求，垃圾收集可以并行进行，一般适合8CPU以上的应用。
年老代大小选择
响应时间优先的应用：年老代使用并发收集器，所以其大小需要小心设置，一般要考虑并发会话率和会话持续时间等一些参数。如果堆设置小了，可以会造成内存碎片、高回收频率以及应用暂停而使用传统的标记清除方式；如果堆大了，则需要较长的收集时间。最优化的方案，一般需要参考以下数据获得：
并发垃圾收集信息
持久代并发收集次数
传统GC信息
花在年轻代和年老代回收上的时间比例
减少年轻代和年老代花费的时间，一般会提高应用的效率
吞吐量优先的应用：一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代。原因是，这样可以尽可能回收掉大部分短期对象，减少中期的对象，而年老代尽存放长期存活对象。
较小堆引起的碎片问题
因为年老代的并发收集器使用标记、清除算法，所以不会对堆进行压缩。当收集器回收时，他会把相邻的空间进行合并，这样可以分配给较大的对象。但是，当堆空间较小时，运行一段时间以后，就会出现“碎片”，如果并发收集器找不到足够的空间，那么并发收集器将会停止，然后使用传统的标记、清除方式进行回收。如果出现“碎片”，可能需要进行如下配置：
-XX:+UseCMSCompactAtFullCollection：使用并发收集器时，开启对年老代的压缩。
-XX:CMSFullGCsBeforeCompaction=0：上面配置开启的情况下，这里设置多少次Full GC后，对年老代进行压缩



爱上租面试？？？
hashmap为什么是线程不安全的？？？
concurrenthashmap分段锁的实现？？？
NIO实现原理？？？
缓存的调用链？？？
分库分表？？？

http://mp.weixin.qq.com/s/T7nbj2aUR-jlmpH3o2GZgQ
spring保证单例？？？

阿里电面？？？
分布式锁 实例保证？？？

rocketmq  消息存储？？？


蜂泰面试？？？
hashmap，cas，jvm，
dubbo踩坑？？？
 Dubbo启动时检查 check="false"   
 Dubbo传输的对象必须序列化（Serializable）
 


主键索引和其他字段的索引的差别？？？

主键一定是唯一性索引，唯一性索引并不一定就是主键。
一个表中可以有多个唯一性索引，但只能有一个主键。
主键列不允许空值，而唯一性索引列允许空值。
主键可以被其他字段作外键引用，而索引不能作为外键引用。

唯一性索引：
这种索引和前面的“普通索引”基本相同，但有一个区别：索引列的所有值都只能出现一次，即必须唯一。

数据库调优，搜索原理算法？？？
是否是周期性的-是--访问高峰或者缓存崩溃引起（）
慢查询SQL语句获得有问题的SQL语句----通过profiling和explain 分析SQL语句
等待执行SQL时间-是--对服务器参数进行调优（如缓冲区、线程数）
---SQL语句执行时间长---优化SQL、建立/优化索引、优化表结构

ROCKETMQ刷盘：
同步刷盘：在消息到达MQ后，RocketMQ需要将数据持久化，同步刷盘是指数据到达内存之后，必须刷到commitlog日志之后才算成功，然后返回producer数据已经发送成功。
异步刷盘：，同步刷盘是指数据到达内存之后,返回producer说数据已经发送成功。，然后再写入commitlog日志。
commitlog就是来存储所有的元信息，包含消息体，类似于Mysql、Oracle的redolog,所以主要有CommitLog在，Consume Queue即使数据丢失，仍然可以恢复出来。
consumequeue：记录数据的位置,以便Consume快速通过consumequeue找到commitlog中的数据




3.2.1优点： 
1、队列轻量化，单个队列数据量非常少。对磁盘的访问串行化，避免磁盘竟争，不会因为队列增加导致IOWAIT增高。

3.2.2缺点： 
写虽然完全是顺序写，但是读却变成了完全的随机读。 
读一条消息，会先读ConsumeQueue，再读CommitLog，增加了开销。 
要保证CommitLog与ConsumeQueue完全的一致，增加了编程的复杂度。

3.2.3以上缺点如何克服： 
随机读，尽可能让读命中page cache，减少IO读操作，所以内存越大越好。如果系统中堆积的消息过多，读数据要访问磁盘会不会由于随机读导致系统性能急剧下降，答案是否定的。 
访问page cache 时，即使只访问1k的消息，系统也会提前预读出更多数据，在下次读时，就可能命中内存。 
随机访问Commit Log磁盘数据，系统IO调度算法设置为NOOP方式，会在一定程度上将完全的随机读变成顺序跳跃方式，而顺序跳跃方式读较完全的随机读性能会高5倍以上。 
另外4k的消息在完全随机访问情况下，仍然可以达到8K次每秒以上的读性能。 
由于Consume Queue存储数据量极少，而且是顺序读，在PAGECACHE预读作用下，Consume Queue的读性能几乎与内存一致，即使堆积情况下。所以可认为Consume Queue完全不会阻碍读性能。 
Commit Log中存储了所有的元信息，包含消息体，类似于Mysql、Oracle的redolog，所以只要有Commit Log在，Consume Queue即使数据丢失，仍然可以恢复出来。



洗衣郎和绿湾面试？？？
object的方法???
hashcode wait notifyAll equals toString  getClass


hashcode的计算方式？？？
默认对象在内存中的地址


跨域，redis的使用，缓存穿透，服务器卡死检测，内存模型，sql中查询索引和between and 的先后使用问题？？？






做过核心项目：
1、分库分表
 基于Debezium Mysql Kafka Connector获取MySQL数据流,对数据流进行异构处理   概念：https://blog.csdn.net/nilin99/article/details/78224785?locationNum=5&fps=1
 Debezium是一个开源项目，为捕获数据更改(Capture Data Change，CDC)提供了一个低延迟的流式处理平台，通过安装配置Debezium监控数据库，可以实时消费行级别(row-level)的更改。身为一个分布式系统，Debezium也拥有良好的容错性。
 Debezium的源端(即支持监控哪些数据库) : MySQL，MongoDB，PostgreSQL，Oracle，SQL Server
 Debezium的目标端(即可以数据导入端) : Kafka
 Debezium的应用 : 实时同步数据，实时消费数据
 
 OLTP迁移 MySQL数据--->POSTGRES数据
 # 全文检索,搜索引擎建立 MySQL->ElasticSearch(全文搜索引擎)
 框架使用：
 Vert.x异步驱动 基于netty的全异步框架
 DruidDatasource
 KafkaClient
 ZkClient
 Guice IOC
 Guava Eventbus
 Lombok（结合IDE使用）
 
 资源路径
/datasource/

POST 创建数据源
{
      "name":"{datasourceName}",
      "druid.url": "jdbc:postgresql://10.200.133.50:5432/fengdai_report",
      "druid.username": "postgres",
      "druid.password": "Tairan@2017"
}
GET 查看数据源
/datasource/{datasourceName}

GET 查看数据源配置
DELETE 删除数据源
/worker/

POST 创建工作器
{
  "name":"{workerName}",
  "key.deserializer": "org.apache.kafka.common.serialization.StringDeserializer",
  "stream.topic": "fengdai.fengdai_riskcontrol.loan_apply_cold_data",
  "bootstrap.servers": "10.200.133.50:9092",
  "group.id": "fd.stream.test2d",
  "process.datasource": "{datasourceName}",
  "value.deserializer": "org.apache.kafka.common.serialization.StringDeserializer"
}
GET 查看工作器
/worker/{workerName}

DELETE 删除并停止工作器
GET 查看工作情况
/worker/{workerName}/_exec 控制工作器,暂时支持block\resume\reset三种

{
    "cmd":"worker.block",
    "args":{}
}
Connector配置(以开发环境为例，database.server.name必须为fengdai)
{
  "name": "fengdai-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "database.hostname": "10.200.133.21",
    "database.port": "3306",
    "database.user": "root",
    "database.password": "kse20429hfds23240azxws",
    "database.server.id": "12306",
    "database.server.name": "fengdai",
    "database.whitelist": "md_funds_db,fengdai_user,fengdai_thirdparty,fengdai_system,fengdai_shop,fengdai_riskcontrol,fengdai_report,fengdai_product,fengdai_finance,fengdai_activity",
    "table.whitelist":"md_funds_db.funds_tradebill,md_funds_db.funds_sys_params,md_funds_db.funds_account,fengdai_user.entrust_account,fengdai_user.entrust_bill_age_map,fengdai_user.entrust_brokerage_rate,fengdai_user.entrust_organization,fengdai_user.sys_account,fengdai_user.sys_user,fengdai_user.user_agent,fengdai_user.user_agent_relate,fengdai_user.user_auth,fengdai_user.promotion_channel,fengdai_thirdparty.apply_to_third,fengdai_thirdparty.certification_model,fengdai_thirdparty.certification_detail,fengdai_system.sys_dict,fengdai_shop.shop_commodity,fengdai_shop.shop_commodity_category,fengdai_shop.settlement_rule,fengdai_riskcontrol.apply_model,fengdai_riskcontrol.entrust_apply,fengdai_riskcontrol.loan_apply,fengdai_riskcontrol.loan_apply_cold_data,fengdai_riskcontrol.loan_apply_ext,fengdai_riskcontrol.loan_apply_fee,fengdai_riskcontrol.overdue_deduction_record,fengdai_riskcontrol.urge_entrust_follow_record,fengdai_riskcontrol.urge_record,fengdai_riskcontrol.act_hi_taskinst,fengdai_riskcontrol.act_hi_actinst,fengdai_riskcontrol.act_hi_detail,fengdai_riskcontrol.loan_credit,fengdai_riskcontrol.loan_urge_back,fengdai_report.batch_manual_repay,fengdai_report.promotion,fengdai_product.product,fengdai_product.sub_product,fengdai_product.product_auto_strategy,fengdai_finance.adjustment_funds,fengdai_finance.biz_loan_bill,fengdai_finance.batch_manual_repay,fengdai_finance.mc_business,fengdai_finance.batch_repay_other,fengdai_finance.loan_bill_detail,fengdai_finance.channel_settlement,fengdai_finance.withdraw_info,fengdai_activity.activity_ditch,fengdai_activity.card_coupon_commodity,fengdai_activity.auto_withdrawal_conf",
    "database.history.kafka.bootstrap.servers": "10.200.133.50:9092",
    "database.history.kafka.topic": "dbhistory.fengdai" ,
    "include.schema.changes": "true"
  }
}
 
补充：把MySQL的binlog发到kafka的topic上面，然后stream消费，并且存储到pgsql上面。由于PostgreSQL的功能强大，支持并行计算、空间数据处理、文本分析、数据挖掘、机器学习、复杂查询、冷热分离存储、分布式架构(citus, greenplum, xl)等。
kafka必须要依赖zookeeper：1、brocker注册；2、topic注册；3、生产者消费者负载均衡；4、消费消息offset记录；5、消费者注册

分库分表思路：
一、背景：
所有的实例都集中在一台机器上面，导致了以下问题：
1、由于部分实例查询慢，导致其他实例受影响；
2、数据量全部集中在一台机器上，无法水平扩展；
3、运维困难（主从、备份、高可用等维护）；
因此需要按照实例场景和领域进行拆分，经过调查发现存下如下困难：
1、跨库查询；
2、跨库存蓄过程；
3、代码依赖（代码中存在跨库查询）；
4、缺少其他数据库服务器（仅有一主一从）；
5、本地配置更改困难。
二、线上情况：
单个数据库实例存放了多个数据库已经按照业务领域进行了拆分：数据集中在风控和财务库。
机器配置：一主一从，两台机器配置相同，内存120G，CPU40核心，硬盘2T；
连接情况：95%主库，5%从库；
跨库查询情况：涉及到10个库，51张表；
三、解决思路：
0、解决跨库查询；系统间调用的方式；
1、找到所有的解决不了的跨库查询，并且列举所有涉及到的表；
2、实时同步MySQL所有涉及的数据到PG数据库；
3、查询跨库涉及到的表，从查询MySQL转到查询PG，并且做好开关，遇到问题随时切回去。
四、选型及原因：
同步技术：debezium mysql kafka connector
原因：
1、debezium技术已经被公共平台所选型，后期可以转接技术维护；
2、debezium支持全量+增量同步，减少了全量的开发步骤；
3、支持实时同步；
4、开源技术已稳定还在迭代中；
5、PG更适合做多表连接查询，join查询，复杂查询。PG技术已经被数据中心支持，后期可以委托迁移维护。SQL查询基本可以无需较大的修改即可迁移。
五、方案设计：
1、基本原理：MySQL binlog->kafka数据流->解析->处理->pg
2、高可用设计：zk<-同步协调->streamprocessor
3、失败处理：忽略处理-人工补偿、定点回溯、重新回溯。
4、通过ZK反馈和同步集群的工作，包括不同内容的同步器、同步位置的重置、暂停和继续。
5、通过work同步器拉取kafka数据处理，经过parser解析器转换为可识别的数据，由processor处理器负责将数据写入其他数据库。
6、允许一个进程存在多个同步器，每张数据表应有自己的worker同步器，worker同步器托管于workpool工作和管理声明周期。
7、后期可以用于建立搜索引擎es，建立缓存系统，数据流开放给数据部分分析。
六、施工方案：
1、准备期：不停服的情况下，把涉及跨库的表全量同步到PG；
2、过渡期：修改原代码的跨库查询语句(改成从PG查询)，并保证查询出来的数据一致，同时，对原有跨库查询保留，并开发对应的切换器，实现修改后的查询功能和原查询功能的切换；
3、观察期：修改后的跨库查询语句是否正确，出现问题，立即切换；
4、拆分期：尝试拆分个别数据库；
5、稳定期：所有数据库，拆分完毕。
七、服务器硬件：风控数据库保留原来数据库、财务数据迁移到新库、基础数据迁移到RDS
1、风控数据库保留原来数据库，主从不变；
2、财务数据迁移到新库，配置是风控数据一半；
3、基础数据迁移到RDS，CPU8*2GHZ，内存8个G，硬盘500G。
4、kafka和ZK配置：CPU4*2GHZ，内存4个G，硬盘500G。
八、方案预计效果：
1、提升查询性能：提升连接情况，及并发时的资源竞争问题
2、提升查询性能：提升MySQL数据预热，两台服务器可以使用更过数据预加载
3、提升查询性能：提升硬盘查询速度，数据量降低了，单服务器磁盘查找速度提升
4、提升稳定性：多数据源按场景分离，相互不干扰
5、提升稳定性：避免了单台过热时，影响其他数据库查询
6、释放存储瓶颈
7、释放了水平扩展能里



2、springboot/cloud封装
对Spring Cloud/Boot的封装扩展、整合公司现有能力、提供最佳实践，做为基础服务框架，支撑公司新项目地研发。
问题：
 1、无法传承，框架的研发人员离职后没有可以接手

 2、上手难度大，很多框架喜欢重复造轮子，做出来的与业界主流思想/标准格格不入，导致学习培训成本很高

 3、功能片面，不通用，服务框架讲求通用性，尽量让整个公司使用同一套规范以方便维护，但很多框架只实现了某些特定场景的功能，无法通用化

 4、维护成本高，尤其是对于完全自研的框架，往往需要专职人员维护
 
解决：
 1、简单容易，用最通用的、标准的、开发人员都熟悉的开发模型

 2、功能全面，尽量重用市场已有能力实现，减少框架自身的维护成本

 3、轻量，原则上不引入高侵入性的三方框架/类库

 4、可替换，只做扩展，尽量不修改基础框架代码，开发人员完全可以直接基于基础框架开发！

内容：
 1、常用的工具集：json、time、HTTP操作、定时器、响应处理、加解密等等；
 2、集群功能：分布式缓存、分布式map、分布式锁、MQ、领导者选举，接口抽象适配不同的实现者，比方说Redis也是可以做MQ的功能；
 3、幂等处理；对于HTTP操作，要求请求方在请求头或者URL请求参数加上操作id标志，拦截器拦截，存入Redis（setnx），或者从Redis获取校验；
 4、同一响应；
 5、消息通知到钉钉；
 6、异常处理；
 7、权限认证；
 8、测试支持；使用了H2数据库、embedded redis(内嵌式Redis).对于单元测试来说，我们应该让它尽量保持单一环境，不要与网络资源相通讯，这样可以保证测试的稳定性与客观性，
 9、swagger文档支持；代码质量检查：sonar
 10、降级通知；Hystrix：降级、断路保护，防止系统雪崩。
 11、跟踪日志；分布式跟踪系统。
 12、monitor; spring boot admin 集成：查看环境、日志信息、线程信息、dumpheap、trace链路、hystrix熔断。
 13、zuul网关：路由和过滤。当然也是可以把zuul做熔断处理。

补充：
常用的开源数据库有：H2，Derby，HSQLDB，MySQL，PostgreSQL。其中H2和HSQLDB类似，十分适合作为嵌入式数据库使用，而其它的数据库大部分都需要安装独立的客户端和服务器端。
　　H2的优势：
　　　　1、h2采用纯Java编写，因此不受平台的限制。
　　　　2、h2只有一个jar文件，十分适合作为嵌入式数据库试用。
　　　　3、h2提供了一个十分方便的web控制台用于操作和管理数据库内容。
hystrix：https://cloud.tencent.com/developer/article/1333814  https://www.cnblogs.com/gaoyanqing/p/7470085.html
Netflix Hystrix是SOA/微服务架构中提供服务隔离、熔断、降级机制的工具/框架。Netflix Hystrix是断路器的一种实现，用于高微服务架构的可用性，是防止服务出现雪崩的利器。
在分布式架构中，一个应用依赖多个服务是非常常见的，如果其中一个依赖由于延迟过高发生阻塞，调用该依赖服务的线程就会阻塞，如果相关业务的QPS较高，就可能产生大量阻塞，每个请求都占用了系统的CPU、内存、网络等资源，
如果该应用的QPS较高，那么该应用所以的服务资源会被快速消耗完毕，直至应用死掉
另外，故障也会在应用之间传递，如果故障服务的上游依赖较多，可能会引起服务的雪崩效应。就跟数据瘫痪，会引起依赖该数据库的应用瘫痪是一样的道理。
线程池隔离中，发起请求的线程和真实执行的线程不是同一个线程，使用信号量隔离时，它们是同一个线程
Q: 什么时候使用线程池隔离，什么使用信号量隔离？
A:  线程池隔离缺点是带来一定的开销（会有一定的延迟，但是在10毫秒之内的），但不会阻塞请求线程，适合于于IO密集型的任务，每一个依赖服务专门会有个线程池，为了保证不会因为某个服务的问题(延迟过高)影响到其他服务。
信号量隔离使用用户请求线程，没有格外线程切换开销，使用与执行时间和执行逻辑都比较短的本地计算。比如CPU密集型的任务。不能设置超时和异步访问，只有在依赖服务是足够可靠的情况下使用。
hystrix请求合并：将一段时间内的多次请求合并为一次请求，常用于网络IO中，能减少IO次数，缺点是增加平均延迟。
hystrix请求cache：
hystrix还实现了监控的功能：
什么时候会发生fallback处理（服务降级）：断路器打开、线程池信号量占满、抛出异常、超过配置的超时时间。
什么是Hazelcast？
Hazelcast是一个内存分布式计算平台，用于管理数据并并行执行执行应用程序。

1. 它是用Java编写的。
2. 与其他一些内存数据库(如redis)不同，Hazelcast是多线程的，这意味着可从所有可用的CPU内核中受益。
3. 与其他内存数据网格不同 - 它设计用于分布式环境。它支持每个群集无限数量的map和缓存。

根据基准测试，Hazelcast在获取数据方面比Redis快56％，在设置数据方面比Redis快44％。




